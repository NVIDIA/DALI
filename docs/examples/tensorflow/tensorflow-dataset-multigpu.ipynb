{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Using Tensorflow DALI plugin: DALI tf.data.Dataset with multiple GPUs\n",
    "\n",
    "### Overview\n",
    "\n",
    "This notebook is a comprehensive example on how to use DALI `tf.data.Dataset` with multiple GPUs.\n",
    "\n",
    "Let's start with creating a pipeline to read MNIST images."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From <ipython-input-1-7025cf4b02b6>:118: flatten (from tensorflow.python.layers.core) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use keras.layers.flatten instead.\n",
      "WARNING:tensorflow:From /home/awolant/.local/lib/python3.6/site-packages/tensorflow_core/python/layers/core.py:332: Layer.apply (from tensorflow.python.keras.engine.base_layer) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Please use `layer.__call__` method instead.\n",
      "WARNING:tensorflow:From <ipython-input-1-7025cf4b02b6>:119: dense (from tensorflow.python.layers.core) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use keras.layers.Dense instead.\n",
      "WARNING:tensorflow:From <ipython-input-1-7025cf4b02b6>:120: dropout (from tensorflow.python.layers.core) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use keras.layers.dropout instead.\n",
      "WARNING:tensorflow:From <ipython-input-1-7025cf4b02b6>:126: softmax_cross_entropy_with_logits (from tensorflow.python.ops.nn_ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "\n",
      "Future major versions of TensorFlow will allow gradients to flow\n",
      "into the labels input on backprop by default.\n",
      "\n",
      "See `tf.nn.softmax_cross_entropy_with_logits_v2`.\n",
      "\n",
      "Step 0, accuracy: 0.03125\n",
      "Step 100, accuracy: 0.78125\n",
      "Step 200, accuracy: 0.875\n",
      "Step 300, accuracy: 0.90625\n"
     ]
    }
   ],
   "source": [
    "import nvidia.dali as dali\n",
    "from nvidia.dali.pipeline import Pipeline\n",
    "import nvidia.dali.ops as ops\n",
    "import nvidia.dali.types as types\n",
    "\n",
    "import os\n",
    "\n",
    "import nvidia.dali.plugin.tf as dali_tf\n",
    "import tensorflow.compat.v1 as tf\n",
    "tf.disable_eager_execution()\n",
    "\n",
    "\n",
    "\n",
    "# Path to MNIST dataset\n",
    "data_path = os.path.join(os.environ['DALI_EXTRA_PATH'], 'db/MNIST/training/')\n",
    "\n",
    "BATCH_SIZE = 32\n",
    "DROPOUT = 0.2\n",
    "IMAGE_SIZE = 28\n",
    "NUM_CLASSES = 10\n",
    "HIDDEN_SIZE = 128\n",
    "EPOCHS = 5\n",
    "ITERATIONS = 100\n",
    "num_gpus = 2\n",
    "\n",
    "\n",
    "class MnistPipeline(Pipeline):\n",
    "    def __init__(self, device_id=0, shard_id=0, num_shards=1, num_threads=4, seed=0):\n",
    "        super(MnistPipeline, self).__init__(\n",
    "            BATCH_SIZE, num_threads, device_id, seed)\n",
    "        self.reader = ops.Caffe2Reader(path=data_path, random_shuffle=True, shard_id=0, num_shards=num_shards)\n",
    "        self.decode = ops.ImageDecoder(\n",
    "            device='mixed',\n",
    "            output_type=types.GRAY)\n",
    "        self.cmn = ops.CropMirrorNormalize(\n",
    "            device='gpu',\n",
    "            output_dtype=types.FLOAT,\n",
    "            image_type=types.GRAY,\n",
    "            mean=[0.],\n",
    "            std=[255.],\n",
    "            output_layout=types.NCHW)\n",
    "\n",
    "    def define_graph(self):\n",
    "        inputs, labels = self.reader(name=\"Reader\")\n",
    "        images = self.decode(inputs)\n",
    "        labels = labels.gpu()\n",
    "        images = self.cmn(images)\n",
    "\n",
    "        return (images, labels)\n",
    "\n",
    "\n",
    "tf.reset_default_graph()\n",
    "\n",
    "options = tf.data.Options()\n",
    "options.experimental_optimization.apply_default_optimizations = False\n",
    "options.experimental_optimization.autotune = False\n",
    "\n",
    "\n",
    "shapes = [\n",
    "    (BATCH_SIZE, IMAGE_SIZE, IMAGE_SIZE),\n",
    "    (BATCH_SIZE)]\n",
    "dtypes = [\n",
    "    tf.float32,\n",
    "    tf.int32]\n",
    "\n",
    "# This function is copied form: https://github.com/tensorflow/models/blob/master/tutorials/image/cifar10/cifar10_multi_gpu_train.py#L102\n",
    "def average_gradients(tower_grads):\n",
    "    average_grads = []\n",
    "    for grad_and_vars in zip(*tower_grads):\n",
    "        # Note that each grad_and_vars looks like the following:\n",
    "        #   ((grad0_gpu0, var0_gpu0), ... , (grad0_gpuN, var0_gpuN))\n",
    "        grads = []\n",
    "        for g, _ in grad_and_vars:\n",
    "            # Add 0 dimension to the gradients to represent the tower.\n",
    "            expanded_g = tf.expand_dims(g, 0)\n",
    "\n",
    "            # Append on a 'tower' dimension which we will average over below.\n",
    "            grads.append(expanded_g)\n",
    "\n",
    "        # Average over the 'tower' dimension.\n",
    "        grad = tf.concat(grads, 0)\n",
    "        grad = tf.reduce_mean(grad, 0)\n",
    "\n",
    "        # Keep in mind that the Variables are redundant because they are shared\n",
    "        # across towers. So .. we will just return the first tower's pointer to\n",
    "        # the Variable.\n",
    "        v = grad_and_vars[0][1]\n",
    "        grad_and_var = (grad, v)\n",
    "        average_grads.append(grad_and_var)\n",
    "    return average_grads\n",
    "\n",
    "\n",
    "iterator_initializers = []\n",
    "\n",
    "with tf.device('/cpu:0'):\n",
    "    tower_grads = []\n",
    "\n",
    "    for i in range(num_gpus):\n",
    "        with tf.device('/gpu:{}'.format(i)):\n",
    "            daliset = dali_tf.DALIDataset(\n",
    "                pipeline=MnistPipeline(device_id=i, shard_id=i, num_shards=num_gpus),\n",
    "                batch_size=BATCH_SIZE,\n",
    "                shapes=shapes,\n",
    "                dtypes=dtypes,\n",
    "                device_id=i).with_options(options)\n",
    "\n",
    "            iterator = tf.data.make_initializable_iterator(daliset)\n",
    "            iterator_initializers.append(iterator.initializer)\n",
    "            images, labels = iterator.get_next()\n",
    "\n",
    "            images = tf.reshape(\n",
    "                images, [BATCH_SIZE, IMAGE_SIZE*IMAGE_SIZE])\n",
    "            labels = tf.reshape(\n",
    "                tf.one_hot(labels, NUM_CLASSES),\n",
    "                [BATCH_SIZE, NUM_CLASSES])\n",
    "\n",
    "            with tf.variable_scope('mnist_net', reuse=(i != 0)):\n",
    "                images = tf.layers.flatten(images)\n",
    "                images = tf.layers.dense(images, HIDDEN_SIZE, activation=tf.nn.relu)\n",
    "                images = tf.layers.dropout(images, rate=DROPOUT, training=True)\n",
    "                images = tf.layers.dense(images, NUM_CLASSES, activation=tf.nn.softmax)\n",
    "\n",
    "            logits_train = images\n",
    "\n",
    "            loss_op = tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits(\n",
    "                logits=logits_train, labels=labels))\n",
    "            optimizer = tf.train.AdamOptimizer()\n",
    "            grads = optimizer.compute_gradients(loss_op)\n",
    "\n",
    "            if i == 0:\n",
    "                correct_pred = tf.equal(\n",
    "                    tf.argmax(logits_train, 1), tf.argmax(labels, 1))\n",
    "                accuracy = tf.reduce_mean(\n",
    "                    tf.cast(correct_pred, tf.float32))\n",
    "\n",
    "            tower_grads.append(grads)\n",
    "\n",
    "    tower_grads = average_gradients(tower_grads)\n",
    "    train_step = optimizer.apply_gradients(tower_grads)\n",
    "\n",
    "\n",
    "with tf.Session() as sess:\n",
    "        sess.run(tf.global_variables_initializer())\n",
    "        sess.run(iterator_initializers)\n",
    "\n",
    "        for i in range(EPOCHS * ITERATIONS):\n",
    "            sess.run(train_step)\n",
    "            if i % ITERATIONS == 0:\n",
    "                train_accuracy = sess.run(accuracy)\n",
    "                print(\"Step %d, accuracy: %g\" % (i, train_accuracy))\n",
    "\n",
    "        final_accuracy = 0\n",
    "        for _ in range(ITERATIONS):\n",
    "            final_accuracy = final_accuracy + \\\n",
    "                accuracy.eval()\n",
    "        final_accuracy = final_accuracy / ITERATIONS\n",
    "\n",
    "        print('Final accuracy: ', final_accuracy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
