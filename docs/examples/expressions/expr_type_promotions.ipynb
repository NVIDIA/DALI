{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# DALI expressions and arithmetic operators\n",
    "\n",
    "In this example, we will see how to use arithmetic operators in DALI Pipeline that alow for elementwise operations on tensors inside a pipeline. We will explain the type promotion rules and show examples of using currently supported arithmetic operators, namely `+`, `-`, `*`, `/` and `//`."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Prepare the test pipeline\n",
    "\n",
    "First, we will prepare helper code, so we can easily manipulate the types and values that will appear as tensors in DALI pipeline.\n",
    "\n",
    "We use `from __future__ import division` to allow `/` and `//` as true division and floor division operators.\n",
    "We will be using numpy as source for the custom provided data and we also need to import several things from DALI needed to create Pipeline and use ExternalSource Operator. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from __future__ import division\n",
    "import numpy as np\n",
    "from nvidia.dali.pipeline import Pipeline\n",
    "import nvidia.dali.ops as ops            \n",
    "import nvidia.dali.types as types\n",
    "from nvidia.dali.types import Constant\n",
    "\n",
    "batch_size = 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Defining the data\n",
    "\n",
    "As we are dealing with binary operators, we need two inputs. \n",
    "We will create a simple helper function that returns two numpy arrays of given numpy types with arbitrary selected values. It is to make the manipulation of types easy. In an actual scenario the data processed by DALI arithmetic operators would be tensors produced by other Operator containing some images, video sequences or other data.\n",
    "\n",
    "Keep in mind that shapes of both inputs need to match as those will be elementwise operations. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "left_magic_values = [42, 8]\n",
    "right_magic_values = [9, 2]\n",
    "\n",
    "def get_data(left_type, right_type):\n",
    "    return ([left_type(left_magic_values)], [right_type(right_magic_values)])\n",
    "\n",
    "batch_size = 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Defining the pipeline\n",
    "\n",
    "The next step is to define the Pipeline. We override `Pipeline.iter_setup`, a method called by the pipeline before every `Pipeline.run`. It is meant to feed the data into `ExternalSource()` operators indicated by `self.left` and `self.right`.\n",
    "The data will be obtained from `get_data` function to which we pass the left and right types. \n",
    "\n",
    "Note, that we do not need to instantiate any additional operators, we can use regular Python arithmetic expressions on the results of other operators in the `define_graph` step.\n",
    "\n",
    "For convenience, we'll wrap the usage of arithmetic operations in a lambda called `operation`, specified when creating the pipeline.\n",
    "\n",
    "`define_graph` will return both our data inputs and the result of applying `operation` to them."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    " class ArithmeticPipeline(Pipeline):                   \n",
    "    def __init__(self, operation, left_type, right_type, batch_size, num_threads, device_id):\n",
    "        super(ArithmeticPipeline, self).__init__(batch_size, num_threads, device_id, seed=12)\n",
    "        self.left_source = ops.ExternalSource()\n",
    "        self.right_source = ops.ExternalSource()\n",
    "        self.operation = operation\n",
    "        self.left_type = left_type\n",
    "        self.right_type = right_type\n",
    "\n",
    "    def define_graph(self):                                                                \n",
    "        self.left = self.left_source()\n",
    "        self.right = self.right_source()\n",
    "        return self.left, self.right, self.operation(self.left, self.right)\n",
    "\n",
    "    def iter_setup(self):\n",
    "        (l, r) = get_data(self.left_type, self.right_type)\n",
    "        self.feed_input(self.left, l)\n",
    "        self.feed_input(self.right, r)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Experiment with the arithmetic operators\n",
    "\n",
    "### Instantiating the Pipeline\n",
    "\n",
    "We create instances of the `ArithmeticPipeline` with different type combinations. \n",
    "\n",
    "Type promotions for binary operators are described below. The type promotion rules are commutative. They apply to `+`, `-`, `*` and `//`. The `/` always returns a float32 for integer inputs, and applies the rules below when at least one of the inputs is a floating point number.\n",
    "\n",
    "| Operand Type | Operand Type | Result Type | Additional Conditions |\n",
    "|:------------:|:------------:|:-----------:| --------------------- |\n",
    "| T      | T      | T                |                        |\n",
    "| floatX | T      | floatX           | where T is not a float |\n",
    "| floatX | floatY | float(max(X, Y)) |                        |\n",
    "| intX   | intY   | int(max(X, Y))   |                        |\n",
    "| uintX  | uintY  | uint(max(X, Y))  |                        |\n",
    "| intX   | uintY  | int2Y            | if X <= Y              |\n",
    "| intX   | uintY  | intX             | if X > Y               |"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Using the Pipeline\n",
    "\n",
    "Let's create a Pipeline that adds two tensors of type `uint8`, run it and see the results."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[42  8]] + [[9 2]] = [[51 10]]; \t\t\t with types uint8 + uint8 -> uint8\n"
     ]
    }
   ],
   "source": [
    "def build_and_run(pipe, op_name):\n",
    "    pipe.build()                                                        \n",
    "    pipe_out = pipe.run()\n",
    "    l = pipe_out[0].as_array()\n",
    "    r = pipe_out[1].as_array()\n",
    "    out = pipe_out[2].as_array()\n",
    "    print(\"{} {} {} = {}; \\t\\t\\t with types {} {} {} -> {}\".format(l, op_name, r, out, l.dtype, op_name, r.dtype, out.dtype))\n",
    "    \n",
    "pipe = ArithmeticPipeline((lambda x, y: x + y), np.uint8, np.uint8, batch_size = batch_size, num_threads = 2, device_id = 0)\n",
    "build_and_run(pipe, \"+\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's see how all of the operators behave with different type combinations by generalizing the example above.\n",
    "You can use the `np_types` in the loops to see all possible type combinations. To reduce the output we limit ourselves to only few of them."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[42  8]] + [[9 2]] = [[51 10]]; \t\t\t with types uint8 + uint8 -> uint8\n",
      "[[42  8]] + [[9 2]] = [[51 10]]; \t\t\t with types uint8 + int32 -> int32\n",
      "[[42  8]] + [[9. 2.]] = [[51. 10.]]; \t\t\t with types uint8 + float32 -> float32\n",
      "[[42  8]] - [[9 2]] = [[33  6]]; \t\t\t with types uint8 - uint8 -> uint8\n",
      "[[42  8]] - [[9 2]] = [[33  6]]; \t\t\t with types uint8 - int32 -> int32\n",
      "[[42  8]] - [[9. 2.]] = [[33.  6.]]; \t\t\t with types uint8 - float32 -> float32\n",
      "[[42  8]] * [[9 2]] = [[122  16]]; \t\t\t with types uint8 * uint8 -> uint8\n",
      "[[42  8]] * [[9 2]] = [[378  16]]; \t\t\t with types uint8 * int32 -> int32\n",
      "[[42  8]] * [[9. 2.]] = [[378.  16.]]; \t\t\t with types uint8 * float32 -> float32\n",
      "[[42  8]] / [[9 2]] = [[4.6666665 4.       ]]; \t\t\t with types uint8 / uint8 -> float32\n",
      "[[42  8]] / [[9 2]] = [[4.6666665 4.       ]]; \t\t\t with types uint8 / int32 -> float32\n",
      "[[42  8]] / [[9. 2.]] = [[4.6666665 4.       ]]; \t\t\t with types uint8 / float32 -> float32\n",
      "[[42  8]] // [[9 2]] = [[4 4]]; \t\t\t with types uint8 // uint8 -> uint8\n",
      "[[42  8]] // [[9 2]] = [[4 4]]; \t\t\t with types uint8 // int32 -> int32\n",
      "[[42  8]] // [[9. 2.]] = [[4.6666665 4.       ]]; \t\t\t with types uint8 // float32 -> float32\n"
     ]
    }
   ],
   "source": [
    "arithmetic_operations = [((lambda x, y: x + y) , \"+\"), ((lambda x, y: x - y) , \"-\"),\n",
    "                         ((lambda x, y: x * y) , \"*\"), ((lambda x, y: x / y) , \"/\"),\n",
    "                         ((lambda x, y: x // y) , \"//\")]\n",
    "\n",
    "np_types = [np.int8, np.int16, np.int32, np.int64, \n",
    "            np.uint8, np.uint16, np.uint32, np.uint64,\n",
    "            np.float32, np.float64]\n",
    "\n",
    "for (op, op_name) in arithmetic_operations:\n",
    "    for left_type in [np.uint8]:\n",
    "        for right_type in [np.uint8, np.int32, np.float32]:\n",
    "            pipe = ArithmeticPipeline(op, left_type, right_type, batch_size=batch_size, num_threads=2, device_id = 0)\n",
    "            build_and_run(pipe, op_name)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Using Constants\n",
    "\n",
    "Instead of operating only on Tensor data, DALI expressions can also work with constants. Those can be either values of Python `int` and `float` types used directly, or those values wrapped in `nvidia.dali.types.Constant`. Operation between tensor and constant results in the constant being broadcasted to all elements of the tensor. The same costant is used with all samples in the batch.\n",
    "\n",
    "*Note: Currently all values of integral constants are passed to DALI as int32 and all values of floating point constants are passed to DALI as float32.*\n",
    "\n",
    "The Python `int` values will be treated as `int32` and the `float` as `float32` in regard to type promotions.\n",
    "\n",
    "The DALI `Constant` can be used to indicate other types. It accepts `DALIDataType` enum values as second argument and has convenience member functions like `.uint8()` or `.float32()` that can be used for conversions.\n",
    "\n",
    "As our expressions will consist of a tensor and a constant, we will adjust our previous pipeline and the helper functions - they only need to generate one tensor."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_magic_values = [[42, 8, 16, 0], [11, 20, 3, 7]]\n",
    "\n",
    "const_batch_size = len(batch_magic_values)\n",
    "\n",
    "def get_tensor(data_type):\n",
    "    return ([data_type(tensor) for tensor in batch_magic_values])\n",
    "\n",
    "\n",
    "\n",
    "class ArithmeticConstantsPipeline(Pipeline):                   \n",
    "    def __init__(self, operation, tensor_data_type,batch_size, num_threads, device_id):\n",
    "        super(ArithmeticConstantsPipeline, self).__init__(batch_size, num_threads, device_id, seed=12)\n",
    "        self.tensor_source = ops.ExternalSource()\n",
    "        self.operation = operation\n",
    "        self.tensor_data_type = tensor_data_type\n",
    "\n",
    "    def define_graph(self):                                                                \n",
    "        self.tensor = self.tensor_source()\n",
    "        return self.tensor, self.operation(self.tensor)\n",
    "\n",
    "    def iter_setup(self):\n",
    "        (t) = get_tensor(self.tensor_data_type)\n",
    "        self.feed_input(self.tensor, t)\n",
    "        \n",
    "def build_and_run_with_const(pipe, op_name, constant, is_const_left = False):\n",
    "    pipe.build()                                                        \n",
    "    pipe_out = pipe.run()\n",
    "    t_in = pipe_out[0].as_array()\n",
    "    t_out = pipe_out[1].as_array()\n",
    "    if is_const_left:\n",
    "        print(\"{} {} {} = \\n{}; \\nwith types {} {} {} -> {}\\n\".format(constant, op_name, t_in, t_out, type(constant), op_name, t_in.dtype, t_out.dtype))\n",
    "    else:\n",
    "        print(\"{} {} {} = \\n{}; \\nwith types {} {} {} -> {}\\n\".format(t_in, op_name, constant, t_out, t_in.dtype, op_name, type(constant), t_out.dtype))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, the `ArithmeticConstantsPipeline` can be parametrized with a function taking the only tensor and returning the result of arithmetic operation between that tensor and a constant.\n",
    "\n",
    "We also adjusted our print message.\n",
    "\n",
    "Now we will check all the examples we mentioned at the beginning: `int`, `float` constants and `nvidia.dali.types.Constant`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[42  8 16  0]\n",
      " [11 20  3  7]] + 10 = \n",
      "[[52 18 26 10]\n",
      " [21 30 13 17]]; \n",
      "with types uint8 + <type 'int'> -> int32\n",
      "\n",
      "[[42.  8. 16.  0.]\n",
      " [11. 20.  3.  7.]] + 10 = \n",
      "[[52. 18. 26. 10.]\n",
      " [21. 30. 13. 17.]]; \n",
      "with types float32 + <type 'int'> -> float32\n",
      "\n",
      "[[42  8 16  0]\n",
      " [11 20  3  7]] + 42.3 = \n",
      "[[84.3 50.3 58.3 42.3]\n",
      " [53.3 62.3 45.3 49.3]]; \n",
      "with types uint8 + <type 'float'> -> float32\n",
      "\n",
      "[[42.  8. 16.  0.]\n",
      " [11. 20.  3.  7.]] + 42.3 = \n",
      "[[84.3 50.3 58.3 42.3]\n",
      " [53.3 62.3 45.3 49.3]]; \n",
      "with types float32 + <type 'float'> -> float32\n",
      "\n"
     ]
    }
   ],
   "source": [
    "constant = 10\n",
    "pipe = ArithmeticConstantsPipeline((lambda x: x + constant), np.uint8, batch_size = const_batch_size, num_threads = 2, device_id = 0)\n",
    "build_and_run_with_const(pipe, \"+\", constant)\n",
    "\n",
    "constant = 10\n",
    "pipe = ArithmeticConstantsPipeline((lambda x: x + constant), np.float32, batch_size = const_batch_size, num_threads = 2, device_id = 0)\n",
    "build_and_run_with_const(pipe, \"+\", constant)\n",
    "\n",
    "\n",
    "constant = 42.3\n",
    "pipe = ArithmeticConstantsPipeline((lambda x: x + constant), np.uint8, batch_size = const_batch_size, num_threads = 2, device_id = 0)\n",
    "build_and_run_with_const(pipe, \"+\", constant)\n",
    "\n",
    "constant = 42.3\n",
    "pipe = ArithmeticConstantsPipeline((lambda x: x + constant), np.float32, batch_size = const_batch_size, num_threads = 2, device_id = 0)\n",
    "build_and_run_with_const(pipe, \"+\", constant)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As we can see the value of the constant is applied to all the elements of the tensor to which it is added.\n",
    "\n",
    "Now let's check how to use the DALI Constant wrapper.\n",
    "\n",
    "Passing an `int` or `float` to DALI Constant marks it as `int32` or `float32` respectively"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[42  8 16  0]\n",
      " [11 20  3  7]] * 10:DALIDataType.INT32 = \n",
      "[[420  80 160   0]\n",
      " [110 200  30  70]]; \n",
      "with types uint8 * <class 'nvidia.dali.types.Constant'> -> int32\n",
      "\n",
      "10.0:DALIDataType.FLOAT * [[42  8 16  0]\n",
      " [11 20  3  7]] = \n",
      "[[420.  80. 160.   0.]\n",
      " [110. 200.  30.  70.]]; \n",
      "with types <class 'nvidia.dali.types.Constant'> * uint8 -> float32\n",
      "\n"
     ]
    }
   ],
   "source": [
    "constant = Constant(10)\n",
    "pipe = ArithmeticConstantsPipeline((lambda x: x * constant), np.uint8, batch_size = const_batch_size, num_threads = 2, device_id = 0)\n",
    "build_and_run_with_const(pipe, \"*\", constant)\n",
    "\n",
    "\n",
    "constant = Constant(10.0)\n",
    "pipe = ArithmeticConstantsPipeline((lambda x: constant * x), np.uint8, batch_size = const_batch_size, num_threads = 2, device_id = 0)\n",
    "build_and_run_with_const(pipe, \"*\", constant, True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can either explicitly specify the type as a second argument, or use convenience conversion member functions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[42  8 16  0]\n",
      " [11 20  3  7]] * 10:DALIDataType.UINT8 = \n",
      "[[164  80 160   0]\n",
      " [110 200  30  70]]; \n",
      "with types uint8 * <class 'nvidia.dali.types.Constant'> -> uint8\n",
      "\n",
      "10:DALIDataType.UINT8 * [[42  8 16  0]\n",
      " [11 20  3  7]] = \n",
      "[[164  80 160   0]\n",
      " [110 200  30  70]]; \n",
      "with types <class 'nvidia.dali.types.Constant'> * uint8 -> uint8\n",
      "\n",
      "10:DALIDataType.UINT8 * [[42  8 16  0]\n",
      " [11 20  3  7]] = \n",
      "[[164  80 160   0]\n",
      " [110 200  30  70]]; \n",
      "with types <class 'nvidia.dali.types.Constant'> * uint8 -> uint8\n",
      "\n"
     ]
    }
   ],
   "source": [
    "constant = Constant(10, types.DALIDataType.UINT8)\n",
    "pipe = ArithmeticConstantsPipeline((lambda x: x * constant), np.uint8, batch_size = const_batch_size, num_threads = 2, device_id = 0)\n",
    "build_and_run_with_const(pipe, \"*\", constant)\n",
    "\n",
    "\n",
    "constant = Constant(10.0, types.DALIDataType.UINT8)\n",
    "pipe = ArithmeticConstantsPipeline((lambda x: constant * x), np.uint8, batch_size = const_batch_size, num_threads = 2, device_id = 0)\n",
    "build_and_run_with_const(pipe, \"*\", constant, True)\n",
    "\n",
    "\n",
    "constant = Constant(10).uint8()\n",
    "pipe = ArithmeticConstantsPipeline((lambda x: constant * x), np.uint8, batch_size = const_batch_size, num_threads = 2, device_id = 0)\n",
    "build_and_run_with_const(pipe, \"*\", constant, True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Treating tensors as scalars\n",
    "\n",
    "As an addition to elementwise operation between Tensors of the same sizes, DALI allows to use tensors containing scalar values - that is a batch of elements with shape `{1}` as one of the operands. The tensor of scalars will behave similarly to a constant described above - each scalar value will be broadcasted to every element of the other tensor operand. Note that contrary to constants, the tensor is still as a batch of elements - each scalar value is broadcasted to corresponding tensor.\n",
    "\n",
    "We will use `Uniform` Operator to generate a batch of random scalar values, that we will use in our example.\n",
    "Let's adjust our example to have inputs of bit more complex shape, for example `{2 x 3}`.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_magic_values = [[[42, 8, 16], [0, 11, 3]], [[11, 20, 15], [7, 99, 101]]]\n",
    "\n",
    "scalar_batch_size = len(batch_magic_values)\n",
    "\n",
    "def get_tensor_v2(data_type):\n",
    "    return ([data_type(tensor) for tensor in batch_magic_values])\n",
    "\n",
    "class ArithmeticScalarsPipeline(Pipeline):                   \n",
    "    def __init__(self, operation, tensor_data_type,batch_size, num_threads, device_id):\n",
    "        super(ArithmeticScalarsPipeline, self).__init__(batch_size, num_threads, device_id, seed=12)\n",
    "        self.tensor_source = ops.ExternalSource()\n",
    "        self.uniform = ops.Uniform(range=[-10, 10])\n",
    "        self.operation = operation\n",
    "        self.tensor_data_type = tensor_data_type\n",
    "\n",
    "    def define_graph(self):                                                                \n",
    "        self.tensor = self.tensor_source()\n",
    "        uni = self.uniform()\n",
    "        return self.tensor, uni, self.operation(self.tensor, uni)\n",
    "\n",
    "    def iter_setup(self):\n",
    "        (t) = get_tensor_v2(self.tensor_data_type)\n",
    "        self.feed_input(self.tensor, t)\n",
    "        \n",
    "def build_and_run_with_scalar(pipe, op_name):\n",
    "    pipe.build()                                                        \n",
    "    pipe_out = pipe.run()\n",
    "    l = pipe_out[0].as_array()\n",
    "    r = pipe_out[1].as_array()\n",
    "    out = pipe_out[2].as_array()\n",
    "    print(\"{} {}\\n{} =\\n{}; \\nwith types {} {} {} -> {} and shapes {} {} {} -> {}\"\n",
    "          .format(l, op_name, r, out, l.dtype, op_name, r.dtype, out.dtype, l.shape, op_name, r.shape, out.shape))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We introduced `Uniform` Operator to `ArithmeticScalarsPipeline`. We're back with lambdas that accept two inputs - the right one will be our batch of scalars.\n",
    "We also adjusted our print message.\n",
    "\n",
    "Now, let's check few possible combinations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[[ 42   8  16]\n",
      "  [  0  11   3]]\n",
      "\n",
      " [[ 11  20  15]\n",
      "  [  7  99 101]]] *\n",
      "[[3.9766016]\n",
      " [0.666193 ]] =\n",
      "[[[167.01727   31.812813  63.625626]\n",
      "  [  0.        43.74262   11.929805]]\n",
      "\n",
      " [[  7.328123  13.32386    9.992895]\n",
      "  [  4.663351  65.95311   67.28549 ]]]; \n",
      "with types int32 * float32 -> float32 and shapes (2, 2, 3) * (2, 1) -> (2, 2, 3)\n"
     ]
    }
   ],
   "source": [
    "pipe = ArithmeticScalarsPipeline((lambda x, y: x * y), np.int32, batch_size = scalar_batch_size, num_threads = 2, device_id = 0)\n",
    "build_and_run_with_scalar(pipe, \"*\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[[ 42.   8.  16.]\n",
      "  [  0.  11.   3.]]\n",
      "\n",
      " [[ 11.  20.  15.]\n",
      "  [  7.  99. 101.]]] +\n",
      "[[3.9766016]\n",
      " [0.666193 ]] =\n",
      "[[[ 45.9766     11.976602   19.9766   ]\n",
      "  [  3.9766016  14.976602    6.9766016]]\n",
      "\n",
      " [[ 11.666193   20.666193   15.666193 ]\n",
      "  [  7.666193   99.66619   101.66619  ]]]; \n",
      "with types float32 + float32 -> float32 and shapes (2, 2, 3) + (2, 1) -> (2, 2, 3)\n"
     ]
    }
   ],
   "source": [
    "pipe = ArithmeticScalarsPipeline((lambda x, y: x + y), np.float32, batch_size = scalar_batch_size, num_threads = 2, device_id = 0)\n",
    "build_and_run_with_scalar(pipe, \"+\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Notice how we add or multiply corresponding elements of two batches (consisting of two elements here). Their shapes do not match, but as the second one operand is a batch two of `{1}`-shaped tensors, it is considered a scalar input."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
