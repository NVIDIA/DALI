{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Loading Bottleneck Detection\n",
    "\n",
    "You have a PyTorch model, a dataset, and you're training it. But you're wondering: \"Is my training being slowed down by data loading?\" This is a common question in deep learning. When your GPU is waiting for data to be loaded and preprocessed, you're not getting the full performance out of your expensive hardware.\n",
    "\n",
    "This quick-start guide shows how easy it is to detect data loading bottlenecks by simply wrapping your existing dataloader.\n",
    "\n",
    "In this tutorial, you will learn how to:\n",
    "- Set up a baseline training scenario\n",
    "- Identify if data loading is the bottleneck\n",
    "- Measure the potential performance gain if data loading were optimized"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Setup\n",
    "First, let's import the necessary libraries and set up our environment."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: cuda\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import time\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "from torchvision import datasets, transforms\n",
    "from tqdm import tqdm\n",
    "import numpy as np\n",
    "\n",
    "from nvidia.dali.plugin.pytorch.loader_evaluator import LoaderEvaluator\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(f\"Using device: {device}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Model & Data Setup\n",
    "\n",
    "We'll use an ultra-light model to make data loading the bottleneck. This helps us clearly see the impact of data loading performance.\n",
    "\n",
    "`DALI_EXTRA_PATH` environment variable should point to the place where data from [DALI extra repository](https://github.com/NVIDIA/DALI_extra) is downloaded. Please make sure that the proper release tag is checked out."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset size: 1000\n"
     ]
    }
   ],
   "source": [
    "# Our model\n",
    "class UltraLightModel(nn.Module):\n",
    "    def __init__(self, num_classes=1000):\n",
    "        super(UltraLightModel, self).__init__()\n",
    "        self.classifier = nn.Linear(3 * 224 * 224, num_classes)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = torch.flatten(x, 1)\n",
    "        x = self.classifier(x)\n",
    "        return x\n",
    "\n",
    "\n",
    "# As our toy dataset is small, we will use RepeatedDataset to artificially enlarge it\n",
    "class RepeatedDataset(Dataset):\n",
    "    def __init__(self, base_ds, target_len: int):\n",
    "        self.base = base_ds\n",
    "        self.target_len = target_len\n",
    "\n",
    "    def __len__(self):\n",
    "        return self.target_len\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        # Map any idx into the valid range of the base dataset\n",
    "        return self.base[idx % len(self.base)]\n",
    "\n",
    "\n",
    "# Dataloader\n",
    "def create_dataloader(data_path, batch_size=32, num_workers=4, target_len=1000):\n",
    "    transform = transforms.Compose(\n",
    "        [\n",
    "            transforms.Resize(256),\n",
    "            transforms.CenterCrop(224),\n",
    "            transforms.ToTensor(),\n",
    "            transforms.Normalize(\n",
    "                mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225]\n",
    "            ),\n",
    "        ]\n",
    "    )\n",
    "\n",
    "    dataset = datasets.ImageFolder(root=data_path, transform=transform)\n",
    "    if target_len > len(dataset):\n",
    "        dataset = RepeatedDataset(dataset, target_len)\n",
    "    dataloader = DataLoader(\n",
    "        dataset,\n",
    "        batch_size=batch_size,\n",
    "        shuffle=True,\n",
    "        num_workers=num_workers,\n",
    "        pin_memory=True,\n",
    "    )\n",
    "    return dataloader\n",
    "\n",
    "\n",
    "# Create your dataloader; for simplicity we'll use a small vision dataset\n",
    "test_data_root = os.environ[\"DALI_EXTRA_PATH\"]\n",
    "test_data_path = os.path.join(test_data_root, \"db\", \"single\", \"jpeg\")\n",
    "\n",
    "dataloader = create_dataloader(\n",
    "    test_data_path, batch_size=32, num_workers=4, target_len=1000\n",
    ")\n",
    "print(f\"Dataset size: {len(dataloader.dataset)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Training Function\n",
    "\n",
    "Now let's create a training function that will train our ultra-light model and collect performance metrics."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_one_epoch(model, dataloader, criterion, optimizer, device, epoch=0):\n",
    "    model.train()\n",
    "\n",
    "    epoch_start_time = time.time()\n",
    "    progress_bar = tqdm(dataloader, desc=f\"Epoch {epoch}\")\n",
    "\n",
    "    for data, target in progress_bar:\n",
    "        data, target = data.to(device), target.to(device)\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        output = model(data)\n",
    "        loss = criterion(output, target)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        progress_bar.set_postfix(\n",
    "            {\"Time\": f\"{time.time() - epoch_start_time:.1f}s\"}\n",
    "        )\n",
    "\n",
    "    epoch_time = time.time() - epoch_start_time\n",
    "    print(f\"Epoch {epoch} - Time: {epoch_time:.2f}s\")\n",
    "\n",
    "    return {\"epoch\": epoch, \"epoch_time\": epoch_time}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Baseline Training\n",
    "\n",
    "Now let's set up the training loop and run our baseline training to see how the ultra-light model performs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Baseline Training (Real Data Loading)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 0: 100%|██████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 32/32 [00:01<00:00, 16.38it/s, Time=1.9s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0 - Time: 1.96s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 1: 100%|██████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 32/32 [00:01<00:00, 16.64it/s, Time=1.9s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1 - Time: 1.92s\n",
      "Baseline average epoch time: 1.94s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "# Your existing training setup\n",
    "model = UltraLightModel(num_classes=1000).to(device)\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = optim.Adam(model.parameters(), lr=0.001)\n",
    "\n",
    "# Train with your existing dataloader\n",
    "print(\"Baseline Training (Real Data Loading)\")\n",
    "baseline_metrics = []\n",
    "for epoch in range(2):\n",
    "    metrics = train_one_epoch(\n",
    "        model, dataloader, criterion, optimizer, device, epoch\n",
    "    )\n",
    "    baseline_metrics.append(metrics)\n",
    "\n",
    "baseline_avg_time = np.mean([m[\"epoch_time\"] for m in baseline_metrics])\n",
    "print(f\"Baseline average epoch time: {baseline_avg_time:.2f}s\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. No-Overhead Training (One Line Change!)\n",
    "\n",
    "Now let's use the Data Loader Evaluator Tool to simulate ideal data loading performance that doesn't impact training speed (its overhead is close to 0). This will help us determine if our training is data loading bottlenecked and whether we can improve its performance by accelerating the data loading part."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Wrap your dataloader with LoaderEvaluator (this is the only change!)\n",
    "dataloader = LoaderEvaluator(\n",
    "    dataloader, mode=\"replay\", num_cached_batches=len(dataloader) // 10\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now let's train with the \"no-overhead\" dataloader to see the performance difference."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "No-Overhead Training (Cached Data Loading)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 0: 100%|██████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 32/32 [00:00<00:00, 66.40it/s, Time=0.5s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0 - Time: 0.48s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 1: 100%|██████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 32/32 [00:00<00:00, 64.63it/s, Time=0.5s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1 - Time: 0.50s\n",
      "No-Overhead average epoch time: 0.49s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "# Train with the same setup, just different dataloader\n",
    "print(\"No-Overhead Training (Cached Data Loading)\")\n",
    "sol_metrics = []\n",
    "for epoch in range(2):\n",
    "    metrics = train_one_epoch(\n",
    "        model, dataloader, criterion, optimizer, device, epoch\n",
    "    )\n",
    "    sol_metrics.append(metrics)\n",
    "\n",
    "sol_avg_time = np.mean([m[\"epoch_time\"] for m in sol_metrics])\n",
    "print(f\"No-Overhead average epoch time: {sol_avg_time:.2f}s\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Results - Is Data Loading Your Bottleneck?\n",
    "\n",
    "Let's compare the performance between baseline training and no-overhead training to determine if we have a data loading bottleneck."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Performance Comparison:\n",
      "Baseline:     1.94s per epoch\n",
      "No-Overhead:  0.49s per epoch\n",
      "Speedup:      3.96x\n",
      "Time saved:   1.45s per epoch (74.8%)\n",
      "\n",
      "*** DATA LOADING BOTTLENECK DETECTED ***\n",
      "You could speed up training by 74.8% by optimizing data loading.\n"
     ]
    }
   ],
   "source": [
    "# Compare performance\n",
    "speedup = baseline_avg_time / sol_avg_time\n",
    "time_reduction = baseline_avg_time - sol_avg_time\n",
    "reduction_percentage = (time_reduction / baseline_avg_time) * 100\n",
    "\n",
    "print(f\"\\nPerformance Comparison:\")\n",
    "print(f\"Baseline:     {baseline_avg_time:.2f}s per epoch\")\n",
    "print(f\"No-Overhead:  {sol_avg_time:.2f}s per epoch\")\n",
    "print(f\"Speedup:      {speedup:.2f}x\")\n",
    "print(\n",
    "    f\"Time saved:   {time_reduction:.2f}s per epoch ({reduction_percentage:.1f}%)\"\n",
    ")\n",
    "\n",
    "# Bottleneck detection\n",
    "if speedup > 1.5:\n",
    "    print(f\"\\n*** DATA LOADING BOTTLENECK DETECTED ***\")\n",
    "    print(\n",
    "        f\"You could speed up training by {reduction_percentage:.1f}% by optimizing data loading.\"\n",
    "    )\n",
    "elif speedup > 1.1:\n",
    "    print(f\"\\n** POTENTIAL DATA LOADING BOTTLENECK **\")\n",
    "    print(f\"Consider optimizing data loading for better performance.\")\n",
    "else:\n",
    "    print(f\"\\n** NO DATA LOADING BOTTLENECK **\")\n",
    "    print(f\"Your training is not significantly limited by data loading.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## That's It!\n",
    "\n",
    "**Summary:**\n",
    "- Wrap your existing dataloader: `LoaderEvaluator(your_dataloader, mode=\"replay\")`\n",
    "- Run the same training code\n",
    "- Compare performance to detect bottlenecks\n",
    "\n",
    "**Next Steps:**\n",
    "- If bottleneck detected: optimize your data loading (increase `num_workers`, use faster storage, etc.)\n",
    "- If no bottleneck: focus optimization efforts elsewhere\n",
    "\n",
    "**Key Insight:** If no-overhead training is significantly faster, your data loading is the bottleneck!\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
