{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Using DALI in PyTorch Lightning\n",
    "\n",
    "### Overview\n",
    "\n",
    "This example shows how to use DALI in PyTorch Lightning.\n",
    "\n",
    "Let us grab [a toy example](https://pytorch-lightning.readthedocs.io/en/latest/introduction_guide.html) of the clasification network and let us see how DALI can accelerate it.\n",
    "\n",
    "DALI_EXTRA_PATH environment variable should point to the place where data from DALI extra repository is downloaded. Please make sure that the proper release tag is checked out."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch.nn import functional as F\n",
    "from torch import nn\n",
    "from pytorch_lightning.core.lightning import LightningModule\n",
    "from pytorch_lightning import Trainer\n",
    "from torch.optim import Adam\n",
    "from torchvision.datasets import MNIST\n",
    "from torchvision import datasets, transforms\n",
    "from torch.utils.data import DataLoader, random_split\n",
    "\n",
    "import os\n",
    "\n",
    "BATCH_SIZE = 64"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now let us implement the bare training class with the native data loader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "class LitMNIST(LightningModule):\n",
    "\n",
    "  def __init__(self):\n",
    "    super().__init__()\n",
    "\n",
    "    # mnist images are (1, 28, 28) (channels, width, height)\n",
    "    self.layer_1 = torch.nn.Linear(28 * 28, 128)\n",
    "    self.layer_2 = torch.nn.Linear(128, 256)\n",
    "    self.layer_3 = torch.nn.Linear(256, 10)\n",
    "\n",
    "  def forward(self, x):\n",
    "    batch_size, channels, width, height = x.size()\n",
    "\n",
    "    # (b, 1, 28, 28) -> (b, 1*28*28)\n",
    "    x = x.view(batch_size, -1)\n",
    "    x = self.layer_1(x)\n",
    "    x = F.relu(x)\n",
    "    x = self.layer_2(x)\n",
    "    x = F.relu(x)\n",
    "    x = self.layer_3(x)\n",
    "\n",
    "    x = F.log_softmax(x, dim=1)\n",
    "    return x\n",
    "\n",
    "  def process_batch(self, batch):\n",
    "      return batch\n",
    "\n",
    "  def training_step(self, batch, batch_idx):\n",
    "      x, y = self.process_batch(batch)\n",
    "      logits = self(x)\n",
    "      loss = F.nll_loss(logits, y)\n",
    "      return loss\n",
    "\n",
    "  def cross_entropy_loss(self, logits, labels):\n",
    "      return F.nll_loss(logits, labels)\n",
    "\n",
    "  def configure_optimizers(self):\n",
    "      return Adam(self.parameters(), lr=1e-3)\n",
    "\n",
    "  def prepare_data(self):# transforms for images\n",
    "      transform=transforms.Compose([transforms.ToTensor(), \n",
    "                                    transforms.Normalize((0.1307,), (0.3081,))])\n",
    "      self.mnist_train = MNIST(os.getcwd(), train=True, download=True, transform=transform)\n",
    "\n",
    "  def train_dataloader(self):\n",
    "       return DataLoader(self.mnist_train, batch_size=64, num_workers=8, pin_memory=True)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And see how it works"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "GPU available: True, used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "/usr/local/lib/python3.6/dist-packages/pytorch_lightning/utilities/distributed.py:45: UserWarning: WORLD_SIZE environment variable (2) is not equal to the computed world size (1). Ignored.\n",
      "  warnings.warn(*args, **kwargs)\n",
      "initializing ddp: GLOBAL_RANK: 0, MEMBER: 1/1\n",
      "/usr/local/lib/python3.6/dist-packages/torch/cuda/__init__.py:125: UserWarning: \n",
      "GeForce GT 710 with CUDA capability sm_35 is not compatible with the current PyTorch installation.\n",
      "The current PyTorch install supports CUDA capabilities sm_37 sm_50 sm_60 sm_70 sm_75.\n",
      "If you want to use the GeForce GT 710 GPU with PyTorch, please check the instructions at https://pytorch.org/get-started/locally/\n",
      "\n",
      "  warnings.warn(incompatible_device_warn.format(device_name, capability, \" \".join(arch_list), device_name))\n",
      "\n",
      "  | Name    | Type   | Params\n",
      "-----------------------------------\n",
      "0 | layer_1 | Linear | 100 K \n",
      "1 | layer_2 | Linear | 33 K  \n",
      "2 | layer_3 | Linear | 2 K   \n",
      "WARNING: Logging before flag parsing goes to stderr.\n",
      "I1014 17:16:18.776570 139719591925568 lightning.py:1215] \n",
      "  | Name    | Type   | Params\n",
      "-----------------------------------\n",
      "0 | layer_1 | Linear | 100 K \n",
      "1 | layer_2 | Linear | 33 K  \n",
      "2 | layer_3 | Linear | 2 K   \n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a6d6ed102f5f4bfe92e0f9361e33662f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(HTML(value='Training'), FloatProgress(value=1.0, bar_style='info', layout=Layout(flex='2'), maxâ€¦"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "1"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model = LitMNIST()\n",
    "trainer = Trainer(gpus=1, distributed_backend=\"ddp\", max_epochs=5)\n",
    "trainer.fit(model)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now let us define a DALI pipeline which would load the data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nvidia.dali as dali\n",
    "from nvidia.dali.pipeline import Pipeline\n",
    "import nvidia.dali.ops as ops\n",
    "import nvidia.dali.types as types\n",
    "from nvidia.dali.plugin.pytorch import DALIClassificationIterator\n",
    "\n",
    "# Path to MNIST dataset\n",
    "data_path = os.path.join(os.environ['DALI_EXTRA_PATH'], 'db/MNIST/training/')\n",
    "\n",
    "class MnistPipeline(Pipeline):\n",
    "    def __init__(self, batch_size, device, device_id=0, shard_id=0, num_shards=1, num_threads=4, seed=0):\n",
    "        super(MnistPipeline, self).__init__(\n",
    "            batch_size, num_threads, device_id, seed)\n",
    "        self.device = device\n",
    "        self.reader = ops.Caffe2Reader(path=data_path, shard_id=shard_id, num_shards=num_shards, random_shuffle=True)\n",
    "        self.decode = ops.ImageDecoder(\n",
    "            device='mixed' if device == 'gpu' else 'cpu',\n",
    "            output_type=types.GRAY)\n",
    "        self.cmn = ops.CropMirrorNormalize(\n",
    "            device=device,\n",
    "            dtype=types.FLOAT,\n",
    "            std=[0.3081 * 255],\n",
    "            mean=[0.1307 * 255],\n",
    "            output_layout=\"CHW\")\n",
    "        self.to_int64 = ops.Cast(dtype=types.INT64, device=device)\n",
    "\n",
    "    def define_graph(self):\n",
    "        inputs, labels = self.reader(name=\"Reader\")\n",
    "        images = self.decode(inputs)\n",
    "        images = self.cmn(images)\n",
    "        if self.device == \"gpu\":\n",
    "            labels = labels.gpu()\n",
    "        # PyTorch expects labels as INT64\n",
    "        labels = self.to_int64(labels)\n",
    "\n",
    "        return (images, labels)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Add DALI to the data preparation step in the training class and adjust the `process_batch` to accept the data returned by DALIClassificationIterator, which returns a list of dictionaries, where each list element corresponds to one pipeline wrapped by the DALIIterator, and entries in the dictionary corresponds to the relevant outputs. Check for details in the DALIGenericIterator documenation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DALILitMNIST(LitMNIST):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "    \n",
    "    def prepare_data(self):\n",
    "        device_id = self.local_rank\n",
    "        shard_id = self.global_rank\n",
    "        num_shards = self.trainer.world_size\n",
    "        mnist_pipeline = MnistPipeline(BATCH_SIZE, device='cpu', device_id=device_id, shard_id=shard_id,\n",
    "                                       num_shards=num_shards, num_threads=8)\n",
    "        self.train_loader = DALIClassificationIterator(mnist_pipeline, reader_name=\"Reader\",\n",
    "                                                       fill_last_batch=False, auto_reset=True)                          \n",
    "    def train_dataloader(self):\n",
    "        return self.train_loader\n",
    "    \n",
    "    def process_batch(self, batch):\n",
    "        x = batch[0][\"data\"]\n",
    "        y = batch[0][\"label\"].squeeze().cuda().long()\n",
    "        return (x, y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, let us run the training:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "GPU available: True, used: True\n",
      "I1014 17:16:44.541009 139719591925568 distributed.py:49] GPU available: True, used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "I1014 17:16:44.543509 139719591925568 distributed.py:49] TPU available: False, using: 0 TPU cores\n",
      "Using environment variable NODE_RANK for node rank (0).\n",
      "I1014 17:16:44.545432 139719591925568 distributed.py:49] Using environment variable NODE_RANK for node rank (0).\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "I1014 17:16:44.547166 139719591925568 accelerator_connector.py:333] LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "\n",
      "  | Name    | Type   | Params\n",
      "-----------------------------------\n",
      "0 | layer_1 | Linear | 100 K \n",
      "1 | layer_2 | Linear | 33 K  \n",
      "2 | layer_3 | Linear | 2 K   \n",
      "I1014 17:16:44.609724 139719591925568 lightning.py:1215] \n",
      "  | Name    | Type   | Params\n",
      "-----------------------------------\n",
      "0 | layer_1 | Linear | 100 K \n",
      "1 | layer_2 | Linear | 33 K  \n",
      "2 | layer_3 | Linear | 2 K   \n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c88a3c2ba1634cf0b89f6a188079d7e1",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(HTML(value='Training'), FloatProgress(value=1.0, bar_style='info', layout=Layout(flex='2'), maxâ€¦"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "1"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model = DALILitMNIST()\n",
    "trainer = Trainer(gpus=1, distributed_backend=\"ddp\", max_epochs=5)\n",
    "trainer.fit(model)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now let us provide the custom DALI iterator wrapper so we don't have to do any extra processing inside `LitMNIST.process_batch`, also PyTorch can learn how big is the dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "class BetterDALILitMNIST(LitMNIST):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "    \n",
    "    def prepare_data(self):\n",
    "        device_id = self.local_rank\n",
    "        shard_id = self.global_rank\n",
    "        num_shards = self.trainer.world_size\n",
    "        mnist_pipeline = MnistPipeline(BATCH_SIZE, device='cpu', device_id=device_id, shard_id=shard_id, num_shards=num_shards, num_threads=8)\n",
    "\n",
    "        class LightingWrapper(DALIClassificationIterator):\n",
    "            def __init__(self, *kargs, **kvargs):\n",
    "                super().__init__(*kargs, **kvargs)\n",
    "            \n",
    "            def __len__(self):\n",
    "               return self.size // self.batch_size\n",
    "    \n",
    "            def __next__(self):\n",
    "                out = super().__next__()\n",
    "                # DALIClassificationIterator calls next during the construction\n",
    "                # so first brach would be already converted to a list not a dict, no need to post process it\n",
    "                if isinstance(out[0], dict):\n",
    "                    # DDP is used so only one pipeline per process\n",
    "                    # also we need to transform dict returned by DALIClassificationIterator to iterable\n",
    "                    # and squeeze the lables\n",
    "                    out = out[0]\n",
    "                    return [out[k] if k != \"label\" else torch.squeeze(out[k]) for k in self.output_map]\n",
    "                else:\n",
    "                    return out\n",
    "\n",
    "        self.train_loader = LightingWrapper(mnist_pipeline, reader_name=\"Reader\", fill_last_batch=False, auto_reset=True)\n",
    "    def train_dataloader(self):\n",
    "        return self.train_loader"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, let us rerun the training:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "GPU available: True, used: True\n",
      "I1014 17:17:07.718175 139719591925568 distributed.py:49] GPU available: True, used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "I1014 17:17:07.720351 139719591925568 distributed.py:49] TPU available: False, using: 0 TPU cores\n",
      "Using environment variable NODE_RANK for node rank (0).\n",
      "I1014 17:17:07.722338 139719591925568 distributed.py:49] Using environment variable NODE_RANK for node rank (0).\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "I1014 17:17:07.724163 139719591925568 accelerator_connector.py:333] LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "\n",
      "  | Name    | Type   | Params\n",
      "-----------------------------------\n",
      "0 | layer_1 | Linear | 100 K \n",
      "1 | layer_2 | Linear | 33 K  \n",
      "2 | layer_3 | Linear | 2 K   \n",
      "I1014 17:17:07.770608 139719591925568 lightning.py:1215] \n",
      "  | Name    | Type   | Params\n",
      "-----------------------------------\n",
      "0 | layer_1 | Linear | 100 K \n",
      "1 | layer_2 | Linear | 33 K  \n",
      "2 | layer_3 | Linear | 2 K   \n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6496abbe3fb04d70af70912812be8cd2",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(HTML(value='Training'), FloatProgress(value=1.0, bar_style='info', layout=Layout(flex='2'), maxâ€¦"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "1"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model = BetterDALILitMNIST()\n",
    "trainer = Trainer(gpus=1, distributed_backend=\"ddp\", max_epochs=5)\n",
    "trainer.fit(model)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
