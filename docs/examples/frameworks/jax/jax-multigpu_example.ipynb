{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Using DALI with JAX\n",
    "\n",
    "This simple example shows how to train a neural network implemented in JAX with DALI pipelines. It builds on MNIST training example from JAX codebse that can be found [here](https://github.com/google/jax/blob/jax-v0.4.13/examples/mnist_classifier_fromscratch.py).\n",
    "\n",
    "We will use MNIST in Caffe2 format from [DALI_extra](https://github.com/NVIDIA/DALI_extra)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-07-28T07:43:41.850101Z",
     "iopub.status.busy": "2023-07-28T07:43:41.849672Z",
     "iopub.status.idle": "2023-07-28T07:43:41.853520Z",
     "shell.execute_reply": "2023-07-28T07:43:41.852990Z"
    }
   },
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "training_data_path = os.path.join(os.environ['DALI_EXTRA_PATH'], 'db/MNIST/training/')\n",
    "validation_data_path = os.path.join(os.environ['DALI_EXTRA_PATH'], 'db/MNIST/testing/')"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First step is to create a pipeline definition function that will later be used to create instances of DALI pipelines. It defines all steps of the preprocessing. In this simple example we have `fn.readers.caffe2` for reading data in Caffe2 format, `fn.decoders.image` for image decoding, `fn.crop_mirror_normalize` used to normalize the images and `fn.reshape` to adjust the shape of the output tensors. We also move the labels from the CPU to the GPU memory with `labels.gpu()` and apply one hot encoding to them for training with `fn.one_hot`.\n",
    "\n",
    "This example focuses on how to use DALI pipeline with JAX. For more information on DALI pipeline look into [Getting started](../../getting_started.ipynb) and [pipeline documentation](../../../pipeline.rst)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next step is to instantiate pipelines and build them. Building creates and initializes pipeline internals."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For DALI pipeline to work with JAX it needs to be wrapped with appropriate DALI iterator. To get the iterator compatible with JAX we need to import it from DALI JAX plugin. In addition to the pipeline we can pass the `output_map`, `reader_name` and `auto_reset` parameters to the iterator. \n",
    "\n",
    "**Here is a quick explnation of how these parameters work:**\n",
    "\n",
    " - `output_map`: iterators return a dictionary with outputs of the pipeline as its values. Keys in this dictionary are defined by `output_map`. For example, `labels` output returned from the DALI pipeline defined above will be accessible as `iterator_output['labels']`,\n",
    " - `reader_name`: setting this parameter introduces the notion of an epoch to our iterator. DALI pipeline itself is infinite, it will return the data indefinately, wrapping around the dataset. DALI readers (such as `fn.readers.caffe2` used in this example) have access to the information about the size of the dataset. If we want to pass this information to the iterator we need to point to the operator that should be queried for the dataset size. We do it by naming the operator (note `name=\"mnist_caffe2_reader\"`) and passing the same name as the value for `reader_name` argument,\n",
    "  - `auto_reset`: this argument controls the behaviour of the iterator after the end of an epoch. If set to `True` will automatically reset the state of the iterator and prepare it to start the next epoch."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Multiple GPUs\n",
    "\n",
    "This section shows how to train the network with multiple GPUs."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "We start by modifiying pipeline definition function. \n",
    "\n",
    "Note new arguments passed to the `fn.readers.caffe2`: `num_shards` and `shard_id`. They are used to controll sharding:\n",
    " - `num_shards` sets total number of shards\n",
    " - `shard_id` tells the pipeline for which shard in the training it is responsible. \n",
    "\n",
    " Also `device_id` argument was removed from the decorator. Since we want these pipelines run on different GPUs we will pass particualr `device_id` in pipeline creation. Most often, `device_id` and `shard_id` will have the same value but it is not a requirement.\n",
    "\n",
    " If you want to learn more about DALI sharding behaviour look into [DALI sharding docs page](../../general/getting_started.ipynb)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-07-28T07:43:58.022258Z",
     "iopub.status.busy": "2023-07-28T07:43:58.021951Z",
     "iopub.status.idle": "2023-07-28T07:43:58.025225Z",
     "shell.execute_reply": "2023-07-28T07:43:58.024884Z"
    }
   },
   "outputs": [],
   "source": [
    "from nvidia.dali import pipeline_def\n",
    "import nvidia.dali.fn as fn\n",
    "import nvidia.dali.types as types\n",
    "import jax\n",
    "\n",
    "\n",
    "batch_size = 200\n",
    "image_size = 28\n",
    "num_classes = 10\n",
    "batch_size_per_gpu = batch_size // jax.device_count()\n",
    "\n",
    "\n",
    "@pipeline_def(batch_size=batch_size_per_gpu, num_threads=4)\n",
    "def mnist_sharded_pipeline(data_path, random_shuffle, num_shards, shard_id):\n",
    "    jpegs, labels = fn.readers.caffe2(\n",
    "        path=data_path,\n",
    "        random_shuffle=random_shuffle,\n",
    "        name=\"mnist_caffe2_reader\",\n",
    "        num_shards=num_shards,\n",
    "        shard_id=shard_id)\n",
    "    images = fn.decoders.image(\n",
    "        jpegs, device='mixed', output_type=types.GRAY)\n",
    "    images = fn.crop_mirror_normalize(\n",
    "        images, dtype=types.FLOAT, std=[255.], output_layout=\"CHW\")\n",
    "    images = fn.reshape(images, shape=[image_size * image_size])\n",
    "\n",
    "    labels = labels.gpu()\n",
    "    \n",
    "    if random_shuffle:\n",
    "        labels = fn.one_hot(labels, num_classes=num_classes)\n",
    "\n",
    "    return images, labels"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Creating pipelines. Note the `device_id` values that are passed to place a pipeline on a different device."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-07-28T07:43:58.027039Z",
     "iopub.status.busy": "2023-07-28T07:43:58.026762Z",
     "iopub.status.idle": "2023-07-28T07:43:58.242130Z",
     "shell.execute_reply": "2023-07-28T07:43:58.240945Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Creating training pipelines\n",
      "Pipeline <nvidia.dali.pipeline.Pipeline object at 0x7fdbeb5638e0> working on device 0\n",
      "Pipeline <nvidia.dali.pipeline.Pipeline object at 0x7fdbeb45c1f0> working on device 1\n",
      "Creating training iterator\n",
      "Number of batches in training iterator = 300\n"
     ]
    }
   ],
   "source": [
    "from nvidia.dali.plugin import jax as dax\n",
    "\n",
    "print('Creating training pipelines')\n",
    "\n",
    "pipelines = []\n",
    "for id, device in enumerate(jax.devices()):\n",
    "    pipeline = mnist_sharded_pipeline(data_path=training_data_path, random_shuffle=True, num_shards=jax.device_count(), shard_id=id, device_id=id)\n",
    "    print(f'Pipeline {pipeline} working on device {pipeline.device_id}')\n",
    "    pipelines.append(pipeline)\n",
    "\n",
    "print('Creating training iterator')\n",
    "\n",
    "training_iterator = dax.DALIGenericIterator(\n",
    "    pipelines,\n",
    "    output_map=[\"images\", \"labels\"],\n",
    "    reader_name=\"mnist_caffe2_reader\",\n",
    "    auto_reset=True)\n",
    "\n",
    "print(f\"Number of batches in training iterator = {len(training_iterator)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Creating validation iterator\n",
      "Number of batches in validation iterator = 100\n"
     ]
    }
   ],
   "source": [
    "print('Creating validation iterator')\n",
    "validation_pipeline = mnist_sharded_pipeline(data_path=validation_data_path, random_shuffle=False, num_shards=1, shard_id=0, device_id=0)\n",
    "\n",
    "validation_iterator = dax.DALIGenericIterator(\n",
    "    validation_pipeline,\n",
    "    output_map=[\"images\", \"labels\"],\n",
    "    reader_name=\"mnist_caffe2_reader\",\n",
    "    auto_reset=True)\n",
    "\n",
    "print(f\"Number of batches in validation iterator = {len(validation_iterator)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-07-28T07:43:58.245168Z",
     "iopub.status.busy": "2023-07-28T07:43:58.244604Z",
     "iopub.status.idle": "2023-07-28T07:43:58.292113Z",
     "shell.execute_reply": "2023-07-28T07:43:58.291012Z"
    }
   },
   "outputs": [],
   "source": [
    "import jax.numpy as jnp\n",
    "from model import init_model, update_parallel, accuracy\n",
    "\n",
    "\n",
    "model = init_model()\n",
    "model = jax.tree_map(lambda x: jnp.array([x] * jax.device_count()), model) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-07-28T07:43:58.294940Z",
     "iopub.status.busy": "2023-07-28T07:43:58.294735Z",
     "iopub.status.idle": "2023-07-28T07:44:12.533682Z",
     "shell.execute_reply": "2023-07-28T07:44:12.533238Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0 sec\n",
      "Test set accuracy 0.6746000051498413\n",
      "Epoch 1 sec\n",
      "Test set accuracy 0.7845000624656677\n",
      "Epoch 2 sec\n",
      "Test set accuracy 0.8244000673294067\n",
      "Epoch 3 sec\n",
      "Test set accuracy 0.8450000286102295\n",
      "Epoch 4 sec\n",
      "Test set accuracy 0.8600000143051147\n",
      "Epoch 5 sec\n",
      "Test set accuracy 0.8705000281333923\n",
      "Epoch 6 sec\n",
      "Test set accuracy 0.878000020980835\n",
      "Epoch 7 sec\n",
      "Test set accuracy 0.8832000494003296\n",
      "Epoch 8 sec\n",
      "Test set accuracy 0.8874000310897827\n",
      "Epoch 9 sec\n",
      "Test set accuracy 0.8919000625610352\n"
     ]
    }
   ],
   "source": [
    "from model import update_parallel\n",
    "\n",
    "num_epochs = 10\n",
    "\n",
    "for epoch in range(num_epochs):\n",
    "    for it, batch in enumerate(training_iterator):\n",
    "        model = update_parallel(model, batch)\n",
    "        \n",
    "    test_acc = accuracy(jax.tree_map(lambda x: x[0], model), validation_iterator)\n",
    "    \n",
    "    print(f\"Epoch {epoch} sec\")\n",
    "    print(f\"Test set accuracy {test_acc}\")"
   ]
  }
 ],
 "metadata": {
  "celltoolbar": "Raw Cell Format",
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
