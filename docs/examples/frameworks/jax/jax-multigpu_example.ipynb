{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Training with multiple GPUs\n",
    "\n",
    "Here we show how to run training from [\"Training neural network with DALI and JAX\"](jax-basic_example.ipynb) on multiple GPUs. If you haven't already done so it is best to start with single GPU example to better understand following content."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "Again, we start with creating a pipeline definition function. The pipeline was slightly modified to support multiple GPUs.\n",
    "\n",
    "Note the new arguments passed to `fn.readers.caffe2`: `num_shards` and `shard_id`. They are used to control sharding:\n",
    " - `num_shards` sets the total number of shards\n",
    " - `shard_id` tells the pipeline for which shard in the training it is responsible. \n",
    "\n",
    " Also, the `device_id` argument was removed from the decorator. Since we want these pipelines to run on different GPUs we will pass particular `device_id` in pipeline creation. Most often, `device_id` and `shard_id` will have the same value but it is not a requirement. In this example we want the total batch size to be the same as in the single GPU version. That is why we define `batch_size_per_gpu` as `batch_size // jax.device_count()`. Note, that if `batch_size` is not divisible by the number of devices this might require some adjustment to make sure all samples are used in every epoch of the training.\n",
    " If you want to learn more about DALI sharding behaviour look into [DALI sharding docs page](../../general/multigpu.ipynb)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-07-28T07:43:58.022258Z",
     "iopub.status.busy": "2023-07-28T07:43:58.021951Z",
     "iopub.status.idle": "2023-07-28T07:43:58.025225Z",
     "shell.execute_reply": "2023-07-28T07:43:58.024884Z"
    }
   },
   "outputs": [],
   "source": [
    "from nvidia.dali import pipeline_def\n",
    "import nvidia.dali.fn as fn\n",
    "import nvidia.dali.types as types\n",
    "import jax\n",
    "import os\n",
    "\n",
    "\n",
    "training_data_path = os.path.join(os.environ['DALI_EXTRA_PATH'], 'db/MNIST/training/')\n",
    "validation_data_path = os.path.join(os.environ['DALI_EXTRA_PATH'], 'db/MNIST/testing/')\n",
    "\n",
    "\n",
    "batch_size = 200\n",
    "image_size = 28\n",
    "num_classes = 10\n",
    "batch_size_per_gpu = batch_size // jax.device_count()\n",
    "\n",
    "\n",
    "@pipeline_def(batch_size=batch_size_per_gpu, num_threads=4, seed=0)\n",
    "def mnist_sharded_pipeline(data_path, random_shuffle, num_shards, shard_id):\n",
    "    jpegs, labels = fn.readers.caffe2(\n",
    "        path=data_path,\n",
    "        random_shuffle=random_shuffle,\n",
    "        name=\"mnist_caffe2_reader\",\n",
    "        num_shards=num_shards,\n",
    "        shard_id=shard_id)\n",
    "    images = fn.decoders.image(\n",
    "        jpegs, device='mixed', output_type=types.GRAY)\n",
    "    images = fn.crop_mirror_normalize(\n",
    "        images, dtype=types.FLOAT, std=[255.], output_layout=\"CHW\")\n",
    "    images = fn.reshape(images, shape=[image_size * image_size])\n",
    "\n",
    "    labels = labels.gpu()\n",
    "    \n",
    "    if random_shuffle:\n",
    "        labels = fn.one_hot(labels, num_classes=num_classes)\n",
    "\n",
    "    return images, labels"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note the `device_id` values that are passed to place a pipeline on a different device."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-07-28T07:43:58.027039Z",
     "iopub.status.busy": "2023-07-28T07:43:58.026762Z",
     "iopub.status.idle": "2023-07-28T07:43:58.242130Z",
     "shell.execute_reply": "2023-07-28T07:43:58.240945Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Creating training pipelines\n",
      "Pipeline <nvidia.dali.pipeline.Pipeline object at 0x7f7e004fae00> working on device 0\n",
      "Pipeline <nvidia.dali.pipeline.Pipeline object at 0x7f7e004fa350> working on device 1\n"
     ]
    }
   ],
   "source": [
    "from nvidia.dali.plugin import jax as dax\n",
    "\n",
    "print('Creating training pipelines')\n",
    "\n",
    "pipelines = []\n",
    "for id, device in enumerate(jax.devices()):\n",
    "    pipeline = mnist_sharded_pipeline(\n",
    "        data_path=training_data_path, random_shuffle=True, num_shards=jax.device_count(), shard_id=id, device_id=id)\n",
    "    print(f'Pipeline {pipeline} working on device {pipeline.device_id}')\n",
    "    pipelines.append(pipeline)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We created multiple DALI pipelines. Each will run its computations on a different GPU. Each of them will start the preprocessing from a different shard of the training dataset. In this configuration each pipeline will move to the next shard in the next epoch. If you want to control this you can look into `stick_to_shard` argument in the readers.\n",
    "\n",
    "Like in the single GPU example, we create training iterator. It will encapsulate all the pipelines that we created and return a dictionary of JAX arrays. With this simple configuration it will return arrays compatible with JAX `pmap`ed functions. Leaves of the returned dictionary will have shape `(num_devices, batch_per_device, ...)` and each slice across the first dimension of the array will reside on a different GPU."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Creating training iterator\n",
      "Number of batches in training iterator = 300\n"
     ]
    }
   ],
   "source": [
    "print('Creating training iterator')\n",
    "training_iterator = dax.DALIGenericIterator(\n",
    "    pipelines,\n",
    "    output_map=[\"images\", \"labels\"],\n",
    "    reader_name=\"mnist_caffe2_reader\",\n",
    "    auto_reset=True)\n",
    "\n",
    "print(f\"Number of batches in training iterator = {len(training_iterator)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For simplicity, we will run validation on one GPU. We can pass `num_shards=1`, `shard_id=0` and `device_id=0` to `mnist_sharded_pipeline`. It will result in a pipeline identical as in the single GPU example and we can create the validation iterator the same way. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Creating validation iterator\n",
      "Number of batches in validation iterator = 100\n"
     ]
    }
   ],
   "source": [
    "print('Creating validation iterator')\n",
    "validation_pipeline = mnist_sharded_pipeline(data_path=validation_data_path, random_shuffle=False, num_shards=1, shard_id=0, device_id=0)\n",
    "\n",
    "validation_iterator = dax.DALIGenericIterator(\n",
    "    validation_pipeline,\n",
    "    output_map=[\"images\", \"labels\"],\n",
    "    reader_name=\"mnist_caffe2_reader\",\n",
    "    auto_reset=True)\n",
    "\n",
    "print(f\"Number of batches in validation iterator = {len(validation_iterator)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For the model to be compatible with pmap-style multiple GPU training we need to replicate it. If you want to learn more about training on multiple GPUs with `pmap` you can look into [Parallel Evaluation in JAX](https://jax.readthedocs.io/en/latest/jax-101/06-parallelism.html) from the JAX documentation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-07-28T07:43:58.245168Z",
     "iopub.status.busy": "2023-07-28T07:43:58.244604Z",
     "iopub.status.idle": "2023-07-28T07:43:58.292113Z",
     "shell.execute_reply": "2023-07-28T07:43:58.291012Z"
    }
   },
   "outputs": [],
   "source": [
    "import jax.numpy as jnp\n",
    "from model import init_model, accuracy\n",
    "\n",
    "\n",
    "model = init_model()\n",
    "model = jax.tree_map(lambda x: jnp.array([x] * jax.device_count()), model) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For multigpu training we import `update_parallel` function. It is the same as the `update` function with added gradients synchronization across the devices. This will ensure that replicas of the model from different devices remain the same. \n",
    "\n",
    "Since we want to run validation on single GPU, we extract only one replica of the model and pass it to `accuracy` function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-07-28T07:43:58.294940Z",
     "iopub.status.busy": "2023-07-28T07:43:58.294735Z",
     "iopub.status.idle": "2023-07-28T07:44:12.533682Z",
     "shell.execute_reply": "2023-07-28T07:44:12.533238Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0 sec\n",
      "Test set accuracy 0.6729000210762024\n",
      "Epoch 1 sec\n",
      "Test set accuracy 0.7845000624656677\n",
      "Epoch 2 sec\n",
      "Test set accuracy 0.8250000476837158\n",
      "Epoch 3 sec\n",
      "Test set accuracy 0.8457000255584717\n",
      "Epoch 4 sec\n",
      "Test set accuracy 0.8600000143051147\n",
      "Epoch 5 sec\n",
      "Test set accuracy 0.8712000250816345\n",
      "Epoch 6 sec\n",
      "Test set accuracy 0.8770000338554382\n",
      "Epoch 7 sec\n",
      "Test set accuracy 0.8746000528335571\n",
      "Epoch 8 sec\n",
      "Test set accuracy 0.8870000243186951\n",
      "Epoch 9 sec\n",
      "Test set accuracy 0.8915000557899475\n"
     ]
    }
   ],
   "source": [
    "from model import update_parallel\n",
    "\n",
    "num_epochs = 10\n",
    "\n",
    "for epoch in range(num_epochs):\n",
    "    for it, batch in enumerate(training_iterator):\n",
    "        model = update_parallel(model, batch)\n",
    "        \n",
    "    test_acc = accuracy(jax.tree_map(lambda x: x[0], model), validation_iterator)\n",
    "    \n",
    "    print(f\"Epoch {epoch} sec\")\n",
    "    print(f\"Test set accuracy {test_acc}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Automatic parallelization\n",
    "\n",
    "The following section shows how to apply automatic parallelization mechanisms in training with DALI and JAX. To learn more about these concepts look into [Distributed arrays and automatic parallelization](https://jax.readthedocs.io/en/latest/notebooks/Distributed_arrays_and_automatic_parallelization.html) JAX tutrial.\n",
    "\n",
    "It is possible to pass `jax.sharding.Sharding` object to DALI iterator. It will be used to construct output arrays consistent with the sharding. In this example we use simple `PositionalSharding` and pass it to `dax.DALIGenericIterator` initialization. Everything else remains the same as in the multiple GPUs example with `pmap` above. We even used the same pipeline objects for this new iterator."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PositionalSharding([[{GPU 0}]\n",
      "                    [{GPU 1}]])\n",
      "Pipeline <nvidia.dali.pipeline.Pipeline object at 0x7f7e004fae00> working on device 0\n",
      "Pipeline <nvidia.dali.pipeline.Pipeline object at 0x7f7e004fa350> working on device 1\n"
     ]
    }
   ],
   "source": [
    "from jax.sharding import PositionalSharding, Mesh\n",
    "from jax.experimental import mesh_utils\n",
    "\n",
    "\n",
    "mesh = mesh_utils.create_device_mesh((jax.device_count(), 1))\n",
    "sharding = PositionalSharding(mesh)\n",
    "\n",
    "print(sharding)\n",
    "for pipeline in pipelines:\n",
    "    print(f'Pipeline {pipeline} working on device {pipeline.device_id}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note that `sharding` and `pipelines` arguments must match. Devices in the `sharding` must be the same as the devices that `pipelines` are working on. See the `pipelines` creation and `sharding` creation. In both we used all available devices in the order provided by `jax.devices()`. Iterator will not copy outputs between the devices. It will assemble a `jax.Array` from the outputs of the pipelines and the passed `sharding`. This requirement might be lifted in the future. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Creating training iterator\n",
      "Number of batches in training iterator = 300\n"
     ]
    }
   ],
   "source": [
    "print('Creating training iterator')\n",
    "training_iterator = dax.DALIGenericIterator(\n",
    "    pipelines,\n",
    "    output_map=[\"images\", \"labels\"],\n",
    "    reader_name=\"mnist_caffe2_reader\",\n",
    "    auto_reset=True,\n",
    "    sharding=sharding)\n",
    "\n",
    "print(f\"Number of batches in training iterator = {len(training_iterator)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This new iterator is ready for the training. This example utilizes automatic parallelization where the computation follows the data placement. This means that we can use the same `update` function that we used in [single GPU training example](jax-basic_example.ipynb) and it will automatically run computations on multiple GPUs.\n",
    "\n",
    "For simplicity we use the same `validation_iterator` as before and run the `accuracy` calculation on a single GPU. Model is spread between the devices and we need to pull it to one of them for this to work. Otherwise JAX would throw an error. In real life scenarios this might not be the best for performance. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0 sec\n",
      "Test set accuracy 0.687000036239624\n",
      "Epoch 1 sec\n",
      "Test set accuracy 0.7791000604629517\n",
      "Epoch 2 sec\n",
      "Test set accuracy 0.8225000500679016\n",
      "Epoch 3 sec\n",
      "Test set accuracy 0.843000054359436\n",
      "Epoch 4 sec\n",
      "Test set accuracy 0.8577000498771667\n",
      "Epoch 5 sec\n",
      "Test set accuracy 0.8681000471115112\n",
      "Epoch 6 sec\n",
      "Test set accuracy 0.8773000240325928\n",
      "Epoch 7 sec\n",
      "Test set accuracy 0.8832000494003296\n",
      "Epoch 8 sec\n",
      "Test set accuracy 0.8872000575065613\n",
      "Epoch 9 sec\n",
      "Test set accuracy 0.8830000162124634\n"
     ]
    }
   ],
   "source": [
    "from model import update\n",
    "\n",
    "model = init_model()\n",
    "\n",
    "for epoch in range(num_epochs):\n",
    "    for it, batch in enumerate(training_iterator):\n",
    "        model = update(model, batch)\n",
    "        \n",
    "    model_on_one_device = jax.tree_map(lambda x: jax.device_put(x, jax.devices()[0]), model)\n",
    "    test_acc = accuracy(model_on_one_device, validation_iterator)\n",
    "    \n",
    "    print(f\"Epoch {epoch} sec\")\n",
    "    print(f\"Test set accuracy {test_acc}\")"
   ]
  }
 ],
 "metadata": {
  "celltoolbar": "Raw Cell Format",
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
