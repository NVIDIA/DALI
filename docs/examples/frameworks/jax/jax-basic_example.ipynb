{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Using DALI with JAX\n",
    "\n",
    "This simple example shows how to train a neural network implemented in JAX with DALI pipelines. It builds on MNIST training example from JAX codebse that can be found [here](https://github.com/google/jax/blob/jax-v0.4.13/examples/mnist_classifier_fromscratch.py).\n",
    "\n",
    "We will use MNIST in Caffe2 format from [DALI_extra](https://github.com/NVIDIA/DALI_extra)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-07-13T09:15:33.180677Z",
     "iopub.status.busy": "2023-07-13T09:15:33.180327Z",
     "iopub.status.idle": "2023-07-13T09:15:33.186174Z",
     "shell.execute_reply": "2023-07-13T09:15:33.185585Z"
    }
   },
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "training_data_path = os.path.join(os.environ['DALI_EXTRA_PATH'], 'db/MNIST/training/')\n",
    "validation_data_path = os.path.join(os.environ['DALI_EXTRA_PATH'], 'db/MNIST/testing/')"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First step is to create a pipeline definition function that will later be used to create instances of DALI pipelines. It defines all steps of the preprocessing. In this simple example we have `fn.readers.caffe2` for reading data in Caffe2 format, `fn.decoders.image` for image decoding, `fn.crop_mirror_normalize` used to normalize the images and `fn.reshape` to adjust the shape of the output tensors. We also move the labels from the CPU to the GPU memory with `labels.gpu()` and apply one hot encoding to them for training with `fn.one_hot`.\n",
    "\n",
    "This example focuses on how to use DALI pipeline with JAX. For more information on DALI pipeline look into [Getting started](../../getting_started.ipynb) and [pipeline documentation](../../../pipeline.rst)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-07-13T09:15:33.189706Z",
     "iopub.status.busy": "2023-07-13T09:15:33.189425Z",
     "iopub.status.idle": "2023-07-13T09:15:33.435045Z",
     "shell.execute_reply": "2023-07-13T09:15:33.434331Z"
    }
   },
   "outputs": [],
   "source": [
    "from nvidia.dali import pipeline_def\n",
    "import nvidia.dali.fn as fn\n",
    "import nvidia.dali.types as types\n",
    "\n",
    "\n",
    "batch_size = 100\n",
    "image_size = 28\n",
    "num_classes = 10\n",
    "\n",
    "\n",
    "@pipeline_def(device_id=0, batch_size=batch_size, num_threads=4)\n",
    "def mnist_pipeline(data_path, random_shuffle):\n",
    "    jpegs, labels = fn.readers.caffe2(\n",
    "        path=data_path,\n",
    "        random_shuffle=random_shuffle,\n",
    "        name=\"mnist_caffe2_reader\")\n",
    "    images = fn.decoders.image(\n",
    "        jpegs, device='mixed', output_type=types.GRAY)\n",
    "    images = fn.crop_mirror_normalize(\n",
    "        images, dtype=types.FLOAT, std=[255.], output_layout=\"CHW\")\n",
    "    images = fn.reshape(images, shape=[image_size * image_size])\n",
    "\n",
    "    labels = labels.gpu()\n",
    "    \n",
    "    if random_shuffle:\n",
    "        labels = fn.one_hot(labels, num_classes=num_classes)\n",
    "\n",
    "    return images, labels"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next step is to instantiate pipelines and build them. Building creates and initializes pipeline internals."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-07-13T09:15:33.461201Z",
     "iopub.status.busy": "2023-07-13T09:15:33.460991Z",
     "iopub.status.idle": "2023-07-13T09:15:33.834000Z",
     "shell.execute_reply": "2023-07-13T09:15:33.833275Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Creating pipelines\n",
      "Building pipelines\n",
      "<nvidia.dali.pipeline.Pipeline object at 0x7f4efc3cf6d0>\n",
      "<nvidia.dali.pipeline.Pipeline object at 0x7f4ed80edd80>\n"
     ]
    }
   ],
   "source": [
    "print('Creating pipelines')\n",
    "training_pipeline = mnist_pipeline(data_path=training_data_path, random_shuffle=True)\n",
    "validation_pipeline = mnist_pipeline(data_path=validation_data_path, random_shuffle=False)\n",
    "\n",
    "print('Building pipelines')\n",
    "training_pipeline.build()\n",
    "validation_pipeline.build()\n",
    "\n",
    "print(training_pipeline)\n",
    "print(validation_pipeline)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For DALI pipeline to work with JAX it needs to be wrapped with appropriate DALI iterator. To get the iterator compatible with JAX we need to import it from DALI JAX plugin. In addition to the pipeline we can pass the `output_map`, `reader_name` and `auto_reset` parameters to the iterator. \n",
    "\n",
    "**Here is a quick explnation of how these parameters work:**\n",
    "\n",
    " - `output_map`: iterators return a dictionary with outputs of the pipeline as its values. Keys in this dictionary are defined by `output_map`. For example, `labels` output returned from the DALI pipeline defined above will be accessible as `iterator_output['labels']`,\n",
    " - `reader_name`: setting this parameter introduces the notion of an epoch to our iterator. DALI pipeline itself is infinite, it will return the data indefinately, wrapping around the dataset. DALI readers (such as `fn.readers.caffe2` used in this example) have access to the information about the size of the dataset. If we want to pass this information to the iterator we need to point to the operator that should be queried for the dataset size. We do it by naming the operator (note `name=\"mnist_caffe2_reader\"`) and passing the same name as the value for `reader_name` argument,\n",
    "  - `auto_reset`: this argument controls the behaviour of the iterator after the end of an epoch. If set to `True` will automatically reset the state of the iterator and prepare it to start the next epoch."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-07-13T09:15:33.836929Z",
     "iopub.status.busy": "2023-07-13T09:15:33.836764Z",
     "iopub.status.idle": "2023-07-13T09:15:38.257966Z",
     "shell.execute_reply": "2023-07-13T09:15:38.256953Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Creating iterators\n",
      "<nvidia.dali.plugin.jax.DALIGenericIterator object at 0x7f4ed804f100>\n",
      "Training iterator size = 60000\n",
      "Validation iterator size = 10000\n"
     ]
    }
   ],
   "source": [
    "from nvidia.dali.plugin import jax as dax\n",
    "\n",
    "\n",
    "print('Creating iterators')\n",
    "training_iterator = dax.DALIGenericIterator(\n",
    "    training_pipeline,\n",
    "    output_map=[\"images\", \"labels\"],\n",
    "    reader_name=\"mnist_caffe2_reader\",\n",
    "    auto_reset=True)\n",
    "\n",
    "validation_iterator = dax.DALIGenericIterator(\n",
    "    validation_pipeline,\n",
    "    output_map=[\"images\", \"labels\"],\n",
    "    reader_name=\"mnist_caffe2_reader\",\n",
    "    auto_reset=True)\n",
    "\n",
    "print(training_iterator)\n",
    "print(f\"Training iterator size = {training_iterator.size}\")\n",
    "print(f\"Validation iterator size = {validation_iterator.size}\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "With the setup above DALI iterators are ready for the training. \n",
    "\n",
    "Next we import training utilities implemented in JAX. `init_model` will create the model instance and initialize its parameters. In this simple example it is a MLP model with two hidden layers. `update` performs one iteration of the training. `accuracy` is a helper function to run validation after each epoch on the test set and get current accuracy of the model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-07-13T09:15:38.262911Z",
     "iopub.status.busy": "2023-07-13T09:15:38.262188Z",
     "iopub.status.idle": "2023-07-13T09:15:38.355742Z",
     "shell.execute_reply": "2023-07-13T09:15:38.355067Z"
    }
   },
   "outputs": [],
   "source": [
    "from model import init_model, update, accuracy"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "At this point everything is ready to run the training."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-07-13T09:15:38.360302Z",
     "iopub.status.busy": "2023-07-13T09:15:38.359051Z",
     "iopub.status.idle": "2023-07-13T09:15:48.635962Z",
     "shell.execute_reply": "2023-07-13T09:15:48.635527Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting training\n",
      "Epoch 0 sec\n",
      "Test set accuracy 0.7848000526428223\n",
      "Epoch 1 sec\n",
      "Test set accuracy 0.8466000556945801\n",
      "Epoch 2 sec\n",
      "Test set accuracy 0.8713000416755676\n",
      "Epoch 3 sec\n",
      "Test set accuracy 0.8840000629425049\n",
      "Epoch 4 sec\n",
      "Test set accuracy 0.891800045967102\n",
      "Epoch 5 sec\n",
      "Test set accuracy 0.8980000615119934\n",
      "Epoch 6 sec\n",
      "Test set accuracy 0.9034000635147095\n",
      "Epoch 7 sec\n",
      "Test set accuracy 0.9084000587463379\n",
      "Epoch 8 sec\n",
      "Test set accuracy 0.9116000533103943\n",
      "Epoch 9 sec\n",
      "Test set accuracy 0.9139000177383423\n"
     ]
    }
   ],
   "source": [
    "print('Starting training')\n",
    "\n",
    "model = init_model()\n",
    "num_epochs = 10\n",
    "\n",
    "for epoch in range(num_epochs):\n",
    "    for batch in training_iterator:\n",
    "        model = update(model, batch)\n",
    "\n",
    "    test_acc = accuracy(model, validation_iterator)\n",
    "    print(f\"Epoch {epoch} sec\")\n",
    "    print(f\"Test set accuracy {test_acc}\")"
   ]
  }
 ],
 "metadata": {
  "celltoolbar": "Raw Cell Format",
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
