{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Using DALI with JAX\n",
    "\n",
    "### Overview\n",
    "\n",
    "This simple example shows how to train a neural network implementet in JAX with DALI pipelines. It builds on MNIST training example from JAX codebse that can be found [here](https://github.com/google/jax/blob/main/examples/mnist_classifier_fromscratch.py). This example is "
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will use MNIST in Caffe2 format from [DALI_extra](https://github.com/NVIDIA/DALI_extra)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "training_data_path = os.path.join(os.environ['DALI_EXTRA_PATH'], 'db/MNIST/training/')\n",
    "validation_data_path = os.path.join(os.environ['DALI_EXTRA_PATH'], 'db/MNIST/testing/')"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First step is to prepare function that will be later used to create instances of DALI pipelines. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nvidia.dali import pipeline_def\n",
    "import nvidia.dali.fn as fn\n",
    "import nvidia.dali.types as types\n",
    "\n",
    "\n",
    "batch_size = 100\n",
    "image_size = 28\n",
    "num_classes = 10\n",
    "\n",
    "\n",
    "@pipeline_def(device_id=0, batch_size=batch_size, num_threads=4)\n",
    "def mnist_pipeline(data_path, random_shuffle):\n",
    "    jpegs, labels = fn.readers.caffe2(\n",
    "        path=data_path,\n",
    "        random_shuffle=random_shuffle,\n",
    "        name=\"mnist_caffe2_reader\")\n",
    "    images = fn.decoders.image(\n",
    "        jpegs, device='mixed', output_type=types.GRAY)\n",
    "    images = fn.crop_mirror_normalize(\n",
    "        images, dtype=types.FLOAT, std=[255.], output_layout=\"CHW\")\n",
    "    images = fn.reshape(images, shape=[image_size * image_size])\n",
    "\n",
    "    labels = labels.gpu()\n",
    "    \n",
    "    if random_shuffle:\n",
    "        labels = fn.one_hot(labels, num_classes=num_classes)\n",
    "\n",
    "    return images, labels"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next step is to instantiate pipelines and build them. Building creates and initializes pipeline internals."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Creating pipelines\n",
      "Building pipelines\n",
      "<nvidia.dali.pipeline.Pipeline object at 0x7f1de056a650>\n",
      "<nvidia.dali.pipeline.Pipeline object at 0x7d1cf4357f40>\n"
     ]
    }
   ],
   "source": [
    "print('Creating pipelines')\n",
    "training_pipeline = mnist_pipeline(data_path=training_data_path, random_shuffle=True)\n",
    "validation_pipeline = mnist_pipeline(data_path=validation_data_path, random_shuffle=False)\n",
    "\n",
    "print('Building pipelines')\n",
    "training_pipeline.build()\n",
    "validation_pipeline.build()\n",
    "\n",
    "print(training_pipeline)\n",
    "print(validation_pipeline)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This example focuses on how to use DALI pipeline with JAX. For more information on DALI pipeline look into [Getting started](../../getting_started.ipynb) and [pipeline documentation](../../../pipeline.rst)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For DALI pipeline to work with JAX it needs to be wrapped with appropriate DALI iterator. To get iterator compatible with JAX we need to import in from DALI JAX plugin. In addition to the pipeline we can pass the `output_map`, `reader_name` and `auto_reset` to the iterator. Quick explnation of how these parameters work:\n",
    "\n",
    " - `output_map`: iterators return a dictionary with outputs of the pipeline as its values. Keys in this dictionary are defined by `output_map`. For example, `labels` output returned from the DALI pipeline defined above will be accessible as `iterator_output['labels']`,\n",
    " - `reader_name`: setting this parameter introduces the notion of an epoch to our iterator. DALI pipeline itself is infinite, it will return the data indefinately, wrapping around the dataset. DALI readers (such as `fn.readers.caffe2` used in this example) have access to the information about the size of the dataset. If we want to pass this information to the iterator we need to point to the operator that should be queried for the dataset size. We do it by naming the operator (note `name=\"mnist_caffe2_reader\"`) and passing the same name as the value for `reader_name` argument,\n",
    "  - `auto_reset`: this argument controls the behaviour of the iterator after the end of an epoch. If set to `True` will automatically reset the state of the iterator and prepare it to start the next epoch."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Creating iterators\n",
      "<nvidia.dali.plugin.jax.DALIGenericIterator object at 0x7d1cf40f6f20>\n",
      "Training iterator size = 60000\n",
      "Validation iterator size = 10000\n"
     ]
    }
   ],
   "source": [
    "from nvidia.dali.plugin import jax as dax\n",
    "\n",
    "\n",
    "print('Creating iterators')\n",
    "training_iterator = dax.DALIGenericIterator(\n",
    "    training_pipeline,\n",
    "    output_map=[\"images\", \"labels\"],\n",
    "    reader_name=\"mnist_caffe2_reader\",\n",
    "    auto_reset=True)\n",
    "\n",
    "validation_iterator = dax.DALIGenericIterator(\n",
    "    validation_pipeline,\n",
    "    output_map=[\"images\", \"labels\"],\n",
    "    reader_name=\"mnist_caffe2_reader\",\n",
    "    auto_reset=True)\n",
    "\n",
    "print(training_iterator)\n",
    "print(f\"Training iterator size = {training_iterator.size}\")\n",
    "print(f\"Validation iterator size = {validation_iterator.size}\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "With the setup above DALI iterators are ready for the training. \n",
    "\n",
    "Next we import training utilities implemented in JAX and set some parameters of the training."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "from model import init_random_params, update, accuracy\n",
    "\n",
    "layer_sizes = [784, 1024, 1024, 10]\n",
    "param_scale = 0.1\n",
    "step_size = 0.001\n",
    "num_epochs = 10"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "At this point everything is ready to run the training."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting training\n",
      "Epoch 0 sec\n",
      "Test set accuracy 0.7789000272750854\n",
      "Epoch 1 sec\n",
      "Test set accuracy 0.8401000499725342\n",
      "Epoch 2 sec\n",
      "Test set accuracy 0.8641000390052795\n",
      "Epoch 3 sec\n",
      "Test set accuracy 0.8773000240325928\n",
      "Epoch 4 sec\n",
      "Test set accuracy 0.8871000409126282\n",
      "Epoch 5 sec\n",
      "Test set accuracy 0.8957000374794006\n",
      "Epoch 6 sec\n",
      "Test set accuracy 0.9025000333786011\n",
      "Epoch 7 sec\n",
      "Test set accuracy 0.9083000421524048\n",
      "Epoch 8 sec\n",
      "Test set accuracy 0.9104000329971313\n",
      "Epoch 9 sec\n",
      "Test set accuracy 0.912600040435791\n"
     ]
    }
   ],
   "source": [
    "print('Starting training')\n",
    "\n",
    "\n",
    "params = init_random_params(param_scale, layer_sizes)\n",
    "\n",
    "for epoch in range(num_epochs):\n",
    "    for batch in training_iterator:\n",
    "        params = update(params, batch, step_size)\n",
    "\n",
    "    test_acc = accuracy(params, validation_iterator)\n",
    "    print(f\"Epoch {epoch} sec\")\n",
    "    print(f\"Test set accuracy {test_acc}\")"
   ]
  }
 ],
 "metadata": {
  "celltoolbar": "Raw Cell Format",
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
