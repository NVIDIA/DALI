{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Using DALI with JAX\n",
    "\n",
    "### Overview\n",
    "\n",
    "This simple example shows how to train a neural network implementet in JAX with DALI pipelines. It builds on MNIST training example from JAX codebse that can be found [here](https://github.com/google/jax/blob/main/examples/mnist_classifier_fromscratch.py)."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will use MNIST in Caffe2 format from [DALI_extra](https://github.com/NVIDIA/DALI_extra)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "training_data_path = os.path.join(os.environ['DALI_EXTRA_PATH'], 'db/MNIST/training/')\n",
    "validation_data_path = os.path.join(os.environ['DALI_EXTRA_PATH'], 'db/MNIST/testing/')"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First step is to prepare function that will be later used to create instances of DALI pipelines. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nvidia.dali import pipeline_def\n",
    "import nvidia.dali.fn as fn\n",
    "import nvidia.dali.types as types\n",
    "\n",
    "\n",
    "batch_size = 100\n",
    "image_size = 28\n",
    "num_classes = 10\n",
    "\n",
    "@pipeline_def(device_id=0, batch_size=batch_size, num_threads=4)\n",
    "def mnist_pipeline(data_path, random_shuffle):\n",
    "    jpegs, labels = fn.readers.caffe2(\n",
    "        path=data_path,\n",
    "        random_shuffle=random_shuffle,\n",
    "        name=\"mnist_caffe2_reader\")\n",
    "    images = fn.decoders.image(\n",
    "        jpegs, device='mixed', output_type=types.GRAY)\n",
    "    images = fn.crop_mirror_normalize(\n",
    "        images, dtype=types.FLOAT, std=[255.], output_layout=\"CHW\")\n",
    "    images = fn.reshape(images, shape=[image_size * image_size])\n",
    "\n",
    "    labels = labels.gpu()\n",
    "    \n",
    "    if random_shuffle:\n",
    "        labels = fn.one_hot(labels, num_classes=num_classes)\n",
    "\n",
    "    return images, labels"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next step is to instantiate pipelines and build them. Building creates and initializes pipeline internals."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Creating pipelines\n",
      "Building pipelines\n",
      "<nvidia.dali.pipeline.Pipeline object at 0x7f34081594e0>\n",
      "<nvidia.dali.pipeline.Pipeline object at 0x7f340815b010>\n"
     ]
    }
   ],
   "source": [
    "print('Creating pipelines')\n",
    "training_pipeline = mnist_pipeline(data_path=training_data_path, random_shuffle=True)\n",
    "validation_pipeline = mnist_pipeline(data_path=validation_data_path, random_shuffle=False)\n",
    "\n",
    "print('Building pipelines')\n",
    "training_pipeline.build()\n",
    "validation_pipeline.build()\n",
    "\n",
    "print(training_pipeline)\n",
    "print(validation_pipeline)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For DALI pipeline to work with JAX it needs to be wrapped with appropriate DALI iterator. To get iterator compatible with JAX we need to import in from DALI JAX plugin."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Creating iterators\n",
      "<nvidia.dali.plugin.jax.DALIGenericIterator object at 0x7f340815bcd0>\n",
      "Training iterator size = 60000\n",
      "Validation iterator size = 10000\n"
     ]
    }
   ],
   "source": [
    "from nvidia.dali.plugin import jax as dax\n",
    "\n",
    "\n",
    "print('Creating iterators')\n",
    "training_iterator = dax.DALIGenericIterator(\n",
    "    training_pipeline,\n",
    "    output_map=[\"images\", \"labels\"],\n",
    "    reader_name=\"mnist_caffe2_reader\",\n",
    "    auto_reset=True)\n",
    "\n",
    "validation_iterator = dax.DALIGenericIterator(\n",
    "    validation_pipeline,\n",
    "    output_map=[\"images\", \"labels\"],\n",
    "    reader_name=\"mnist_caffe2_reader\",\n",
    "    auto_reset=True)\n",
    "\n",
    "print(training_iterator)\n",
    "print(f\"Training iterator size = {training_iterator.size}\")\n",
    "print(f\"Validation iterator size = {validation_iterator.size}\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "DALI iterators are ready for the training. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [],
   "source": [
    "from model import init_random_params, update, accuracy"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting training\n",
      "Epoch 0 sec\n",
      "Test set accuracy 0.7761000394821167\n",
      "Epoch 1 sec\n",
      "Test set accuracy 0.835800051689148\n",
      "Epoch 2 sec\n",
      "Test set accuracy 0.8623000383377075\n",
      "Epoch 3 sec\n",
      "Test set accuracy 0.8778000473976135\n",
      "Epoch 4 sec\n",
      "Test set accuracy 0.8875000476837158\n",
      "Epoch 5 sec\n",
      "Test set accuracy 0.896600067615509\n",
      "Epoch 6 sec\n",
      "Test set accuracy 0.9018000364303589\n",
      "Epoch 7 sec\n",
      "Test set accuracy 0.9058000445365906\n",
      "Epoch 8 sec\n",
      "Test set accuracy 0.9095000624656677\n",
      "Epoch 9 sec\n",
      "Test set accuracy 0.913800060749054\n"
     ]
    }
   ],
   "source": [
    "print('Starting training')\n",
    "layer_sizes = [784, 1024, 1024, 10]\n",
    "param_scale = 0.1\n",
    "step_size = 0.001\n",
    "num_epochs = 10\n",
    "\n",
    "params = init_random_params(param_scale, layer_sizes)\n",
    "\n",
    "for epoch in range(num_epochs):\n",
    "    for batch in training_iterator:\n",
    "        params = update(params, batch, step_size)\n",
    "\n",
    "    test_acc = accuracy(params, validation_iterator)\n",
    "    print(f\"Epoch {epoch} sec\")\n",
    "    print(f\"Test set accuracy {test_acc}\")"
   ]
  }
 ],
 "metadata": {
  "celltoolbar": "Raw Cell Format",
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
