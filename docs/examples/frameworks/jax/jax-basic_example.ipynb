{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Using DALI with JAX\n",
    "\n",
    "This simple example shows how to train a neural network implemented in JAX with DALI pipelines. It builds on MNIST training example from JAX codebse that can be found [here](https://github.com/google/jax/blob/jax-v0.4.13/examples/mnist_classifier_fromscratch.py).\n",
    "\n",
    "We will use MNIST in Caffe2 format from [DALI_extra](https://github.com/NVIDIA/DALI_extra)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-07-13T09:15:33.180677Z",
     "iopub.status.busy": "2023-07-13T09:15:33.180327Z",
     "iopub.status.idle": "2023-07-13T09:15:33.186174Z",
     "shell.execute_reply": "2023-07-13T09:15:33.185585Z"
    }
   },
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "training_data_path = os.path.join(os.environ['DALI_EXTRA_PATH'], 'db/MNIST/training/')\n",
    "validation_data_path = os.path.join(os.environ['DALI_EXTRA_PATH'], 'db/MNIST/testing/')"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First step is to create a pipeline definition function that will later be used to create instances of DALI pipelines. It defines all steps of the preprocessing. In this simple example we have `fn.readers.caffe2` for reading data in Caffe2 format, `fn.decoders.image` for image decoding, `fn.crop_mirror_normalize` used to normalize the images and `fn.reshape` to adjust the shape of the output tensors. We also move the labels from the CPU to the GPU memory with `labels.gpu()` and apply one hot encoding to them for training with `fn.one_hot`.\n",
    "\n",
    "This example focuses on how to use DALI pipeline with JAX. For more information on DALI pipeline look into [Getting started](../../getting_started.ipynb) and [pipeline documentation](../../../pipeline.rst)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-07-13T09:15:33.189706Z",
     "iopub.status.busy": "2023-07-13T09:15:33.189425Z",
     "iopub.status.idle": "2023-07-13T09:15:33.435045Z",
     "shell.execute_reply": "2023-07-13T09:15:33.434331Z"
    }
   },
   "outputs": [],
   "source": [
    "from nvidia.dali import pipeline_def\n",
    "import nvidia.dali.fn as fn\n",
    "import nvidia.dali.types as types\n",
    "\n",
    "\n",
    "batch_size = 100\n",
    "image_size = 28\n",
    "num_classes = 10\n",
    "\n",
    "\n",
    "@pipeline_def(device_id=0, batch_size=batch_size, num_threads=4)\n",
    "def mnist_pipeline(data_path, random_shuffle):\n",
    "    jpegs, labels = fn.readers.caffe2(\n",
    "        path=data_path,\n",
    "        random_shuffle=random_shuffle,\n",
    "        name=\"mnist_caffe2_reader\")\n",
    "    images = fn.decoders.image(\n",
    "        jpegs, device='mixed', output_type=types.GRAY)\n",
    "    images = fn.crop_mirror_normalize(\n",
    "        images, dtype=types.FLOAT, std=[255.], output_layout=\"CHW\")\n",
    "    images = fn.reshape(images, shape=[image_size * image_size])\n",
    "\n",
    "    labels = labels.gpu()\n",
    "    \n",
    "    if random_shuffle:\n",
    "        labels = fn.one_hot(labels, num_classes=num_classes)\n",
    "\n",
    "    return images, labels"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next step is to instantiate pipelines and build them. Building creates and initializes pipeline internals."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-07-13T09:15:33.461201Z",
     "iopub.status.busy": "2023-07-13T09:15:33.460991Z",
     "iopub.status.idle": "2023-07-13T09:15:33.834000Z",
     "shell.execute_reply": "2023-07-13T09:15:33.833275Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Creating pipelines\n",
      "Building pipelines\n",
      "<nvidia.dali.pipeline.Pipeline object at 0x7f9c7028fe50>\n",
      "<nvidia.dali.pipeline.Pipeline object at 0x7f9c74f221d0>\n"
     ]
    }
   ],
   "source": [
    "print('Creating pipelines')\n",
    "training_pipeline = mnist_pipeline(data_path=training_data_path, random_shuffle=True)\n",
    "validation_pipeline = mnist_pipeline(data_path=validation_data_path, random_shuffle=False)\n",
    "\n",
    "print('Building pipelines')\n",
    "training_pipeline.build()\n",
    "validation_pipeline.build()\n",
    "\n",
    "print(training_pipeline)\n",
    "print(validation_pipeline)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For DALI pipeline to work with JAX it needs to be wrapped with appropriate DALI iterator. To get the iterator compatible with JAX we need to import it from DALI JAX plugin. In addition to the pipeline we can pass the `output_map`, `reader_name` and `auto_reset` parameters to the iterator. \n",
    "\n",
    "**Here is a quick explnation of how these parameters work:**\n",
    "\n",
    " - `output_map`: iterators return a dictionary with outputs of the pipeline as its values. Keys in this dictionary are defined by `output_map`. For example, `labels` output returned from the DALI pipeline defined above will be accessible as `iterator_output['labels']`,\n",
    " - `reader_name`: setting this parameter introduces the notion of an epoch to our iterator. DALI pipeline itself is infinite, it will return the data indefinately, wrapping around the dataset. DALI readers (such as `fn.readers.caffe2` used in this example) have access to the information about the size of the dataset. If we want to pass this information to the iterator we need to point to the operator that should be queried for the dataset size. We do it by naming the operator (note `name=\"mnist_caffe2_reader\"`) and passing the same name as the value for `reader_name` argument,\n",
    "  - `auto_reset`: this argument controls the behaviour of the iterator after the end of an epoch. If set to `True` will automatically reset the state of the iterator and prepare it to start the next epoch."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-07-13T09:15:33.836929Z",
     "iopub.status.busy": "2023-07-13T09:15:33.836764Z",
     "iopub.status.idle": "2023-07-13T09:15:38.257966Z",
     "shell.execute_reply": "2023-07-13T09:15:38.256953Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Creating iterators\n",
      "<nvidia.dali.plugin.jax.DALIGenericIterator object at 0x7f9c7028f250>\n",
      "Training iterator size = 60000\n",
      "Validation iterator size = 10000\n"
     ]
    }
   ],
   "source": [
    "from nvidia.dali.plugin import jax as dax\n",
    "\n",
    "\n",
    "print('Creating iterators')\n",
    "training_iterator = dax.DALIGenericIterator(\n",
    "    training_pipeline,\n",
    "    output_map=[\"images\", \"labels\"],\n",
    "    reader_name=\"mnist_caffe2_reader\",\n",
    "    auto_reset=True)\n",
    "\n",
    "validation_iterator = dax.DALIGenericIterator(\n",
    "    validation_pipeline,\n",
    "    output_map=[\"images\", \"labels\"],\n",
    "    reader_name=\"mnist_caffe2_reader\",\n",
    "    auto_reset=True)\n",
    "\n",
    "print(training_iterator)\n",
    "print(f\"Training iterator size = {training_iterator.size}\")\n",
    "print(f\"Validation iterator size = {validation_iterator.size}\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "With the setup above DALI iterators are ready for the training. \n",
    "\n",
    "Next we import training utilities implemented in JAX. `init_model` will create the model instance and initialize its parameters. In this simple example it is a MLP model with two hidden layers. `update` performs one iteration of the training. `accuracy` is a helper function to run validation after each epoch on the test set and get current accuracy of the model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-07-13T09:15:38.262911Z",
     "iopub.status.busy": "2023-07-13T09:15:38.262188Z",
     "iopub.status.idle": "2023-07-13T09:15:38.355742Z",
     "shell.execute_reply": "2023-07-13T09:15:38.355067Z"
    }
   },
   "outputs": [],
   "source": [
    "from model import init_model, update, accuracy"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "At this point everything is ready to run the training."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-07-13T09:15:38.360302Z",
     "iopub.status.busy": "2023-07-13T09:15:38.359051Z",
     "iopub.status.idle": "2023-07-13T09:15:48.635962Z",
     "shell.execute_reply": "2023-07-13T09:15:48.635527Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting training\n",
      "Epoch 0 sec\n",
      "Test set accuracy 0.784600019454956\n",
      "Epoch 1 sec\n",
      "Test set accuracy 0.8463000655174255\n",
      "Epoch 2 sec\n",
      "Test set accuracy 0.87090003490448\n",
      "Epoch 3 sec\n",
      "Test set accuracy 0.8835000395774841\n",
      "Epoch 4 sec\n",
      "Test set accuracy 0.891800045967102\n",
      "Epoch 5 sec\n",
      "Test set accuracy 0.8980000615119934\n",
      "Epoch 6 sec\n",
      "Test set accuracy 0.9033000469207764\n",
      "Epoch 7 sec\n",
      "Test set accuracy 0.9085000157356262\n",
      "Epoch 8 sec\n",
      "Test set accuracy 0.9119000434875488\n",
      "Epoch 9 sec\n",
      "Test set accuracy 0.9140000343322754\n"
     ]
    }
   ],
   "source": [
    "print('Starting training')\n",
    "\n",
    "model = init_model()\n",
    "num_epochs = 10\n",
    "\n",
    "for epoch in range(num_epochs):\n",
    "    for batch in training_iterator:\n",
    "        model = update(model, batch)\n",
    "\n",
    "    test_acc = accuracy(model, validation_iterator)\n",
    "    print(f\"Epoch {epoch} sec\")\n",
    "    print(f\"Test set accuracy {test_acc}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Multiple GPUs\n",
    "\n",
    "This section shows how to train the network with multiple GPUs. This simple example discusses setup with many GPUs availible in one JAX process on a single node.\n",
    "\n",
    "We start by modifiying pipeline definition function. \n",
    "\n",
    "Note new arguments passed to the `fn.readers.caffe2`: `num_shards` and `shard_id`. They are used to controll sharding:\n",
    " - `num_shards` sets total number of shards\n",
    " - `shard_id` tells the pipeline for which shard in the training it is responsible. \n",
    "\n",
    " Also `device_id` argument was removed from the decorator. Since we want these pipelines run on different GPUs we will pass particualr `device_id` in pipeline creation. Most often, `device_id` and `shard_id` will have the same value but it is not a requirement.\n",
    "\n",
    " If you want to learn more about DALI sharding behaviour look into [DALI sharding docs page]()."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_devices = 2\n",
    "\n",
    "batch_size_per_gpu = batch_size // num_devices\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "@pipeline_def(batch_size=batch_size_per_gpu, num_threads=4)\n",
    "def mnist_sharded_pipeline(data_path, random_shuffle, num_shards, shard_id):\n",
    "    jpegs, labels = fn.readers.caffe2(\n",
    "        path=data_path,\n",
    "        random_shuffle=random_shuffle,\n",
    "        name=\"mnist_caffe2_reader\",\n",
    "        num_shards=num_shards,\n",
    "        shard_id=shard_id)\n",
    "    images = fn.decoders.image(\n",
    "        jpegs, device='mixed', output_type=types.GRAY)\n",
    "    images = fn.crop_mirror_normalize(\n",
    "        images, dtype=types.FLOAT, std=[255.], output_layout=\"CHW\")\n",
    "    images = fn.reshape(images, shape=[image_size * image_size])\n",
    "\n",
    "    labels = labels.gpu()\n",
    "    \n",
    "    if random_shuffle:\n",
    "        labels = fn.one_hot(labels, num_classes=num_classes)\n",
    "\n",
    "    return images, labels"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Creating pipelines. Note the `device_id` values that are passed to place a pipeline on a different device."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Creating training pipelines\n",
      "Pipeline <nvidia.dali.pipeline.Pipeline object at 0x7d9b7c0aa350> working on device 0\n",
      "Pipeline <nvidia.dali.pipeline.Pipeline object at 0x7d9b7c0a9ab0> working on device 1\n"
     ]
    }
   ],
   "source": [
    "print('Creating training pipelines')\n",
    "training_pipeline_0 = mnist_sharded_pipeline(data_path=training_data_path, random_shuffle=True, num_shards=2, shard_id=0, device_id=0)\n",
    "training_pipeline_1 = mnist_sharded_pipeline(data_path=training_data_path, random_shuffle=True, num_shards=2, shard_id=1, device_id=1)\n",
    "\n",
    "print(f'Pipeline {training_pipeline_0} working on device {training_pipeline_0.device_id}')\n",
    "print(f'Pipeline {training_pipeline_1} working on device {training_pipeline_1.device_id}')\n",
    "\n",
    "validation_pipeline = mnist_sharded_pipeline(data_path=validation_data_path, random_shuffle=False, num_shards=1, shard_id=0, device_id=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "import jax.numpy as jnp\n",
    "from jax import tree_map\n",
    "\n",
    "\n",
    "model = init_model()\n",
    "model = tree_map(lambda x: jnp.array([x] * num_devices), model) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "from model import update_parallel\n",
    "from jax import pmap\n",
    "\n",
    "update_parallel = pmap(update_parallel, axis_name='data')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Creating training iterator\n",
      "Training iterator size = 30000\n",
      "Validation iterator size = 10000\n"
     ]
    }
   ],
   "source": [
    "print('Creating training iterator')\n",
    "\n",
    "training_iterator = dax.DALIGenericIterator(\n",
    "    [training_pipeline_0, training_pipeline_1],\n",
    "    output_map=[\"images\", \"labels\"],\n",
    "    reader_name=\"mnist_caffe2_reader\",\n",
    "    auto_reset=True)\n",
    "\n",
    "validation_iterator = dax.DALIGenericIterator(\n",
    "    validation_pipeline,\n",
    "    output_map=[\"images\", \"labels\"],\n",
    "    reader_name=\"mnist_caffe2_reader\",\n",
    "    auto_reset=True)\n",
    "\n",
    "print(f\"Training iterator size = {training_iterator.size}\")\n",
    "print(f\"Validation iterator size = {validation_iterator.size}\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0 sec\n",
      "Test set accuracy 0.7860000133514404\n",
      "Epoch 1 sec\n",
      "Test set accuracy 0.8438000679016113\n",
      "Epoch 2 sec\n",
      "Test set accuracy 0.8681000471115112\n",
      "Epoch 3 sec\n",
      "Test set accuracy 0.8776000142097473\n",
      "Epoch 4 sec\n",
      "Test set accuracy 0.8905000686645508\n",
      "Epoch 5 sec\n",
      "Test set accuracy 0.8975000381469727\n",
      "Epoch 6 sec\n",
      "Test set accuracy 0.9029000401496887\n",
      "Epoch 7 sec\n",
      "Test set accuracy 0.9031000137329102\n",
      "Epoch 8 sec\n",
      "Test set accuracy 0.9109000563621521\n",
      "Epoch 9 sec\n",
      "Test set accuracy 0.9129000306129456\n"
     ]
    }
   ],
   "source": [
    "for epoch in range(num_epochs):\n",
    "    for batch in training_iterator:\n",
    "        model = update_parallel(model, batch)\n",
    "        \n",
    "    test_acc = accuracy(tree_map(lambda x: x[0], model), validation_iterator)\n",
    "    \n",
    "    print(f\"Epoch {epoch} sec\")\n",
    "    print(f\"Test set accuracy {test_acc}\")\n"
   ]
  }
 ],
 "metadata": {
  "celltoolbar": "Raw Cell Format",
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
