{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Training neural network with DALI and Pax\n",
    "\n",
    "This simple example shows how to train a neural network implemented in JAX with DALI pipelines. It builds on MNIST training example from Pax codebse that can be found [here](https://github.com/google/paxml/blob/paxml-v1.1.0/paxml/tasks/vision/params/mnist.py).\n",
    "\n",
    "We will use MNIST in Caffe2 format from [DALI_extra](https://github.com/NVIDIA/DALI_extra)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "training_data_path = os.path.join(os.environ['DALI_EXTRA_PATH'], 'db/MNIST/training/')\n",
    "validation_data_path = os.path.join(os.environ['DALI_EXTRA_PATH'], 'db/MNIST/testing/')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First step is to create a pipeline definition function that will later be used to create instances of DALI pipelines. It defines all steps of the preprocessing. In this simple example we have `fn.readers.caffe2` for reading data in Caffe2 format, `fn.decoders.image` for image decoding, `fn.crop_mirror_normalize` used to normalize the images and `fn.reshape` to adjust the shape of the output tensors.\n",
    "\n",
    "\n",
    "This example focuses on how to use DALI pipeline with Pax. For more information on DALI pipeline look into [Getting started](../../getting_started.ipynb) and [pipeline documentation](../../../pipeline.rst)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nvidia.dali import pipeline_def\n",
    "import nvidia.dali.fn as fn\n",
    "import nvidia.dali.types as types\n",
    "\n",
    "\n",
    "@pipeline_def(device_id=0, num_threads=4, seed=0)\n",
    "def mnist_pipeline(data_path, random_shuffle):\n",
    "    jpegs, labels = fn.readers.caffe2(\n",
    "        path=data_path,\n",
    "        random_shuffle=random_shuffle,\n",
    "        name=\"mnist_caffe2_reader\")\n",
    "    images = fn.decoders.image(\n",
    "        jpegs, device='mixed', output_type=types.GRAY)\n",
    "    images = fn.crop_mirror_normalize(\n",
    "        images, dtype=types.FLOAT, std=[255.], output_layout=\"HWC\")\n",
    "\n",
    "    labels = labels.gpu()\n",
    "    labels = fn.reshape(labels, shape=[])\n",
    "\n",
    "    return images, labels"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This example uses Pax data input defined in Praxis. We will create a simple wrapper that uses `DALIGenericIterator` for JAX to connect DALI and PAX `Experiment`. To learn more about how DALI interfaces with JAX look int [basic DALI and JAX tutorial](jax-basic_example.ipynb)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from praxis import base_input\n",
    "from nvidia.dali.plugin import jax as dax\n",
    "\n",
    "\n",
    "class MnistDaliInput(base_input.BaseInput):\n",
    "    def __post_init__(self):\n",
    "      super().__post_init__()\n",
    "      \n",
    "      data_path = training_data_path if self.is_training else validation_data_path\n",
    "      \n",
    "      training_pipeline = mnist_pipeline(data_path=data_path, random_shuffle=self.is_training, batch_size=self.batch_size)\n",
    "      self._iterator = dax.DALIGenericIterator(\n",
    "        training_pipeline,\n",
    "        output_map=[\"inputs\", \"labels\"],\n",
    "        reader_name=\"mnist_caffe2_reader\",\n",
    "        auto_reset=True)\n",
    "      \n",
    "    def get_next(self):\n",
    "      try:\n",
    "        return next(self._iterator)\n",
    "      except StopIteration:\n",
    "        self._iterator.reset()\n",
    "        return next(self._iterator)\n",
    "    \n",
    "    \n",
    "    def reset(self) -> None:\n",
    "      super().reset()\n",
    "      self._iterator = self._iterator.reset()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we run model training with Pax."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "I0926 00:50:18.542376 140035128160896 py_utils.py:1015] [PAX STATUS]: E2E time: Starting timer for <_main> @ <.../paxml/main.py:445>\n",
      "I0926 00:50:18.542492 140035128160896 main.py:450] [PAX STATUS]: Program start.\n",
      "I0926 00:50:18.542548 140035128160896 py_utils.py:1015] [PAX STATUS]: Starting timer for <setup_jax> @ <.../paxml/main.py:465>\n",
      "I0926 00:50:18.617591 140035128160896 xla_bridge.py:622] Unable to initialize backend 'rocm': NOT_FOUND: Could not find registered platform with name: \"rocm\". Available platform names are: CUDA Interpreter\n",
      "I0926 00:50:18.617926 140035128160896 xla_bridge.py:622] Unable to initialize backend 'tpu': module 'jaxlib.xla_extension' has no attribute 'get_tpu_client'\n",
      "I0926 00:50:18.618066 140035128160896 setup_jax.py:78] JAX process: 0 / 1\n",
      "I0926 00:50:18.618119 140035128160896 setup_jax.py:79] JAX devices: [gpu(id=0)]\n",
      "I0926 00:50:18.618341 140035128160896 setup_jax.py:80] jax.device_count(): 1\n",
      "I0926 00:50:18.618447 140035128160896 setup_jax.py:81] jax.local_device_count(): 1\n",
      "I0926 00:50:18.618483 140035128160896 setup_jax.py:82] jax.process_count(): 1\n",
      "I0926 00:50:18.618521 140035128160896 py_utils.py:1025] [PAX STATUS]: Elapsed time for <setup_jax>: 0.08 seconds  (@ <.../paxml/main.py:465>)\n",
      "I0926 00:50:18.618578 140035128160896 py_utils.py:1015] [PAX STATUS]: Starting timer for <get_experiment> @ <.../paxml/main.py:475>\n",
      "Registered experiment `pax_examples.dali_pax_example.MnistExperiment`\n",
      "I0926 00:50:19.118913 140035128160896 py_utils.py:1025] [PAX STATUS]: Elapsed time for <get_experiment>: 0.50 seconds  (@ <.../paxml/main.py:475>)\n",
      "I0926 00:50:19.119021 140035128160896 py_utils.py:1015] [PAX STATUS]: Starting timer for <run> @ <.../paxml/main.py:487>\n",
      "I0926 00:50:19.119059 140035128160896 py_utils.py:1015] [PAX STATUS]: Starting timer for <_setup_xm_work_unit> @ <.../paxml/main.py:404>\n",
      "I0926 00:50:19.119102 140035128160896 local.py:45] Setting task status: process_index: 0, process_count: 1\n",
      "I0926 00:50:19.119293 140035128160896 local.py:50] Created artifact job_log_dir of type ArtifactType.DIRECTORY and value /tmp/dali_pax_logs.\n",
      "I0926 00:50:19.119323 140035128160896 py_utils.py:1025] [PAX STATUS]: Elapsed time for <_setup_xm_work_unit>: 0.00 seconds  (@ <.../paxml/main.py:404>)\n",
      "I0926 00:50:19.119359 140035128160896 py_utils.py:1015] [PAX STATUS]: Starting timer for <get_search_space> @ <.../paxml/main.py:416>\n",
      "I0926 00:50:19.125236 140035128160896 py_utils.py:1025] [PAX STATUS]: Elapsed time for <get_search_space>: 0.01 seconds  (@ <.../paxml/main.py:416>)\n",
      "I0926 00:50:19.126615 140035128160896 py_utils.py:1015] [PAX STATUS]: Starting timer for <run_experiment> @ <.../paxml/main.py:420>\n",
      "I0926 00:50:19.126947 140035128160896 py_utils.py:1015] [PAX STATUS]: Starting timer for <write_hparams_file> @ <.../paxml/main.py:276>\n",
      "I0926 00:50:19.133759 140035128160896 py_utils.py:1025] [PAX STATUS]: Elapsed time for <write_hparams_file>: 0.01 seconds  (@ <.../paxml/main.py:276>)\n",
      "I0926 00:50:19.137744 140035128160896 local.py:45] Setting task status: Train experiment pax_examples.dali_pax_example.MnistExperiment at /tmp/dali_pax_logs\n",
      "I0926 00:50:19.137817 140035128160896 py_utils.py:1015] [PAX STATUS]: Starting timer for <train_and_evaluate> @ <.../paxml/main.py:285>\n",
      "I0926 00:50:19.142003 140035128160896 train.py:149] [PAX STATUS]: Getting dataset configurations.\n",
      "I0926 00:50:19.143289 140035128160896 train.py:158] [PAX STATUS]: Done getting dataset configurations.\n",
      "I0926 00:50:19.143336 140035128160896 train.py:160] train_input_p:\n",
      "I0926 00:50:19.143484 140035128160896 train.py:164]   batch_padding_size : 0\n",
      "I0926 00:50:19.143517 140035128160896 train.py:164]   batch_size : 1024\n",
      "I0926 00:50:19.143543 140035128160896 train.py:164]   cls : type/pax_examples.dali_pax_input/MnistDaliInput\n",
      "I0926 00:50:19.143567 140035128160896 train.py:164]   custom_device_order : NoneType\n",
      "I0926 00:50:19.143589 140035128160896 train.py:164]   eval_loop_num_batches : 1\n",
      "I0926 00:50:19.143611 140035128160896 train.py:164]   experimental_remote_input : False\n",
      "I0926 00:50:19.143632 140035128160896 train.py:164]   infeed_host_index : 0\n",
      "I0926 00:50:19.143653 140035128160896 train.py:164]   input_checkpointing_enabled : False\n",
      "I0926 00:50:19.143674 140035128160896 train.py:164]   input_random_seed : NoneType\n",
      "I0926 00:50:19.143696 140035128160896 train.py:164]   is_training : True\n",
      "I0926 00:50:19.143716 140035128160896 train.py:164]   name : ''\n",
      "I0926 00:50:19.143738 140035128160896 train.py:164]   num_infeed_hosts : 0\n",
      "I0926 00:50:19.143759 140035128160896 train.py:164]   reset_for_eval : False\n",
      "I0926 00:50:19.143780 140035128160896 train.py:164]   tf_data_service_address : NoneType\n",
      "I0926 00:50:19.143802 140035128160896 train.py:165] task_p:\n",
      "I0926 00:50:19.144927 140035128160896 train.py:167]   cls : type/paxml.tasks_lib/SingleTask\n",
      "I0926 00:50:19.144968 140035128160896 train.py:167]   decode.cls : type/paxml.tasks_lib/SingleTask.Decode\n",
      "I0926 00:50:19.144994 140035128160896 train.py:167]   decode.prng_key_fold_with_batch_index : False\n",
      "I0926 00:50:19.145018 140035128160896 train.py:167]   decode.profiler_capture_step : 1\n",
      "I0926 00:50:19.145040 140035128160896 train.py:167]   decode.profiler_max_num_hosts : NoneType\n",
      "I0926 00:50:19.145062 140035128160896 train.py:167]   decode.profiler_min_duration_sec : 1.0\n",
      "I0926 00:50:19.145083 140035128160896 train.py:167]   decode.profiler_num_steps : 0\n",
      "I0926 00:50:19.145104 140035128160896 train.py:167]   decode.random_seed : 1234\n",
      "I0926 00:50:19.145126 140035128160896 train.py:167]   early_stopping_fn : NoneType\n",
      "I0926 00:50:19.145147 140035128160896 train.py:167]   evaluate.apply_mutable_list : ['aux_loss', 'summaries', 'non_trainable']\n",
      "I0926 00:50:19.145168 140035128160896 train.py:167]   evaluate.cls : type/paxml.tasks_lib/SingleTask.Evaluate\n",
      "I0926 00:50:19.145189 140035128160896 train.py:167]   evaluate.random_seed : 1234\n",
      "I0926 00:50:19.145210 140035128160896 train.py:167]   infer.cls : type/paxml.tasks_lib/SingleTask.Infer\n",
      "I0926 00:50:19.145230 140035128160896 train.py:167]   infer.random_seed : 1234\n",
      "I0926 00:50:19.145251 140035128160896 train.py:167]   infer_writer : NoneType\n",
      "I0926 00:50:19.145272 140035128160896 train.py:167]   loss_aggregator : NoneType\n",
      "I0926 00:50:19.145292 140035128160896 train.py:167]   metrics : NoneType\n",
      "I0926 00:50:19.145313 140035128160896 train.py:167]   model.activation_split_dims_mapping.out : NoneType\n",
      "I0926 00:50:19.145334 140035128160896 train.py:167]   model.cls : type/pax_examples.dali_pax_example/CNNModel\n",
      "I0926 00:50:19.145355 140035128160896 train.py:167]   model.contiguous_submeshes : NoneType\n",
      "I0926 00:50:19.145375 140035128160896 train.py:167]   model.dcn_mesh_shape : NoneType\n",
      "I0926 00:50:19.145396 140035128160896 train.py:167]   model.dtype : type/jax.numpy/float32\n",
      "I0926 00:50:19.145417 140035128160896 train.py:167]   model.fprop_dtype : Float64DType\n",
      "I0926 00:50:19.145437 140035128160896 train.py:167]   model.ici_mesh_shape : [1, 1, 1]\n",
      "I0926 00:50:19.145458 140035128160896 train.py:167]   model.mesh_axis_names : ('replica', 'data', 'mdl')\n",
      "I0926 00:50:19.145479 140035128160896 train.py:167]   model.name : NoneType\n",
      "I0926 00:50:19.145500 140035128160896 train.py:167]   model.network_tpl.activation_split_dims_mapping.out : NoneType\n",
      "I0926 00:50:19.145521 140035128160896 train.py:167]   model.network_tpl.activation_tpl.activation_split_dims_mapping.out : NoneType\n",
      "I0926 00:50:19.145543 140035128160896 train.py:167]   model.network_tpl.activation_tpl.cls : type/praxis.layers.activations/ReLU\n",
      "I0926 00:50:19.145564 140035128160896 train.py:167]   model.network_tpl.activation_tpl.contiguous_submeshes : NoneType\n",
      "I0926 00:50:19.145585 140035128160896 train.py:167]   model.network_tpl.activation_tpl.dcn_mesh_shape : NoneType\n",
      "I0926 00:50:19.145606 140035128160896 train.py:167]   model.network_tpl.activation_tpl.dtype : type/jax.numpy/float32\n",
      "I0926 00:50:19.145627 140035128160896 train.py:167]   model.network_tpl.activation_tpl.fprop_dtype : NoneType\n",
      "I0926 00:50:19.145648 140035128160896 train.py:167]   model.network_tpl.activation_tpl.ici_mesh_shape : NoneType\n",
      "I0926 00:50:19.145669 140035128160896 train.py:167]   model.network_tpl.activation_tpl.mesh_axis_names : NoneType\n",
      "I0926 00:50:19.145690 140035128160896 train.py:167]   model.network_tpl.activation_tpl.name : NoneType\n",
      "I0926 00:50:19.145712 140035128160896 train.py:167]   model.network_tpl.activation_tpl.params_init.cls : type/praxis.base_layer/WeightInit\n",
      "I0926 00:50:19.145733 140035128160896 train.py:167]   model.network_tpl.activation_tpl.params_init.method : 'xavier'\n",
      "I0926 00:50:19.145754 140035128160896 train.py:167]   model.network_tpl.activation_tpl.params_init.scale : 1.000001\n",
      "I0926 00:50:19.145775 140035128160896 train.py:167]   model.network_tpl.activation_tpl.shared_weight_layer_id : NoneType\n",
      "I0926 00:50:19.145796 140035128160896 train.py:167]   model.network_tpl.activation_tpl.skip_lp_regularization : NoneType\n",
      "I0926 00:50:19.145817 140035128160896 train.py:167]   model.network_tpl.activation_tpl.weight_split_dims_mapping.wt : NoneType\n",
      "I0926 00:50:19.145838 140035128160896 train.py:167]   model.network_tpl.cls : type/pax_examples.dali_pax_example/CNN\n",
      "I0926 00:50:19.145859 140035128160896 train.py:167]   model.network_tpl.contiguous_submeshes : NoneType\n",
      "I0926 00:50:19.145880 140035128160896 train.py:167]   model.network_tpl.dcn_mesh_shape : NoneType\n",
      "I0926 00:50:19.145901 140035128160896 train.py:167]   model.network_tpl.dtype : type/jax.numpy/float32\n",
      "I0926 00:50:19.145923 140035128160896 train.py:167]   model.network_tpl.fprop_dtype : NoneType\n",
      "I0926 00:50:19.145944 140035128160896 train.py:167]   model.network_tpl.height : 28\n",
      "I0926 00:50:19.145965 140035128160896 train.py:167]   model.network_tpl.ici_mesh_shape : NoneType\n",
      "I0926 00:50:19.145987 140035128160896 train.py:167]   model.network_tpl.kernel_size : 3\n",
      "I0926 00:50:19.146007 140035128160896 train.py:167]   model.network_tpl.mesh_axis_names : NoneType\n",
      "I0926 00:50:19.146034 140035128160896 train.py:167]   model.network_tpl.name : 'mnist_model'\n",
      "I0926 00:50:19.146054 140035128160896 train.py:167]   model.network_tpl.num_classes : 10\n",
      "I0926 00:50:19.146075 140035128160896 train.py:167]   model.network_tpl.params_init.cls : type/praxis.base_layer/WeightInit\n",
      "I0926 00:50:19.146096 140035128160896 train.py:167]   model.network_tpl.params_init.method : 'xavier'\n",
      "I0926 00:50:19.146117 140035128160896 train.py:167]   model.network_tpl.params_init.scale : 1.000001\n",
      "I0926 00:50:19.146138 140035128160896 train.py:167]   model.network_tpl.shared_weight_layer_id : NoneType\n",
      "I0926 00:50:19.146159 140035128160896 train.py:167]   model.network_tpl.skip_lp_regularization : NoneType\n",
      "I0926 00:50:19.146180 140035128160896 train.py:167]   model.network_tpl.weight_split_dims_mapping.wt : NoneType\n",
      "I0926 00:50:19.146201 140035128160896 train.py:167]   model.network_tpl.width : 28\n",
      "I0926 00:50:19.146222 140035128160896 train.py:167]   model.params_init.cls : type/praxis.base_layer/WeightInit\n",
      "I0926 00:50:19.146243 140035128160896 train.py:167]   model.params_init.method : 'xavier'\n",
      "I0926 00:50:19.146264 140035128160896 train.py:167]   model.params_init.scale : 1.000001\n",
      "I0926 00:50:19.146285 140035128160896 train.py:167]   model.shared_weight_layer_id : NoneType\n",
      "I0926 00:50:19.146306 140035128160896 train.py:167]   model.skip_lp_regularization : NoneType\n",
      "I0926 00:50:19.146327 140035128160896 train.py:167]   model.weight_split_dims_mapping.wt : NoneType\n",
      "I0926 00:50:19.146348 140035128160896 train.py:167]   name : 'mnist_task'\n",
      "I0926 00:50:19.146369 140035128160896 train.py:167]   summary_verbosity : 3\n",
      "I0926 00:50:19.146389 140035128160896 train.py:167]   train.always_use_train_for_model_init : True\n",
      "I0926 00:50:19.146410 140035128160896 train.py:167]   train.apply_mutable_list : ['aux_loss', 'summaries', 'non_trainable', 'batch_stats', 'params_axes']\n",
      "I0926 00:50:19.146432 140035128160896 train.py:167]   train.async_summary_writing : True\n",
      "I0926 00:50:19.146453 140035128160896 train.py:167]   train.cls : type/paxml.tasks_lib/SingleTask.Train\n",
      "I0926 00:50:19.146474 140035128160896 train.py:167]   train.decode_interval_steps : NoneType\n",
      "I0926 00:50:19.146495 140035128160896 train.py:167]   train.decode_start_after_n_steps : 0\n",
      "I0926 00:50:19.146516 140035128160896 train.py:167]   train.decode_use_ema_states : False\n",
      "I0926 00:50:19.146537 140035128160896 train.py:167]   train.enable_input_checkpointing : False\n",
      "I0926 00:50:19.146558 140035128160896 train.py:167]   train.enforce_input_specs : True\n",
      "I0926 00:50:19.146579 140035128160896 train.py:167]   train.eval_interval_steps : 100\n",
      "I0926 00:50:19.146617 140035128160896 train.py:167]   train.eval_skip_train : True\n",
      "I0926 00:50:19.146640 140035128160896 train.py:167]   train.eval_use_ema_states : False\n",
      "I0926 00:50:19.146663 140035128160896 train.py:167]   train.external_checkpoint_handler : NoneType\n",
      "I0926 00:50:19.146687 140035128160896 train.py:167]   train.external_checkpoint_path : NoneType\n",
      "I0926 00:50:19.146710 140035128160896 train.py:167]   train.inputs_split_mapping : NoneType\n",
      "I0926 00:50:19.146733 140035128160896 train.py:167]   train.learner.check_valid_step : True\n",
      "I0926 00:50:19.146756 140035128160896 train.py:167]   train.learner.cls : type/paxml.learners/Learner\n",
      "I0926 00:50:19.146778 140035128160896 train.py:167]   train.learner.enable_skip_step_on_gradient_anomalies : True\n",
      "I0926 00:50:19.146801 140035128160896 train.py:167]   train.learner.force_repeat_prefix_structure : False\n",
      "I0926 00:50:19.146825 140035128160896 train.py:167]   train.learner.grad_norm_individual_vars : False\n",
      "I0926 00:50:19.146848 140035128160896 train.py:167]   train.learner.grad_norm_summary : True\n",
      "I0926 00:50:19.146871 140035128160896 train.py:167]   train.learner.keep_optimizer_state_for_excluded_vars : False\n",
      "I0926 00:50:19.146894 140035128160896 train.py:167]   train.learner.loss_name : 'loss'\n",
      "I0926 00:50:19.146917 140035128160896 train.py:167]   train.learner.name : ''\n",
      "I0926 00:50:19.146940 140035128160896 train.py:167]   train.learner.optimizer.clip_gradient_norm_to_value : 0.0\n",
      "I0926 00:50:19.146964 140035128160896 train.py:167]   train.learner.optimizer.clip_gradient_single_norm_to_value : 0.0\n",
      "I0926 00:50:19.146986 140035128160896 train.py:167]   train.learner.optimizer.cls : type/praxis.optimizers/ShardedSgd\n",
      "I0926 00:50:19.147010 140035128160896 train.py:167]   train.learner.optimizer.decoupled_weight_decay : NoneType\n",
      "I0926 00:50:19.147033 140035128160896 train.py:167]   train.learner.optimizer.ema_decay : 0.0\n",
      "I0926 00:50:19.147056 140035128160896 train.py:167]   train.learner.optimizer.ewc_regularizer_weight : 0.0\n",
      "I0926 00:50:19.147079 140035128160896 train.py:167]   train.learner.optimizer.ewc_weight_per_var : NoneType\n",
      "I0926 00:50:19.147102 140035128160896 train.py:167]   train.learner.optimizer.l1_regularizer_weight : NoneType\n",
      "I0926 00:50:19.147125 140035128160896 train.py:167]   train.learner.optimizer.l2_regularizer_weight : NoneType\n",
      "I0926 00:50:19.147148 140035128160896 train.py:167]   train.learner.optimizer.learning_rate : 0.01\n",
      "I0926 00:50:19.147171 140035128160896 train.py:167]   train.learner.optimizer.lr_schedule.cls : type/praxis.schedules/Constant\n",
      "I0926 00:50:19.147194 140035128160896 train.py:167]   train.learner.optimizer.lr_schedule.name : ''\n",
      "I0926 00:50:19.147217 140035128160896 train.py:167]   train.learner.optimizer.lr_schedule.value : 1\n",
      "I0926 00:50:19.147240 140035128160896 train.py:167]   train.learner.optimizer.momentum : 0.1\n",
      "I0926 00:50:19.147264 140035128160896 train.py:167]   train.learner.optimizer.name : ''\n",
      "I0926 00:50:19.147296 140035128160896 train.py:167]   train.learner.optimizer.nesterov : False\n",
      "I0926 00:50:19.147317 140035128160896 train.py:167]   train.learner.optimizer.skip_lp_1d_vectors : False\n",
      "I0926 00:50:19.147338 140035128160896 train.py:167]   train.learner.repeat_prefix_sep : '#'\n",
      "I0926 00:50:19.147359 140035128160896 train.py:167]   train.learner.scale_update_by_var_norm : False\n",
      "I0926 00:50:19.147380 140035128160896 train.py:167]   train.learner.skip_step_gradient_norm_value : 0.0\n",
      "I0926 00:50:19.147401 140035128160896 train.py:167]   train.learner.skip_zero_gradients : NoneType\n",
      "I0926 00:50:19.147422 140035128160896 train.py:167]   train.learner.stochastic_gradient : NoneType\n",
      "I0926 00:50:19.147444 140035128160896 train.py:167]   train.learner.var_norm_summary : True\n",
      "I0926 00:50:19.147465 140035128160896 train.py:167]   train.learner.vectorize_on_repeat_prefix : True\n",
      "I0926 00:50:19.147486 140035128160896 train.py:167]   train.log_train_output_interval_steps : NoneType\n",
      "I0926 00:50:19.147507 140035128160896 train.py:167]   train.max_inflight_steps : 2\n",
      "I0926 00:50:19.147528 140035128160896 train.py:167]   train.num_train_steps : 1000\n",
      "I0926 00:50:19.147549 140035128160896 train.py:167]   train.profiler_capture_step : NoneType\n",
      "I0926 00:50:19.147570 140035128160896 train.py:167]   train.profiler_max_num_hosts : NoneType\n",
      "I0926 00:50:19.147591 140035128160896 train.py:167]   train.profiler_min_duration_sec : 1.0\n",
      "I0926 00:50:19.147612 140035128160896 train.py:167]   train.profiler_num_steps : 2\n",
      "I0926 00:50:19.147633 140035128160896 train.py:167]   train.random_seed : 1234\n",
      "I0926 00:50:19.147654 140035128160896 train.py:167]   train.restore_transformations : NoneType\n",
      "I0926 00:50:19.147675 140035128160896 train.py:167]   train.save_interval_steps : 5000\n",
      "I0926 00:50:19.147696 140035128160896 train.py:167]   train.save_keep_interval_duration : '12h'\n",
      "I0926 00:50:19.147717 140035128160896 train.py:167]   train.save_max_to_keep : 10\n",
      "I0926 00:50:19.147738 140035128160896 train.py:167]   train.summary_accumulate_interval_steps : NoneType\n",
      "I0926 00:50:19.147759 140035128160896 train.py:167]   train.summary_interval_steps : 100\n",
      "I0926 00:50:19.147780 140035128160896 train.py:167]   train.tensorstore_metadata_key : NoneType\n",
      "I0926 00:50:19.147801 140035128160896 train.py:167]   train.variable_norm_summary : True\n",
      "I0926 00:50:19.147822 140035128160896 train.py:167]   vn.cls : type/paxml.tasks_lib/SingleTask.VariationalNoise\n",
      "I0926 00:50:19.147843 140035128160896 train.py:167]   vn.vn_regex : ''\n",
      "I0926 00:50:19.147864 140035128160896 train.py:167]   vn.vn_scale : 0.0\n",
      "I0926 00:50:19.147885 140035128160896 train.py:167]   vn.vn_start_step : 0\n",
      "I0926 00:50:19.147908 140035128160896 train.py:170] [PAX STATUS]: Creating task\n",
      "I0926 00:50:19.153830 140035128160896 py_utils.py:1015] [PAX STATUS]: Starting timer for <_create_checkpointer> @ <.../paxml/train.py:178>\n",
      "I0926 00:50:19.153895 140035128160896 checkpoint_creators.py:555] [PAX STATUS]: Creating checkpointer.\n",
      "I0926 00:50:19.154047 140035128160896 py_utils.py:339] Starting sync_global_devices checkpointer:makedirs:/tmp/dali_pax_logs/checkpoints across 1 devices globally\n",
      "W0926 00:50:19.324969 140035128160896 dispatch.py:271] Finished tracing + transforming _reduce_sum for pjit in 0.00054931640625 sec\n",
      "W0926 00:50:19.325407 140035128160896 dispatch.py:271] Finished tracing + transforming _psum for pjit in 0.001432657241821289 sec\n",
      "W0926 00:50:19.327380 140035128160896 pxla.py:1764] Compiling _psum for with global shapes and types [ShapedArray(uint32[1])]. Argument mapping: (GSPMDSharding({replicated}),).\n",
      "W0926 00:50:19.329179 140035128160896 dispatch.py:271] Finished jaxpr to MLIR module conversion jit(_psum) in 0.001672983169555664 sec\n",
      "W0926 00:50:19.382656 140035128160896 dispatch.py:271] Finished XLA compilation of jit(_psum) in 0.053209543228149414 sec\n",
      "I0926 00:50:19.387668 140035128160896 py_utils.py:342] Finished sync_global_devices checkpointer:makedirs:/tmp/dali_pax_logs/checkpoints across 1 devices globally\n",
      "I0926 00:50:19.389428 140035128160896 utils.py:400] Cleaning up existing temporary directories at /tmp/dali_pax_logs/checkpoints.\n",
      "I0926 00:50:19.390844 140035128160896 py_utils.py:1025] [PAX STATUS]: Elapsed time for <_create_checkpointer>: 0.24 seconds  (@ <.../paxml/train.py:178>)\n",
      "I0926 00:50:19.390908 140035128160896 train.py:204] [PAX STATUS]: Initializing partitioner\n",
      "I0926 00:50:19.390957 140035128160896 partitioning.py:759] Using SPMD sharding for model parallelism.\n",
      "I0926 00:50:19.390985 140035128160896 partitioning.py:765] creating mesh with py_utils.create_device_mesh\n",
      "I0926 00:50:19.391064 140035128160896 py_utils.py:700] device_mesh: [[[gpu(id=0)]]]\n",
      "I0926 00:50:19.391356 140035128160896 partitioning.py:773] device_mesh: [[[gpu(id=0)]]]\n",
      "I0926 00:50:19.392432 140035128160896 partitioning.py:426] input_p.tf_data_service_address: None\n",
      "I0926 00:50:19.392513 140035128160896 train.py:227] [PAX STATUS]: Initializing train program.\n",
      "I0926 00:50:19.392561 140035128160896 train.py:230] [PAX STATUS]: Initializing eval programs.\n",
      "I0926 00:50:19.392587 140035128160896 train.py:239] [PAX STATUS]: Initializing decode programs.\n",
      "I0926 00:50:19.392611 140035128160896 train.py:255] [PAX STATUS]: Creating executor.\n",
      "I0926 00:50:19.392641 140035128160896 train.py:259] [PAX STATUS]: Setting up executor.\n",
      "/home/awolant/Projects/DALI/dali/operators/reader/loader/lmdb.h:61: lmdb 0 /home/awolant/Projects/DALI_extra/db/MNIST/training/ has 60000 entries\n",
      "/home/awolant/Projects/DALI/dali/operators/reader/loader/lmdb.h:76: lmdb 0 /home/awolant/Projects/DALI_extra/db/MNIST/training/ rewind to the begin from 0\n",
      "/home/awolant/Projects/DALI/dali/operators/decoder/nvjpeg/nvjpeg_decoder_decoupled_api.h:192: NVJPEG_BACKEND_HARDWARE is either disabled or not supported\n",
      "/home/awolant/Projects/DALI/dali/operators/reader/loader/lmdb.h:76: lmdb 0 /home/awolant/Projects/DALI_extra/db/MNIST/training/ rewind to the begin from 0\n",
      "W0926 00:50:19.695808 140035128160896 dispatch.py:271] Finished tracing + transforming jit(copy) in 0.0002460479736328125 sec\n",
      "W0926 00:50:19.696140 140035128160896 pxla.py:1764] Compiling copy for with global shapes and types [ShapedArray(float32[1024,28,28,1])]. Argument mapping: (GSPMDSharding({replicated}),).\n",
      "W0926 00:50:19.697573 140035128160896 dispatch.py:271] Finished jaxpr to MLIR module conversion jit(copy) in 0.0012850761413574219 sec\n",
      "W0926 00:50:19.711167 140035128160896 dispatch.py:271] Finished XLA compilation of jit(copy) in 0.013259172439575195 sec\n",
      "W0926 00:50:19.712627 140035128160896 dispatch.py:271] Finished tracing + transforming jit(copy) in 0.00018024444580078125 sec\n",
      "W0926 00:50:19.712842 140035128160896 pxla.py:1764] Compiling copy for with global shapes and types [ShapedArray(int32[1024])]. Argument mapping: (GSPMDSharding({replicated}),).\n",
      "W0926 00:50:19.713848 140035128160896 dispatch.py:271] Finished jaxpr to MLIR module conversion jit(copy) in 0.0008952617645263672 sec\n",
      "W0926 00:50:19.724873 140035128160896 dispatch.py:271] Finished XLA compilation of jit(copy) in 0.010770797729492188 sec\n",
      "W0926 00:50:19.725575 140035128160896 base_input.py:1108] b/292156360: The input specs is generated based on the first data batch. It is recommended to define an explicit input spec provider param in BaseExperiment.get_input_specs_provider_params(), which is more deterministic and efficient.\n",
      "I0926 00:50:19.725647 140035128160896 py_utils.py:1015] [PAX STATUS]: Starting timer for <get_next_padded> @ <.../praxis/base_input.py:1116>\n",
      "I0926 00:50:19.725683 140035128160896 py_utils.py:1025] [PAX STATUS]: Elapsed time for <get_next_padded>: 0.00 seconds  (@ <.../praxis/base_input.py:1116>)\n",
      "W0926 00:50:19.726252 140035128160896 dispatch.py:271] Finished tracing + transforming jit(convert_element_type) in 0.00021791458129882812 sec\n",
      "W0926 00:50:19.763960 140035128160896 dispatch.py:271] Finished tracing + transforming <lambda> for pjit in 0.03663444519042969 sec\n",
      "W0926 00:50:19.764972 140035128160896 dispatch.py:271] Finished tracing + transforming _threefry_seed for pjit in 0.03818631172180176 sec\n",
      "W0926 00:50:19.765537 140035128160896 pxla.py:1764] Compiling _threefry_seed for with global shapes and types [ShapedArray(int32[])]. Argument mapping: (GSPMDSharding({replicated}),).\n",
      "W0926 00:50:19.768255 140035128160896 dispatch.py:271] Finished jaxpr to MLIR module conversion jit(_threefry_seed) in 0.00258636474609375 sec\n",
      "W0926 00:50:19.816388 140035128160896 dispatch.py:271] Finished XLA compilation of jit(_threefry_seed) in 0.04785418510437012 sec\n",
      "W0926 00:50:19.817826 140035128160896 dispatch.py:271] Finished tracing + transforming jit(concatenate) in 0.000232696533203125 sec\n",
      "W0926 00:50:19.818121 140035128160896 pxla.py:1764] Compiling concatenate for with global shapes and types [ShapedArray(uint32[2]), ShapedArray(uint32[2])]. Argument mapping: (GSPMDSharding({replicated}), GSPMDSharding({replicated})).\n",
      "W0926 00:50:19.819451 140035128160896 dispatch.py:271] Finished jaxpr to MLIR module conversion jit(concatenate) in 0.0011851787567138672 sec\n",
      "W0926 00:50:19.865855 140035128160896 dispatch.py:271] Finished XLA compilation of jit(concatenate) in 0.04613900184631348 sec\n",
      "I0926 00:50:19.867256 140035128160896 partitioning.py:426] input_p.tf_data_service_address: None\n",
      "I0926 00:50:19.867377 140035128160896 executors.py:142] [PAX STATUS]: Instantiating train input pipeline (<PaxConfig[MnistDaliInput(\n",
      "  batch_size=1024,\n",
      "  num_infeed_hosts=1,\n",
      "  infeed_host_index=0,\n",
      "  is_training=True,\n",
      "  tf_data_service_address=None)]>)\n",
      "/home/awolant/Projects/DALI/dali/operators/reader/loader/lmdb.h:61: lmdb 0 /home/awolant/Projects/DALI_extra/db/MNIST/training/ has 60000 entries\n",
      "/home/awolant/Projects/DALI/dali/operators/reader/loader/lmdb.h:76: lmdb 0 /home/awolant/Projects/DALI_extra/db/MNIST/training/ rewind to the begin from 0\n",
      "/home/awolant/Projects/DALI/dali/operators/decoder/nvjpeg/nvjpeg_decoder_decoupled_api.h:192: NVJPEG_BACKEND_HARDWARE is either disabled or not supported\n",
      "/home/awolant/Projects/DALI/dali/operators/reader/loader/lmdb.h:76: lmdb 0 /home/awolant/Projects/DALI_extra/db/MNIST/training/ rewind to the begin from 0\n",
      "I0926 00:50:19.963162 140035128160896 trainer_lib.py:1662] Spec yielded by InputSpecProvider for model init: {'inputs': ShapeDtypeStruct(shape=(1024, 28, 28, 1), dtype=float32),\n",
      " 'labels': ShapeDtypeStruct(shape=(1024,), dtype=int32)}\n",
      "I0926 00:50:19.963389 140035128160896 trainer_lib.py:1689] Modified spec for pjit global batch size: {'inputs': ShapeDtypeStruct(shape=(1024, 28, 28, 1), dtype=float32),\n",
      " 'labels': ShapeDtypeStruct(shape=(1024,), dtype=int32)}\n",
      "I0926 00:50:19.963435 140035128160896 executors.py:201] [PAX STATUS]: Setting up partitioner\n",
      "I0926 00:50:19.963467 140035128160896 partitioning.py:356] [PAX STATUS]: Getting input shapes from spec.\n",
      "I0926 00:50:19.963492 140035128160896 executors.py:208] [PAX STATUS]: Getting train state metadata.\n",
      "/home/awolant/.local/lib/python3.10/site-packages/jax/_src/numpy/array_methods.py:728: UserWarning: Explicitly requested dtype float64 requested in astype is not available, and will be truncated to dtype float32. To enable more dtypes, set the jax_enable_x64 configuration option or the JAX_ENABLE_X64 shell environment variable. See https://github.com/google/jax#current-gotchas for more.\n",
      "  return getattr(self.aval, name).fun(self, *args, **kwargs)\n",
      "I0926 00:50:20.076595 140035128160896 executors.py:212] [PAX STATUS]: Writing post init model hparams.\n",
      "I0926 00:50:20.076739 140035128160896 trainer_lib.py:212] post_init_model_params: /tmp/dali_pax_logs/post_init_model_params.txt\n",
      "I0926 00:50:20.312406 140035128160896 executors.py:220] [PAX STATUS]: Starting checkpoint load / variable init.\n",
      "W0926 00:50:20.312867 140035128160896 dispatch.py:271] Finished tracing + transforming jit(reshape) in 0.00013828277587890625 sec\n",
      "W0926 00:50:20.313088 140035128160896 pxla.py:1764] Compiling reshape for with global shapes and types [ShapedArray(uint32[4])]. Argument mapping: (GSPMDSharding({replicated}),).\n",
      "W0926 00:50:20.314186 140035128160896 dispatch.py:271] Finished jaxpr to MLIR module conversion jit(reshape) in 0.0010006427764892578 sec\n",
      "W0926 00:50:20.326616 140035128160896 dispatch.py:271] Finished XLA compilation of jit(reshape) in 0.012186288833618164 sec\n",
      "W0926 00:50:20.328583 140035128160896 dispatch.py:271] Finished tracing + transforming ravel for pjit in 0.00016117095947265625 sec\n",
      "W0926 00:50:20.329236 140035128160896 dispatch.py:271] Finished tracing + transforming threefry_2x32 for pjit in 0.0012459754943847656 sec\n",
      "W0926 00:50:20.330146 140035128160896 dispatch.py:271] Finished tracing + transforming _threefry_split_original for pjit in 0.0024442672729492188 sec\n",
      "W0926 00:50:20.332202 140035128160896 pxla.py:1764] Compiling _threefry_split_original for with global shapes and types [ShapedArray(uint32[2,2])]. Argument mapping: (GSPMDSharding({replicated}),).\n",
      "W0926 00:50:20.334819 140035128160896 dispatch.py:271] Finished jaxpr to MLIR module conversion jit(_threefry_split_original) in 0.0025060176849365234 sec\n",
      "W0926 00:50:20.406200 140035128160896 dispatch.py:271] Finished XLA compilation of jit(_threefry_split_original) in 0.07113766670227051 sec\n",
      "W0926 00:50:20.407783 140035128160896 dispatch.py:271] Finished tracing + transforming jit(transpose) in 0.0002014636993408203 sec\n",
      "W0926 00:50:20.408048 140035128160896 pxla.py:1764] Compiling transpose for with global shapes and types [ShapedArray(uint32[2,2,2])]. Argument mapping: (GSPMDSharding({replicated}),).\n",
      "W0926 00:50:20.409307 140035128160896 dispatch.py:271] Finished jaxpr to MLIR module conversion jit(transpose) in 0.0011415481567382812 sec\n",
      "W0926 00:50:20.457095 140035128160896 dispatch.py:271] Finished XLA compilation of jit(transpose) in 0.047499656677246094 sec\n",
      "W0926 00:50:20.458707 140035128160896 dispatch.py:271] Finished tracing + transforming jit(reshape) in 0.00022339820861816406 sec\n",
      "W0926 00:50:20.459010 140035128160896 pxla.py:1764] Compiling reshape for with global shapes and types [ShapedArray(uint32[2,2,2])]. Argument mapping: (GSPMDSharding({replicated}),).\n",
      "W0926 00:50:20.460231 140035128160896 dispatch.py:271] Finished jaxpr to MLIR module conversion jit(reshape) in 0.0010936260223388672 sec\n",
      "W0926 00:50:20.472781 140035128160896 dispatch.py:271] Finished XLA compilation of jit(reshape) in 0.01228785514831543 sec\n",
      "W0926 00:50:20.474261 140035128160896 dispatch.py:271] Finished tracing + transforming _unstack for pjit in 0.0005447864532470703 sec\n",
      "W0926 00:50:20.474821 140035128160896 pxla.py:1764] Compiling _unstack for with global shapes and types [ShapedArray(uint32[2,4])]. Argument mapping: (GSPMDSharding({replicated}),).\n",
      "W0926 00:50:20.476780 140035128160896 dispatch.py:271] Finished jaxpr to MLIR module conversion jit(_unstack) in 0.0018336772918701172 sec\n",
      "W0926 00:50:20.524481 140035128160896 dispatch.py:271] Finished XLA compilation of jit(_unstack) in 0.04744148254394531 sec\n",
      "I0926 00:50:20.528380 140035128160896 trainer_lib.py:1490] unpadded_out_shape: TrainState(step=(), mdl_vars={'params': {'network': {'conv1': {'b': (32,), 'w': (3, 3, 1, 32)}, 'conv2': {'b': (64,), 'w': (3, 3, 32, 64)}, 'dense_0': {'bias': {'b': (256,)}, 'linear': {'w': (3136, 256)}}, 'dense_1': {'w': (256, 10)}, 'dense_2': {'b': (10,)}}}}, opt_states=[({'count': ()}, {'count': ()}, (TraceState(trace={'params': {'network': {'conv1': {'b': (32,), 'w': (3, 3, 1, 32)}, 'conv2': {'b': (64,), 'w': (3, 3, 32, 64)}, 'dense_0': {'bias': {'b': (256,)}, 'linear': {'w': (3136, 256)}}, 'dense_1': {'w': (256, 10)}, 'dense_2': {'b': (10,)}}}}), ScaleByScheduleState(count=())), {'count': ()})], extra_state=())\n",
      "I0926 00:50:20.528646 140035128160896 trainer_lib.py:1491] train_state_partition_specs: TrainState(step=PartitionSpec(), mdl_vars={'params': {'network': {'conv1': {'b': PartitionSpec(None,), 'w': PartitionSpec(None, None, None, None)}, 'conv2': {'b': PartitionSpec(None,), 'w': PartitionSpec(None, None, None, None)}, 'dense_0': {'bias': {'b': PartitionSpec(None,)}, 'linear': {'w': PartitionSpec(None, None)}}, 'dense_1': {'w': PartitionSpec(None, None)}, 'dense_2': {'b': PartitionSpec(None,)}}}}, opt_states=[({'count': PartitionSpec()}, {'count': PartitionSpec()}, (TraceState(trace={'params': {'network': {'conv1': {'b': PartitionSpec(None,), 'w': PartitionSpec(None, None, None, None)}, 'conv2': {'b': PartitionSpec(None,), 'w': PartitionSpec(None, None, None, None)}, 'dense_0': {'bias': {'b': PartitionSpec(None,)}, 'linear': {'w': PartitionSpec(None, None)}}, 'dense_1': {'w': PartitionSpec(None, None)}, 'dense_2': {'b': PartitionSpec(None,)}}}}), ScaleByScheduleState(count=PartitionSpec())), {'count': PartitionSpec()})], extra_state=())\n",
      "I0926 00:50:20.530107 140035128160896 trainer_lib.py:379] init_var prng_seed: {'params': Traced<ShapedArray(uint32[4])>with<DynamicJaxprTrace(level=1/0)>, 'random': Traced<ShapedArray(uint32[4])>with<DynamicJaxprTrace(level=1/0)>, 'dropout': Traced<ShapedArray(uint32[4])>with<DynamicJaxprTrace(level=1/0)>}\n",
      "I0926 00:50:20.530303 140035128160896 trainer_lib.py:380] var_weight_hparams: {'params': {'network': {'conv1': {'b': WeightHParams(shape=[32], init=WeightInit(method='constant', scale=0.0), dtype=<class 'jax.numpy.float32'>, collections=[], mesh_shape=None, tensor_split_dims_mapping=None, repeat_prefix=None, repeat_prefix_split_dims_mapping=None, repeat_optimizer_dims_mapping=None, fan_in_axes=None, fan_out_axes=None), 'w': WeightHParams(shape=(3, 3, 1, 32), init=WeightInit(method='xavier', scale=1.000001), dtype=<class 'jax.numpy.float32'>, collections=[], mesh_shape=(1, 1, 1), tensor_split_dims_mapping=(-1, -1, -1, -1), repeat_prefix=None, repeat_prefix_split_dims_mapping=None, repeat_optimizer_dims_mapping=None, fan_in_axes=None, fan_out_axes=None)}, 'conv2': {'b': WeightHParams(shape=[64], init=WeightInit(method='constant', scale=0.0), dtype=<class 'jax.numpy.float32'>, collections=[], mesh_shape=None, tensor_split_dims_mapping=None, repeat_prefix=None, repeat_prefix_split_dims_mapping=None, repeat_optimizer_dims_mapping=None, fan_in_axes=None, fan_out_axes=None), 'w': WeightHParams(shape=(3, 3, 32, 64), init=WeightInit(method='xavier', scale=1.000001), dtype=<class 'jax.numpy.float32'>, collections=[], mesh_shape=(1, 1, 1), tensor_split_dims_mapping=(-1, -1, -1, -1), repeat_prefix=None, repeat_prefix_split_dims_mapping=None, repeat_optimizer_dims_mapping=None, fan_in_axes=None, fan_out_axes=None)}, 'dense_0': {'bias': {'b': WeightHParams(shape=[256], init=WeightInit(method='constant', scale=0.0), dtype=<class 'jax.numpy.float32'>, collections=[], mesh_shape=(1, 1, 1), tensor_split_dims_mapping=(-1,), repeat_prefix=None, repeat_prefix_split_dims_mapping=None, repeat_optimizer_dims_mapping=None, fan_in_axes=None, fan_out_axes=None)}, 'linear': {'w': WeightHParams(shape=[3136, 256], init=WeightInit(method='xavier', scale=1.000001), dtype=<class 'jax.numpy.float32'>, collections=[], mesh_shape=(1, 1, 1), tensor_split_dims_mapping=(-1, -1), repeat_prefix=None, repeat_prefix_split_dims_mapping=None, repeat_optimizer_dims_mapping=None, fan_in_axes=None, fan_out_axes=None)}}, 'dense_1': {'w': WeightHParams(shape=[256, 10], init=WeightInit(method='xavier', scale=1.000001), dtype=<class 'jax.numpy.float32'>, collections=[], mesh_shape=(1, 1, 1), tensor_split_dims_mapping=(-1, -1), repeat_prefix=None, repeat_prefix_split_dims_mapping=None, repeat_optimizer_dims_mapping=None, fan_in_axes=None, fan_out_axes=None)}, 'dense_2': {'b': WeightHParams(shape=[10], init=WeightInit(method='constant', scale=0.0), dtype=<class 'jax.numpy.float32'>, collections=[], mesh_shape=(1, 1, 1), tensor_split_dims_mapping=(-1,), repeat_prefix=None, repeat_prefix_split_dims_mapping=None, repeat_optimizer_dims_mapping=None, fan_in_axes=None, fan_out_axes=None)}}}}\n",
      "I0926 00:50:20.555382 140035128160896 base_layer.py:530] Sets tensor_split_dims_mapping of a param of shape (3, 3, 1, 32) to (-1, -1, -1, -1)\n",
      "I0926 00:50:20.555982 140035128160896 base_layer.py:635] Creating var /network/conv1/w with shape=(3, 3, 1, 32), dtype=<class 'jax.numpy.float32'>, init method=xavier and scale=1.000001\n",
      "W0926 00:50:20.557385 140035128160896 dispatch.py:271] Finished tracing + transforming <lambda> for pjit in 0.00035071372985839844 sec\n",
      "W0926 00:50:20.558216 140035128160896 dispatch.py:271] Finished tracing + transforming <lambda> for pjit in 0.0003399848937988281 sec\n",
      "W0926 00:50:20.558960 140035128160896 dispatch.py:271] Finished tracing + transforming fn for pjit in 0.00032711029052734375 sec\n",
      "W0926 00:50:20.559776 140035128160896 dispatch.py:271] Finished tracing + transforming fn for pjit in 0.00040841102600097656 sec\n",
      "W0926 00:50:20.560286 140035128160896 dispatch.py:271] Finished tracing + transforming _uniform for pjit in 0.003947734832763672 sec\n",
      "W0926 00:50:20.561064 140035128160896 dispatch.py:271] Finished tracing + transforming fn for pjit in 0.0003235340118408203 sec\n",
      "I0926 00:50:20.561945 140035128160896 base_layer.py:635] Creating var /network/conv1/b with shape=[32], dtype=<class 'jax.numpy.float32'>, init method=constant and scale=0.0\n",
      "W0926 00:50:20.562677 140035128160896 dispatch.py:271] Finished tracing + transforming fn for pjit in 0.00034499168395996094 sec\n",
      "W0926 00:50:20.567355 140035128160896 dispatch.py:271] Finished tracing + transforming fn for pjit in 0.0003361701965332031 sec\n",
      "W0926 00:50:20.568519 140035128160896 dispatch.py:271] Finished tracing + transforming <lambda> for pjit in 0.00033664703369140625 sec\n",
      "W0926 00:50:20.568961 140035128160896 dispatch.py:271] Finished tracing + transforming relu for pjit in 0.0009648799896240234 sec\n",
      "W0926 00:50:20.570652 140035128160896 dispatch.py:271] Finished tracing + transforming reciprocal for pjit in 0.00026702880859375 sec\n",
      "W0926 00:50:20.571342 140035128160896 dispatch.py:271] Finished tracing + transforming fn for pjit in 0.0003104209899902344 sec\n",
      "I0926 00:50:20.571966 140035128160896 base_layer.py:530] Sets tensor_split_dims_mapping of a param of shape (3, 3, 32, 64) to (-1, -1, -1, -1)\n",
      "I0926 00:50:20.572571 140035128160896 base_layer.py:635] Creating var /network/conv2/w with shape=(3, 3, 32, 64), dtype=<class 'jax.numpy.float32'>, init method=xavier and scale=1.000001\n",
      "W0926 00:50:20.573806 140035128160896 dispatch.py:271] Finished tracing + transforming <lambda> for pjit in 0.000278472900390625 sec\n",
      "W0926 00:50:20.574812 140035128160896 dispatch.py:271] Finished tracing + transforming fn for pjit in 0.00031495094299316406 sec\n",
      "W0926 00:50:20.575534 140035128160896 dispatch.py:271] Finished tracing + transforming fn for pjit in 0.0003147125244140625 sec\n",
      "W0926 00:50:20.576065 140035128160896 dispatch.py:271] Finished tracing + transforming _uniform for pjit in 0.0031881332397460938 sec\n",
      "W0926 00:50:20.576792 140035128160896 dispatch.py:271] Finished tracing + transforming fn for pjit in 0.0003185272216796875 sec\n",
      "I0926 00:50:20.577649 140035128160896 base_layer.py:635] Creating var /network/conv2/b with shape=[64], dtype=<class 'jax.numpy.float32'>, init method=constant and scale=0.0\n",
      "W0926 00:50:20.578456 140035128160896 dispatch.py:271] Finished tracing + transforming fn for pjit in 0.00044655799865722656 sec\n",
      "W0926 00:50:20.583195 140035128160896 dispatch.py:271] Finished tracing + transforming fn for pjit in 0.0003714561462402344 sec\n",
      "W0926 00:50:20.584391 140035128160896 dispatch.py:271] Finished tracing + transforming <lambda> for pjit in 0.00034427642822265625 sec\n",
      "W0926 00:50:20.584835 140035128160896 dispatch.py:271] Finished tracing + transforming relu for pjit in 0.0009756088256835938 sec\n",
      "W0926 00:50:20.586524 140035128160896 dispatch.py:271] Finished tracing + transforming reciprocal for pjit in 0.0003342628479003906 sec\n",
      "W0926 00:50:20.587214 140035128160896 dispatch.py:271] Finished tracing + transforming fn for pjit in 0.0003247261047363281 sec\n",
      "I0926 00:50:20.592664 140035128160896 base_layer.py:530] Sets tensor_split_dims_mapping of a param of shape [3136, 256] to (-1, -1)\n",
      "I0926 00:50:20.593165 140035128160896 base_layer.py:635] Creating var /network/dense_0/linear/w with shape=[3136, 256], dtype=<class 'jax.numpy.float32'>, init method=xavier and scale=1.000001\n",
      "W0926 00:50:20.594397 140035128160896 dispatch.py:271] Finished tracing + transforming <lambda> for pjit in 0.00028514862060546875 sec\n",
      "W0926 00:50:20.595076 140035128160896 dispatch.py:271] Finished tracing + transforming <lambda> for pjit in 0.0002796649932861328 sec\n",
      "W0926 00:50:20.595744 140035128160896 dispatch.py:271] Finished tracing + transforming fn for pjit in 0.0002818107604980469 sec\n",
      "W0926 00:50:20.596404 140035128160896 dispatch.py:271] Finished tracing + transforming fn for pjit in 0.00028705596923828125 sec\n",
      "W0926 00:50:20.596891 140035128160896 dispatch.py:271] Finished tracing + transforming _uniform for pjit in 0.0034263134002685547 sec\n",
      "W0926 00:50:20.597635 140035128160896 dispatch.py:271] Finished tracing + transforming fn for pjit in 0.0003695487976074219 sec\n",
      "W0926 00:50:20.600393 140035128160896 dispatch.py:271] Finished tracing + transforming _einsum for pjit in 0.00045680999755859375 sec\n",
      "I0926 00:50:20.600956 140035128160896 base_layer.py:530] Sets tensor_split_dims_mapping of a param of shape [256] to (-1,)\n",
      "I0926 00:50:20.601426 140035128160896 base_layer.py:635] Creating var /network/dense_0/bias/b with shape=[256], dtype=<class 'jax.numpy.float32'>, init method=constant and scale=0.0\n",
      "W0926 00:50:20.602088 140035128160896 dispatch.py:271] Finished tracing + transforming fn for pjit in 0.00032806396484375 sec\n",
      "W0926 00:50:20.603183 140035128160896 dispatch.py:271] Finished tracing + transforming fn for pjit in 0.0004489421844482422 sec\n",
      "W0926 00:50:20.604482 140035128160896 dispatch.py:271] Finished tracing + transforming <lambda> for pjit in 0.0003361701965332031 sec\n",
      "W0926 00:50:20.604938 140035128160896 dispatch.py:271] Finished tracing + transforming relu for pjit in 0.0010614395141601562 sec\n",
      "I0926 00:50:20.605397 140035128160896 base_layer.py:530] Sets tensor_split_dims_mapping of a param of shape [256, 10] to (-1, -1)\n",
      "I0926 00:50:20.605857 140035128160896 base_layer.py:635] Creating var /network/dense_1/w with shape=[256, 10], dtype=<class 'jax.numpy.float32'>, init method=xavier and scale=1.000001\n",
      "W0926 00:50:20.607065 140035128160896 dispatch.py:271] Finished tracing + transforming <lambda> for pjit in 0.0002777576446533203 sec\n",
      "W0926 00:50:20.607985 140035128160896 dispatch.py:271] Finished tracing + transforming fn for pjit in 0.0002815723419189453 sec\n",
      "W0926 00:50:20.608634 140035128160896 dispatch.py:271] Finished tracing + transforming fn for pjit in 0.0002834796905517578 sec\n",
      "W0926 00:50:20.609114 140035128160896 dispatch.py:271] Finished tracing + transforming _uniform for pjit in 0.0029637813568115234 sec\n",
      "W0926 00:50:20.609832 140035128160896 dispatch.py:271] Finished tracing + transforming fn for pjit in 0.0003180503845214844 sec\n",
      "W0926 00:50:20.612562 140035128160896 dispatch.py:271] Finished tracing + transforming _einsum for pjit in 0.00042438507080078125 sec\n",
      "I0926 00:50:20.613113 140035128160896 base_layer.py:530] Sets tensor_split_dims_mapping of a param of shape [10] to (-1,)\n",
      "I0926 00:50:20.613572 140035128160896 base_layer.py:635] Creating var /network/dense_2/b with shape=[10], dtype=<class 'jax.numpy.float32'>, init method=constant and scale=0.0\n",
      "W0926 00:50:20.614235 140035128160896 dispatch.py:271] Finished tracing + transforming fn for pjit in 0.0003333091735839844 sec\n",
      "W0926 00:50:20.615289 140035128160896 dispatch.py:271] Finished tracing + transforming fn for pjit in 0.00043773651123046875 sec\n",
      "W0926 00:50:20.616391 140035128160896 dispatch.py:271] Finished tracing + transforming _reduce_max for pjit in 0.00043892860412597656 sec\n",
      "W0926 00:50:20.617276 140035128160896 dispatch.py:271] Finished tracing + transforming <lambda> for pjit in 0.0003762245178222656 sec\n",
      "W0926 00:50:20.617853 140035128160896 dispatch.py:271] Finished tracing + transforming <lambda> for pjit in 0.00021767616271972656 sec\n",
      "W0926 00:50:20.618707 140035128160896 dispatch.py:271] Finished tracing + transforming _reduce_sum for pjit in 0.0004756450653076172 sec\n",
      "W0926 00:50:20.619385 140035128160896 dispatch.py:271] Finished tracing + transforming <lambda> for pjit in 0.00021982192993164062 sec\n",
      "W0926 00:50:20.619973 140035128160896 dispatch.py:271] Finished tracing + transforming log_softmax for pjit in 0.0042307376861572266 sec\n",
      "W0926 00:50:20.621044 140035128160896 dispatch.py:271] Finished tracing + transforming <lambda> for pjit in 0.0002856254577636719 sec\n",
      "W0926 00:50:20.621469 140035128160896 dispatch.py:271] Finished tracing + transforming _one_hot for pjit in 0.0011107921600341797 sec\n",
      "W0926 00:50:20.622070 140035128160896 dispatch.py:271] Finished tracing + transforming fn for pjit in 0.0002942085266113281 sec\n",
      "W0926 00:50:20.622857 140035128160896 dispatch.py:271] Finished tracing + transforming _reduce_sum for pjit in 0.00038123130798339844 sec\n",
      "W0926 00:50:20.623449 140035128160896 dispatch.py:271] Finished tracing + transforming <lambda> for pjit in 0.0002200603485107422 sec\n",
      "W0926 00:50:20.625036 140035128160896 dispatch.py:271] Finished tracing + transforming _reduce_sum for pjit in 0.0009942054748535156 sec\n",
      "W0926 00:50:20.625550 140035128160896 dispatch.py:271] Finished tracing + transforming _mean for pjit in 0.001741170883178711 sec\n",
      "W0926 00:50:20.626273 140035128160896 dispatch.py:271] Finished tracing + transforming _argmax for pjit in 0.00023674964904785156 sec\n",
      "W0926 00:50:20.626920 140035128160896 dispatch.py:271] Finished tracing + transforming <lambda> for pjit in 0.00028896331787109375 sec\n",
      "W0926 00:50:20.628709 140035128160896 dispatch.py:271] Finished tracing + transforming init_fn for pjit in 0.09804153442382812 sec\n",
      "I0926 00:50:20.629577 140035128160896 trainer_lib.py:399] initial_vars: {'params': {'network': {'conv1': {'b': (32,), 'w': (3, 3, 1, 32)}, 'conv2': {'b': (64,), 'w': (3, 3, 32, 64)}, 'dense_0': {'bias': {'b': (256,)}, 'linear': {'w': (3136, 256)}}, 'dense_1': {'w': (256, 10)}, 'dense_2': {'b': (10,)}}}}\n",
      "W0926 00:50:20.632352 140035128160896 dispatch.py:271] Finished tracing + transforming init_model_from_seed for pjit in 0.10289525985717773 sec\n",
      "W0926 00:50:20.634742 140035128160896 pxla.py:1764] Compiling init_model_from_seed for with global shapes and types [ShapedArray(uint32[4])]. Argument mapping: (GSPMDSharding({replicated}),).\n",
      "W0926 00:50:20.637970 140035128160896 dispatch.py:271] Finished tracing + transforming ravel for pjit in 0.00015211105346679688 sec\n",
      "W0926 00:50:20.638577 140035128160896 dispatch.py:271] Finished tracing + transforming threefry_2x32 for pjit in 0.0011477470397949219 sec\n",
      "W0926 00:50:20.639312 140035128160896 dispatch.py:271] Finished tracing + transforming _threefry_split_original for pjit in 0.0021169185638427734 sec\n",
      "W0926 00:50:20.645914 140035128160896 dispatch.py:271] Finished tracing + transforming <lambda> for pjit in 0.00023698806762695312 sec\n",
      "W0926 00:50:20.646440 140035128160896 dispatch.py:271] Finished tracing + transforming _threefry_seed for pjit in 0.0011243820190429688 sec\n",
      "W0926 00:50:20.647701 140035128160896 dispatch.py:271] Finished tracing + transforming ravel for pjit in 0.00014281272888183594 sec\n",
      "W0926 00:50:20.648280 140035128160896 dispatch.py:271] Finished tracing + transforming threefry_2x32 for pjit in 0.0012598037719726562 sec\n",
      "W0926 00:50:20.649010 140035128160896 dispatch.py:271] Finished tracing + transforming _threefry_fold_in for pjit in 0.0038480758666992188 sec\n",
      "W0926 00:50:20.680892 140035128160896 dispatch.py:271] Finished jaxpr to MLIR module conversion pjit(init_model_from_seed) in 0.04599595069885254 sec\n",
      "W0926 00:50:21.026238 140035128160896 dispatch.py:271] Finished XLA compilation of pjit(init_model_from_seed) in 0.3450157642364502 sec\n",
      "I0926 00:50:21.031028 140035128160896 partitioning.py:849] partitioned train state shapes (global shape): TrainState(step=(), mdl_vars={'params': {'network': {'conv1': {'b': (32,), 'w': (3, 3, 1, 32)}, 'conv2': {'b': (64,), 'w': (3, 3, 32, 64)}, 'dense_0': {'bias': {'b': (256,)}, 'linear': {'w': (3136, 256)}}, 'dense_1': {'w': (256, 10)}, 'dense_2': {'b': (10,)}}}}, opt_states=[({'count': ()}, {'count': ()}, (TraceState(trace={'params': {'network': {'conv1': {'b': (32,), 'w': (3, 3, 1, 32)}, 'conv2': {'b': (64,), 'w': (3, 3, 32, 64)}, 'dense_0': {'bias': {'b': (256,)}, 'linear': {'w': (3136, 256)}}, 'dense_1': {'w': (256, 10)}, 'dense_2': {'b': (10,)}}}}), ScaleByScheduleState(count=())), {'count': ()})], extra_state=())\n",
      "I0926 00:50:21.031174 140035128160896 partitioning.py:857] root prng key: [2113592192 1902136347 2113592192 1902136347]\n",
      "I0926 00:50:21.031917 140035128160896 executors.py:233] [PAX STATUS]: Checkpoint load / variable init took 0 seconds\n",
      "W0926 00:50:21.034181 140035128160896 dispatch.py:271] Finished tracing + transforming ravel for pjit in 0.00020599365234375 sec\n",
      "W0926 00:50:21.034968 140035128160896 dispatch.py:271] Finished tracing + transforming threefry_2x32 for pjit in 0.0014350414276123047 sec\n",
      "W0926 00:50:21.035906 140035128160896 dispatch.py:271] Finished tracing + transforming _threefry_split_original for pjit in 0.0026776790618896484 sec\n",
      "W0926 00:50:21.037981 140035128160896 pxla.py:1764] Compiling _threefry_split_original for with global shapes and types [ShapedArray(uint32[2,2])]. Argument mapping: (GSPMDSharding({replicated}),).\n",
      "W0926 00:50:21.040746 140035128160896 dispatch.py:271] Finished jaxpr to MLIR module conversion jit(_threefry_split_original) in 0.0026197433471679688 sec\n",
      "W0926 00:50:21.113022 140035128160896 dispatch.py:271] Finished XLA compilation of jit(_threefry_split_original) in 0.07202816009521484 sec\n",
      "W0926 00:50:21.114359 140035128160896 dispatch.py:271] Finished tracing + transforming jit(transpose) in 0.0002186298370361328 sec\n",
      "W0926 00:50:21.114622 140035128160896 pxla.py:1764] Compiling transpose for with global shapes and types [ShapedArray(uint32[2,3,2])]. Argument mapping: (GSPMDSharding({replicated}),).\n",
      "W0926 00:50:21.115918 140035128160896 dispatch.py:271] Finished jaxpr to MLIR module conversion jit(transpose) in 0.0011675357818603516 sec\n",
      "W0926 00:50:21.174007 140035128160896 dispatch.py:271] Finished XLA compilation of jit(transpose) in 0.057822465896606445 sec\n",
      "W0926 00:50:21.175831 140035128160896 dispatch.py:271] Finished tracing + transforming jit(reshape) in 0.00021767616271972656 sec\n",
      "W0926 00:50:21.176083 140035128160896 pxla.py:1764] Compiling reshape for with global shapes and types [ShapedArray(uint32[3,2,2])]. Argument mapping: (GSPMDSharding({replicated}),).\n",
      "W0926 00:50:21.177230 140035128160896 dispatch.py:271] Finished jaxpr to MLIR module conversion jit(reshape) in 0.0010280609130859375 sec\n",
      "W0926 00:50:21.197661 140035128160896 dispatch.py:271] Finished XLA compilation of jit(reshape) in 0.020180463790893555 sec\n",
      "W0926 00:50:21.199207 140035128160896 dispatch.py:271] Finished tracing + transforming _unstack for pjit in 0.0005853176116943359 sec\n",
      "W0926 00:50:21.199742 140035128160896 pxla.py:1764] Compiling _unstack for with global shapes and types [ShapedArray(uint32[3,4])]. Argument mapping: (GSPMDSharding({replicated}),).\n",
      "W0926 00:50:21.201554 140035128160896 dispatch.py:271] Finished jaxpr to MLIR module conversion jit(_unstack) in 0.0016906261444091797 sec\n",
      "W0926 00:50:21.250947 140035128160896 dispatch.py:271] Finished XLA compilation of jit(_unstack) in 0.049132585525512695 sec\n",
      "I0926 00:50:21.252069 140035128160896 executors.py:246] train prng seed: [3002578388 2149995687 3002578388 2149995687]\n",
      "I0926 00:50:21.252384 140035128160896 executors.py:247] eval prng seed: [2702578219 1490354220 2702578219 1490354220]\n",
      "I0926 00:50:21.252587 140035128160896 executors.py:248] decode prng seed: [ 117142398 1914648840  117142398 1914648840]\n",
      "W0926 00:50:21.253660 140035128160896 dispatch.py:271] Finished tracing + transforming _identity for pjit in 0.00037479400634765625 sec\n",
      "W0926 00:50:21.254134 140035128160896 pxla.py:1764] Compiling _identity for with global shapes and types [ShapedArray(uint32[4])]. Argument mapping: (GSPMDSharding({replicated}),).\n",
      "W0926 00:50:21.255140 140035128160896 dispatch.py:271] Finished jaxpr to MLIR module conversion pjit(_identity) in 0.0008852481842041016 sec\n",
      "W0926 00:50:21.266665 140035128160896 dispatch.py:271] Finished XLA compilation of pjit(_identity) in 0.011282682418823242 sec\n",
      "W0926 00:50:21.268036 140035128160896 dispatch.py:271] Finished tracing + transforming _identity for pjit in 0.00017070770263671875 sec\n",
      "W0926 00:50:21.268507 140035128160896 pxla.py:1764] Compiling _identity for with global shapes and types [ShapedArray(uint32[4])]. Argument mapping: (GSPMDSharding({replicated}),).\n",
      "W0926 00:50:21.269478 140035128160896 dispatch.py:271] Finished jaxpr to MLIR module conversion pjit(_identity) in 0.0008537769317626953 sec\n",
      "W0926 00:50:21.280770 140035128160896 dispatch.py:271] Finished XLA compilation of pjit(_identity) in 0.011050224304199219 sec\n",
      "W0926 00:50:21.282355 140035128160896 dispatch.py:271] Finished tracing + transforming _identity for pjit in 0.0006964206695556641 sec\n",
      "W0926 00:50:21.282959 140035128160896 pxla.py:1764] Compiling _identity for with global shapes and types [ShapedArray(uint32[4])]. Argument mapping: (GSPMDSharding({replicated}),).\n",
      "W0926 00:50:21.283987 140035128160896 dispatch.py:271] Finished jaxpr to MLIR module conversion pjit(_identity) in 0.0008978843688964844 sec\n",
      "W0926 00:50:21.295829 140035128160896 dispatch.py:271] Finished XLA compilation of pjit(_identity) in 0.01158761978149414 sec\n",
      "I0926 00:50:21.296851 140035128160896 executors.py:267] Starting executor.\n",
      "I0926 00:50:21.297135 140035128160896 executors.py:331] Model initial global_step=0\n",
      "I0926 00:50:21.297190 140035128160896 executors.py:338] [PAX STATUS]: Starting training loop.\n",
      "I0926 00:50:21.297220 140035128160896 programs.py:246] [PAX STATUS]: Setting up BaseTrainProgram.\n",
      "I0926 00:50:21.297355 140035128160896 summary_utils.py:281] Opening SummaryWriter `/tmp/dali_pax_logs/summaries/train`...\n",
      "I0926 00:50:21.306444 140035128160896 py_utils.py:339] Starting sync_global_devices Start training loop from step: 0 across 1 devices globally\n",
      "W0926 00:50:21.308297 140035128160896 dispatch.py:271] Finished tracing + transforming _reduce_sum for pjit in 0.0005743503570556641 sec\n",
      "W0926 00:50:21.308648 140035128160896 dispatch.py:271] Finished tracing + transforming _psum for pjit in 0.0012574195861816406 sec\n",
      "W0926 00:50:21.309091 140035128160896 pxla.py:1764] Compiling _psum for with global shapes and types [ShapedArray(uint32[1])]. Argument mapping: (GSPMDSharding({replicated}),).\n",
      "W0926 00:50:21.310475 140035128160896 dispatch.py:271] Finished jaxpr to MLIR module conversion jit(_psum) in 0.0012738704681396484 sec\n",
      "W0926 00:50:21.322649 140035128160896 dispatch.py:271] Finished XLA compilation of jit(_psum) in 0.011924982070922852 sec\n",
      "I0926 00:50:21.323509 140035128160896 py_utils.py:342] Finished sync_global_devices Start training loop from step: 0 across 1 devices globally\n",
      "I0926 00:50:21.487885 140035128160896 executors.py:381] [PAX STATUS]: Beginning step `0`.\n",
      "W0926 00:50:21.490651 140035128160896 dispatch.py:271] Finished tracing + transforming _reduce_sum for pjit in 0.0005023479461669922 sec\n",
      "W0926 00:50:21.491294 140035128160896 dispatch.py:271] Finished tracing + transforming _psum for pjit in 0.0014290809631347656 sec\n",
      "W0926 00:50:21.491827 140035128160896 pxla.py:1764] Compiling _psum for with global shapes and types [ShapedArray(int32[1]), ShapedArray(int32[1])]. Argument mapping: (GSPMDSharding({replicated}), GSPMDSharding({replicated})).\n",
      "W0926 00:50:21.493570 140035128160896 dispatch.py:271] Finished jaxpr to MLIR module conversion jit(_psum) in 0.0016236305236816406 sec\n",
      "W0926 00:50:21.547752 140035128160896 dispatch.py:271] Finished XLA compilation of jit(_psum) in 0.053933143615722656 sec\n",
      "I0926 00:50:21.551666 140035128160896 checkpointer.py:67] Saving item to /tmp/dali_pax_logs/checkpoints/checkpoint_00000000.orbax-checkpoint-tmp-1695682221488857/state.\n",
      "I0926 00:50:21.613192 140035128160896 utils.py:518] Renaming /tmp/dali_pax_logs/checkpoints/checkpoint_00000000.orbax-checkpoint-tmp-1695682221488857/state.orbax-checkpoint-tmp-1695682221551790 to /tmp/dali_pax_logs/checkpoints/checkpoint_00000000.orbax-checkpoint-tmp-1695682221488857/state\n",
      "I0926 00:50:21.613416 140035128160896 utils.py:562] Finished saving checkpoint to `/tmp/dali_pax_logs/checkpoints/checkpoint_00000000.orbax-checkpoint-tmp-1695682221488857/state`.\n",
      "I0926 00:50:21.614742 140035128160896 checkpointer.py:67] Saving item to /tmp/dali_pax_logs/checkpoints/checkpoint_00000000.orbax-checkpoint-tmp-1695682221488857/metadata.\n",
      "I0926 00:50:21.620648 140035128160896 utils.py:518] Renaming /tmp/dali_pax_logs/checkpoints/checkpoint_00000000.orbax-checkpoint-tmp-1695682221488857/metadata.orbax-checkpoint-tmp-1695682221614859 to /tmp/dali_pax_logs/checkpoints/checkpoint_00000000.orbax-checkpoint-tmp-1695682221488857/metadata\n",
      "I0926 00:50:21.620765 140035128160896 utils.py:562] Finished saving checkpoint to `/tmp/dali_pax_logs/checkpoints/checkpoint_00000000.orbax-checkpoint-tmp-1695682221488857/metadata`.\n",
      "I0926 00:50:21.621875 140035128160896 utils.py:518] Renaming /tmp/dali_pax_logs/checkpoints/checkpoint_00000000.orbax-checkpoint-tmp-1695682221488857 to /tmp/dali_pax_logs/checkpoints/checkpoint_00000000\n",
      "I0926 00:50:21.622976 140035128160896 py_utils.py:1015] [PAX STATUS]: Starting timer for <run> @ <.../paxml/executors.py:406>\n",
      "I0926 00:50:21.623042 140035128160896 programs.py:304] [PAX STATUS]:  Retrieving inputs.\n",
      "I0926 00:50:21.623081 140035128160896 py_utils.py:1015] [PAX STATUS]: Starting timer for <get_next_padded> @ <.../paxml/programs.py:305>\n",
      "I0926 00:50:21.623119 140035128160896 py_utils.py:1025] [PAX STATUS]: Elapsed time for <get_next_padded>: 0.00 seconds  (@ <.../paxml/programs.py:305>)\n",
      "I0926 00:50:21.623176 140035128160896 partitioning.py:806] Checking input spec [pjit partitioner]\n",
      "I0926 00:50:21.623277 140035128160896 py_utils.py:1015] [PAX STATUS]: Starting timer for <partition> @ <.../paxml/programs.py:584>\n",
      "I0926 00:50:21.623317 140035128160896 trainer_lib.py:1606] get_input_partition_specs from mesh_axis_names=('replica', 'data', 'mdl') and inputs_shape_dtype={'inputs': ShapeDtypeStruct(shape=(1024, 28, 28, 1), dtype=float32), 'labels': ShapeDtypeStruct(shape=(1024,), dtype=int32)}\n",
      "I0926 00:50:21.623408 140035128160896 partitioning.py:931] step_fn inputs_partition_spec={'inputs': PartitionSpec(('replica', 'data', 'mdl'), None, None, None), 'labels': PartitionSpec(('replica', 'data', 'mdl'),)}\n",
      "I0926 00:50:21.623552 140035128160896 partitioning.py:1027] step_fn fn_in_partition_specs=(TrainState(step=PartitionSpec(), mdl_vars={'params': {'network': {'conv1': {'b': PartitionSpec(None,), 'w': PartitionSpec(None, None, None, None)}, 'conv2': {'b': PartitionSpec(None,), 'w': PartitionSpec(None, None, None, None)}, 'dense_0': {'bias': {'b': PartitionSpec(None,)}, 'linear': {'w': PartitionSpec(None, None)}}, 'dense_1': {'w': PartitionSpec(None, None)}, 'dense_2': {'b': PartitionSpec(None,)}}}}, opt_states=[({'count': PartitionSpec()}, {'count': PartitionSpec()}, (TraceState(trace={'params': {'network': {'conv1': {'b': PartitionSpec(None,), 'w': PartitionSpec(None, None, None, None)}, 'conv2': {'b': PartitionSpec(None,), 'w': PartitionSpec(None, None, None, None)}, 'dense_0': {'bias': {'b': PartitionSpec(None,)}, 'linear': {'w': PartitionSpec(None, None)}}, 'dense_1': {'w': PartitionSpec(None, None)}, 'dense_2': {'b': PartitionSpec(None,)}}}}), ScaleByScheduleState(count=PartitionSpec())), {'count': PartitionSpec()})], extra_state=()), PartitionSpec(), {'inputs': PartitionSpec(('replica', 'data', 'mdl'), None, None, None), 'labels': PartitionSpec(('replica', 'data', 'mdl'),)})\n",
      "I0926 00:50:21.623637 140035128160896 partitioning.py:1028] step_fn fn_out_partition_specs=(TrainState(step=PartitionSpec(), mdl_vars={'params': {'network': {'conv1': {'b': PartitionSpec(None,), 'w': PartitionSpec(None, None, None, None)}, 'conv2': {'b': PartitionSpec(None,), 'w': PartitionSpec(None, None, None, None)}, 'dense_0': {'bias': {'b': PartitionSpec(None,)}, 'linear': {'w': PartitionSpec(None, None)}}, 'dense_1': {'w': PartitionSpec(None, None)}, 'dense_2': {'b': PartitionSpec(None,)}}}}, opt_states=[({'count': PartitionSpec()}, {'count': PartitionSpec()}, (TraceState(trace={'params': {'network': {'conv1': {'b': PartitionSpec(None,), 'w': PartitionSpec(None, None, None, None)}, 'conv2': {'b': PartitionSpec(None,), 'w': PartitionSpec(None, None, None, None)}, 'dense_0': {'bias': {'b': PartitionSpec(None,)}, 'linear': {'w': PartitionSpec(None, None)}}, 'dense_1': {'w': PartitionSpec(None, None)}, 'dense_2': {'b': PartitionSpec(None,)}}}}), ScaleByScheduleState(count=PartitionSpec())), {'count': PartitionSpec()})], extra_state=()), PartitionSpec())\n",
      "I0926 00:50:21.623977 140035128160896 py_utils.py:1025] [PAX STATUS]: Elapsed time for <partition>: 0.00 seconds  (@ <.../paxml/programs.py:584>)\n",
      "W0926 00:50:21.624450 140035128160896 dispatch.py:271] Finished tracing + transforming swapaxes for pjit in 0.00021696090698242188 sec\n",
      "W0926 00:50:21.625631 140035128160896 dispatch.py:271] Finished tracing + transforming swapaxes for pjit in 0.0001780986785888672 sec\n",
      "I0926 00:50:21.626832 140035128160896 programs.py:316] [PAX STATUS]:  Retrieved inputs.\n",
      "I0926 00:50:21.626917 140035128160896 programs.py:327] [PAX STATUS]:  Performing train_step().\n",
      "I0926 00:50:21.628363 140035128160896 trainer_lib.py:1563] No resharding of input as mapping_dict is None.\n",
      "I0926 00:50:21.628414 140035128160896 trainer_lib.py:1563] No resharding of input as mapping_dict is None.\n",
      "I0926 00:50:21.629156 140035128160896 trainer_lib.py:945] Bprop included var: params/network/conv1/b\n",
      "I0926 00:50:21.629208 140035128160896 trainer_lib.py:945] Bprop included var: params/network/conv1/w\n",
      "I0926 00:50:21.629237 140035128160896 trainer_lib.py:945] Bprop included var: params/network/conv2/b\n",
      "I0926 00:50:21.629262 140035128160896 trainer_lib.py:945] Bprop included var: params/network/conv2/w\n",
      "I0926 00:50:21.629285 140035128160896 trainer_lib.py:945] Bprop included var: params/network/dense_0/bias/b\n",
      "I0926 00:50:21.629308 140035128160896 trainer_lib.py:945] Bprop included var: params/network/dense_0/linear/w\n",
      "I0926 00:50:21.629330 140035128160896 trainer_lib.py:945] Bprop included var: params/network/dense_1/w\n",
      "I0926 00:50:21.629351 140035128160896 trainer_lib.py:945] Bprop included var: params/network/dense_2/b\n",
      "I0926 00:50:21.655431 140035128160896 base_layer.py:530] Sets tensor_split_dims_mapping of a param of shape (3, 3, 1, 32) to (-1, -1, -1, -1)\n",
      "I0926 00:50:21.655815 140035128160896 base_layer.py:635] Creating var /network/conv1/w with shape=(3, 3, 1, 32), dtype=<class 'jax.numpy.float32'>, init method=xavier and scale=1.000001\n",
      "I0926 00:50:21.657204 140035128160896 base_layer.py:635] Creating var /network/conv1/b with shape=[32], dtype=<class 'jax.numpy.float32'>, init method=constant and scale=0.0\n",
      "W0926 00:50:21.664985 140035128160896 dispatch.py:271] Finished tracing + transforming fn for pjit in 0.000392913818359375 sec\n",
      "I0926 00:50:21.668843 140035128160896 base_layer.py:530] Sets tensor_split_dims_mapping of a param of shape (3, 3, 32, 64) to (-1, -1, -1, -1)\n",
      "I0926 00:50:21.669092 140035128160896 base_layer.py:635] Creating var /network/conv2/w with shape=(3, 3, 32, 64), dtype=<class 'jax.numpy.float32'>, init method=xavier and scale=1.000001\n",
      "I0926 00:50:21.670262 140035128160896 base_layer.py:635] Creating var /network/conv2/b with shape=[64], dtype=<class 'jax.numpy.float32'>, init method=constant and scale=0.0\n",
      "W0926 00:50:21.677632 140035128160896 dispatch.py:271] Finished tracing + transforming fn for pjit in 0.0003466606140136719 sec\n",
      "I0926 00:50:21.685791 140035128160896 base_layer.py:530] Sets tensor_split_dims_mapping of a param of shape [3136, 256] to (-1, -1)\n",
      "I0926 00:50:21.686072 140035128160896 base_layer.py:635] Creating var /network/dense_0/linear/w with shape=[3136, 256], dtype=<class 'jax.numpy.float32'>, init method=xavier and scale=1.000001\n",
      "I0926 00:50:21.691334 140035128160896 base_layer.py:530] Sets tensor_split_dims_mapping of a param of shape [256] to (-1,)\n",
      "I0926 00:50:21.691585 140035128160896 base_layer.py:635] Creating var /network/dense_0/bias/b with shape=[256], dtype=<class 'jax.numpy.float32'>, init method=constant and scale=0.0\n",
      "W0926 00:50:21.695287 140035128160896 dispatch.py:271] Finished tracing + transforming fn for pjit in 0.0003478527069091797 sec\n",
      "I0926 00:50:21.696123 140035128160896 base_layer.py:530] Sets tensor_split_dims_mapping of a param of shape [256, 10] to (-1, -1)\n",
      "I0926 00:50:21.696372 140035128160896 base_layer.py:635] Creating var /network/dense_1/w with shape=[256, 10], dtype=<class 'jax.numpy.float32'>, init method=xavier and scale=1.000001\n",
      "I0926 00:50:21.701516 140035128160896 base_layer.py:530] Sets tensor_split_dims_mapping of a param of shape [10] to (-1,)\n",
      "I0926 00:50:21.701765 140035128160896 base_layer.py:635] Creating var /network/dense_2/b with shape=[10], dtype=<class 'jax.numpy.float32'>, init method=constant and scale=0.0\n",
      "W0926 00:50:21.727529 140035128160896 dispatch.py:271] Finished tracing + transforming fn for pjit in 0.00035762786865234375 sec\n",
      "W0926 00:50:21.728408 140035128160896 dispatch.py:271] Finished tracing + transforming fn for pjit in 0.0002849102020263672 sec\n",
      "W0926 00:50:21.729189 140035128160896 dispatch.py:271] Finished tracing + transforming _reduce_sum for pjit in 0.00037932395935058594 sec\n",
      "W0926 00:50:21.729878 140035128160896 dispatch.py:271] Finished tracing + transforming fn for pjit in 0.0003104209899902344 sec\n",
      "W0926 00:50:21.730656 140035128160896 dispatch.py:271] Finished tracing + transforming _reduce_sum for pjit in 0.0003707408905029297 sec\n",
      "W0926 00:50:21.731389 140035128160896 dispatch.py:271] Finished tracing + transforming fn for pjit in 0.00034999847412109375 sec\n",
      "W0926 00:50:21.732160 140035128160896 dispatch.py:271] Finished tracing + transforming _reduce_sum for pjit in 0.0003769397735595703 sec\n",
      "W0926 00:50:21.732938 140035128160896 dispatch.py:271] Finished tracing + transforming fn for pjit in 0.0003924369812011719 sec\n",
      "W0926 00:50:21.733691 140035128160896 dispatch.py:271] Finished tracing + transforming _reduce_sum for pjit in 0.0003743171691894531 sec\n",
      "W0926 00:50:21.734392 140035128160896 dispatch.py:271] Finished tracing + transforming fn for pjit in 0.00031185150146484375 sec\n",
      "W0926 00:50:21.735156 140035128160896 dispatch.py:271] Finished tracing + transforming _reduce_sum for pjit in 0.0003714561462402344 sec\n",
      "W0926 00:50:21.735831 140035128160896 dispatch.py:271] Finished tracing + transforming fn for pjit in 0.0002865791320800781 sec\n",
      "W0926 00:50:21.736548 140035128160896 dispatch.py:271] Finished tracing + transforming _reduce_sum for pjit in 0.0003483295440673828 sec\n",
      "W0926 00:50:21.737214 140035128160896 dispatch.py:271] Finished tracing + transforming fn for pjit in 0.0002989768981933594 sec\n",
      "W0926 00:50:21.737975 140035128160896 dispatch.py:271] Finished tracing + transforming _reduce_sum for pjit in 0.0003783702850341797 sec\n",
      "W0926 00:50:21.738714 140035128160896 dispatch.py:271] Finished tracing + transforming fn for pjit in 0.00033164024353027344 sec\n",
      "W0926 00:50:21.739989 140035128160896 dispatch.py:271] Finished tracing + transforming _reduce_sum for pjit in 0.0008533000946044922 sec\n",
      "W0926 00:50:21.741454 140035128160896 dispatch.py:271] Finished tracing + transforming _reduce_sum for pjit in 0.00037217140197753906 sec\n",
      "W0926 00:50:21.742075 140035128160896 dispatch.py:271] Finished tracing + transforming <lambda> for pjit in 0.0002529621124267578 sec\n",
      "W0926 00:50:21.742730 140035128160896 dispatch.py:271] Finished tracing + transforming isfinite for pjit in 0.00021529197692871094 sec\n",
      "W0926 00:50:21.743398 140035128160896 dispatch.py:271] Finished tracing + transforming _reduce_all for pjit in 0.00036597251892089844 sec\n",
      "W0926 00:50:21.749502 140035128160896 dispatch.py:271] Finished tracing + transforming fn for pjit in 0.0003330707550048828 sec\n",
      "W0926 00:50:21.750686 140035128160896 dispatch.py:271] Finished tracing + transforming fn for pjit in 0.00032258033752441406 sec\n",
      "W0926 00:50:21.751393 140035128160896 dispatch.py:271] Finished tracing + transforming fn for pjit in 0.0002770423889160156 sec\n",
      "W0926 00:50:21.752411 140035128160896 dispatch.py:271] Finished tracing + transforming fn for pjit in 0.0003597736358642578 sec\n",
      "W0926 00:50:21.753163 140035128160896 dispatch.py:271] Finished tracing + transforming fn for pjit in 0.0003330707550048828 sec\n",
      "W0926 00:50:21.753844 140035128160896 dispatch.py:271] Finished tracing + transforming fn for pjit in 0.0002779960632324219 sec\n",
      "W0926 00:50:21.754786 140035128160896 dispatch.py:271] Finished tracing + transforming fn for pjit in 0.000278472900390625 sec\n",
      "W0926 00:50:21.755527 140035128160896 dispatch.py:271] Finished tracing + transforming fn for pjit in 0.0003349781036376953 sec\n",
      "W0926 00:50:21.756203 140035128160896 dispatch.py:271] Finished tracing + transforming fn for pjit in 0.0002777576446533203 sec\n",
      "W0926 00:50:21.757102 140035128160896 dispatch.py:271] Finished tracing + transforming fn for pjit in 0.00026035308837890625 sec\n",
      "W0926 00:50:21.757972 140035128160896 dispatch.py:271] Finished tracing + transforming fn for pjit in 0.00026226043701171875 sec\n",
      "W0926 00:50:21.758769 140035128160896 dispatch.py:271] Finished tracing + transforming fn for pjit in 0.0003991127014160156 sec\n",
      "W0926 00:50:21.759508 140035128160896 dispatch.py:271] Finished tracing + transforming fn for pjit in 0.0003037452697753906 sec\n",
      "W0926 00:50:21.760610 140035128160896 dispatch.py:271] Finished tracing + transforming fn for pjit in 0.0003294944763183594 sec\n",
      "W0926 00:50:21.761388 140035128160896 dispatch.py:271] Finished tracing + transforming fn for pjit in 0.0002651214599609375 sec\n",
      "W0926 00:50:21.762028 140035128160896 dispatch.py:271] Finished tracing + transforming fn for pjit in 0.0002570152282714844 sec\n",
      "W0926 00:50:21.762757 140035128160896 dispatch.py:271] Finished tracing + transforming fn for pjit in 0.0002694129943847656 sec\n",
      "W0926 00:50:21.763451 140035128160896 dispatch.py:271] Finished tracing + transforming fn for pjit in 0.0002791881561279297 sec\n",
      "W0926 00:50:21.764120 140035128160896 dispatch.py:271] Finished tracing + transforming fn for pjit in 0.0002562999725341797 sec\n",
      "W0926 00:50:21.764824 140035128160896 dispatch.py:271] Finished tracing + transforming fn for pjit in 0.00031495094299316406 sec\n",
      "W0926 00:50:21.765491 140035128160896 dispatch.py:271] Finished tracing + transforming fn for pjit in 0.000278472900390625 sec\n",
      "W0926 00:50:21.766209 140035128160896 dispatch.py:271] Finished tracing + transforming fn for pjit in 0.0002982616424560547 sec\n",
      "W0926 00:50:21.767116 140035128160896 dispatch.py:271] Finished tracing + transforming fn for pjit in 0.0003383159637451172 sec\n",
      "W0926 00:50:21.767833 140035128160896 dispatch.py:271] Finished tracing + transforming fn for pjit in 0.00028061866760253906 sec\n",
      "W0926 00:50:21.768642 140035128160896 dispatch.py:271] Finished tracing + transforming _broadcast_arrays for pjit in 0.0001499652862548828 sec\n",
      "W0926 00:50:21.768933 140035128160896 dispatch.py:271] Finished tracing + transforming _where for pjit in 0.0007061958312988281 sec\n",
      "W0926 00:50:21.770839 140035128160896 dispatch.py:271] Finished tracing + transforming _broadcast_arrays for pjit in 0.00040411949157714844 sec\n",
      "W0926 00:50:21.771220 140035128160896 dispatch.py:271] Finished tracing + transforming _where for pjit in 0.0010273456573486328 sec\n",
      "W0926 00:50:21.772186 140035128160896 dispatch.py:271] Finished tracing + transforming _broadcast_arrays for pjit in 0.00028896331787109375 sec\n",
      "W0926 00:50:21.772580 140035128160896 dispatch.py:271] Finished tracing + transforming _where for pjit in 0.0009088516235351562 sec\n",
      "W0926 00:50:21.773592 140035128160896 dispatch.py:271] Finished tracing + transforming _broadcast_arrays for pjit in 0.00029397010803222656 sec\n",
      "W0926 00:50:21.773967 140035128160896 dispatch.py:271] Finished tracing + transforming _where for pjit in 0.0009152889251708984 sec\n",
      "W0926 00:50:21.774985 140035128160896 dispatch.py:271] Finished tracing + transforming _broadcast_arrays for pjit in 0.00030159950256347656 sec\n",
      "W0926 00:50:21.775368 140035128160896 dispatch.py:271] Finished tracing + transforming _where for pjit in 0.000926971435546875 sec\n",
      "W0926 00:50:21.776379 140035128160896 dispatch.py:271] Finished tracing + transforming _broadcast_arrays for pjit in 0.00027632713317871094 sec\n",
      "W0926 00:50:21.776766 140035128160896 dispatch.py:271] Finished tracing + transforming _where for pjit in 0.0009560585021972656 sec\n",
      "W0926 00:50:21.777777 140035128160896 dispatch.py:271] Finished tracing + transforming _broadcast_arrays for pjit in 0.000301361083984375 sec\n",
      "W0926 00:50:21.778168 140035128160896 dispatch.py:271] Finished tracing + transforming _where for pjit in 0.0009334087371826172 sec\n",
      "W0926 00:50:21.779175 140035128160896 dispatch.py:271] Finished tracing + transforming _broadcast_arrays for pjit in 0.000301361083984375 sec\n",
      "W0926 00:50:21.779584 140035128160896 dispatch.py:271] Finished tracing + transforming _where for pjit in 0.0009441375732421875 sec\n",
      "W0926 00:50:21.780759 140035128160896 dispatch.py:271] Finished tracing + transforming _broadcast_arrays for pjit in 0.0004055500030517578 sec\n",
      "W0926 00:50:21.781149 140035128160896 dispatch.py:271] Finished tracing + transforming _where for pjit in 0.0010571479797363281 sec\n",
      "W0926 00:50:21.781894 140035128160896 dispatch.py:271] Finished tracing + transforming _where for pjit in 0.00039839744567871094 sec\n",
      "W0926 00:50:21.796252 140035128160896 dispatch.py:271] Finished tracing + transforming fn for pjit in 0.00035262107849121094 sec\n",
      "W0926 00:50:21.796982 140035128160896 dispatch.py:271] Finished tracing + transforming absolute for pjit in 0.0002086162567138672 sec\n",
      "W0926 00:50:21.797761 140035128160896 dispatch.py:271] Finished tracing + transforming _reduce_max for pjit in 0.00041174888610839844 sec\n",
      "W0926 00:50:21.798506 140035128160896 dispatch.py:271] Finished tracing + transforming <lambda> for pjit in 0.0003414154052734375 sec\n",
      "W0926 00:50:21.799228 140035128160896 dispatch.py:271] Finished tracing + transforming true_divide for pjit in 0.0002911090850830078 sec\n",
      "W0926 00:50:21.800245 140035128160896 dispatch.py:271] Finished tracing + transforming fn for pjit in 0.0002791881561279297 sec\n",
      "W0926 00:50:21.801259 140035128160896 dispatch.py:271] Finished tracing + transforming absolute for pjit in 0.00021338462829589844 sec\n",
      "W0926 00:50:21.802007 140035128160896 dispatch.py:271] Finished tracing + transforming _reduce_max for pjit in 0.000354766845703125 sec\n",
      "W0926 00:50:21.803060 140035128160896 dispatch.py:271] Finished tracing + transforming true_divide for pjit in 0.000301361083984375 sec\n",
      "W0926 00:50:21.804759 140035128160896 dispatch.py:271] Finished tracing + transforming absolute for pjit in 0.00019478797912597656 sec\n",
      "W0926 00:50:21.805457 140035128160896 dispatch.py:271] Finished tracing + transforming _reduce_max for pjit in 0.000339508056640625 sec\n",
      "W0926 00:50:21.806420 140035128160896 dispatch.py:271] Finished tracing + transforming true_divide for pjit in 0.000274658203125 sec\n",
      "W0926 00:50:21.808023 140035128160896 dispatch.py:271] Finished tracing + transforming absolute for pjit in 0.00019240379333496094 sec\n",
      "W0926 00:50:21.808795 140035128160896 dispatch.py:271] Finished tracing + transforming _reduce_max for pjit in 0.00037360191345214844 sec\n",
      "W0926 00:50:21.809806 140035128160896 dispatch.py:271] Finished tracing + transforming true_divide for pjit in 0.00033545494079589844 sec\n",
      "W0926 00:50:21.811411 140035128160896 dispatch.py:271] Finished tracing + transforming absolute for pjit in 0.00018906593322753906 sec\n",
      "W0926 00:50:21.812122 140035128160896 dispatch.py:271] Finished tracing + transforming _reduce_max for pjit in 0.000331878662109375 sec\n",
      "W0926 00:50:21.813085 140035128160896 dispatch.py:271] Finished tracing + transforming true_divide for pjit in 0.0002741813659667969 sec\n",
      "W0926 00:50:21.814635 140035128160896 dispatch.py:271] Finished tracing + transforming absolute for pjit in 0.0001888275146484375 sec\n",
      "W0926 00:50:21.816047 140035128160896 dispatch.py:271] Finished tracing + transforming _reduce_max for pjit in 0.0010073184967041016 sec\n",
      "W0926 00:50:21.816920 140035128160896 dispatch.py:271] Finished tracing + transforming true_divide for pjit in 0.00027108192443847656 sec\n",
      "W0926 00:50:21.818527 140035128160896 dispatch.py:271] Finished tracing + transforming absolute for pjit in 0.00019216537475585938 sec\n",
      "W0926 00:50:21.819224 140035128160896 dispatch.py:271] Finished tracing + transforming _reduce_max for pjit in 0.00033473968505859375 sec\n",
      "W0926 00:50:21.820147 140035128160896 dispatch.py:271] Finished tracing + transforming true_divide for pjit in 0.0002493858337402344 sec\n",
      "W0926 00:50:21.821755 140035128160896 dispatch.py:271] Finished tracing + transforming absolute for pjit in 0.00024890899658203125 sec\n",
      "W0926 00:50:21.822513 140035128160896 dispatch.py:271] Finished tracing + transforming _reduce_max for pjit in 0.0003609657287597656 sec\n",
      "W0926 00:50:21.823475 140035128160896 dispatch.py:271] Finished tracing + transforming true_divide for pjit in 0.0002694129943847656 sec\n",
      "W0926 00:50:21.825456 140035128160896 dispatch.py:271] Finished tracing + transforming true_divide for pjit in 0.0002765655517578125 sec\n",
      "W0926 00:50:21.835554 140035128160896 dispatch.py:271] Finished tracing + transforming _wrapped_step_fn for pjit in 0.2079455852508545 sec\n",
      "W0926 00:50:21.842659 140035128160896 pxla.py:1764] Compiling _wrapped_step_fn for with global shapes and types [ShapedArray(uint32[]), ShapedArray(float32[32]), ShapedArray(float32[3,3,1,32]), ShapedArray(float32[64]), ShapedArray(float32[3,3,32,64]), ShapedArray(float32[256]), ShapedArray(float32[3136,256]), ShapedArray(float32[256,10]), ShapedArray(float32[10]), ShapedArray(int32[]), ShapedArray(int32[]), ShapedArray(float32[32]), ShapedArray(float32[3,3,1,32]), ShapedArray(float32[64]), ShapedArray(float32[3,3,32,64]), ShapedArray(float32[256]), ShapedArray(float32[3136,256]), ShapedArray(float32[256,10]), ShapedArray(float32[10]), ShapedArray(int32[]), ShapedArray(int32[]), ShapedArray(float32[1024,28,28,1]), ShapedArray(int32[1024])]. Argument mapping: (GSPMDSharding({replicated}), GSPMDSharding({replicated}), GSPMDSharding({replicated}), GSPMDSharding({replicated}), GSPMDSharding({replicated}), GSPMDSharding({replicated}), GSPMDSharding({replicated}), GSPMDSharding({replicated}), GSPMDSharding({replicated}), GSPMDSharding({replicated}), GSPMDSharding({replicated}), GSPMDSharding({replicated}), GSPMDSharding({replicated}), GSPMDSharding({replicated}), GSPMDSharding({replicated}), GSPMDSharding({replicated}), GSPMDSharding({replicated}), GSPMDSharding({replicated}), GSPMDSharding({replicated}), GSPMDSharding({replicated}), GSPMDSharding({replicated}), GSPMDSharding({devices=[1,1,1,1]0}), GSPMDSharding({devices=[1]0})).\n",
      "W0926 00:50:21.931399 140035128160896 dispatch.py:271] Finished jaxpr to MLIR module conversion pjit(_wrapped_step_fn) in 0.08833861351013184 sec\n",
      "W0926 00:50:23.754925 140035128160896 dispatch.py:271] Finished XLA compilation of pjit(_wrapped_step_fn) in 1.8231542110443115 sec\n",
      "I0926 00:50:23.759412 140035128160896 programs.py:345] [PAX STATUS]: train_step() took 2.132405 seconds.\n",
      "I0926 00:50:23.759545 140035128160896 programs.py:357] [PAX STATUS]:  Writing summaries (attempt).\n",
      "I0926 00:50:23.759674 140035128160896 py_utils.py:1025] [PAX STATUS]: Elapsed time for <run>: 2.14 seconds  (@ <.../paxml/executors.py:406>)\n",
      "I0926 00:50:23.759729 140035128160896 executors.py:472] [PAX STATUS]: Step `0` completed.\n",
      "I0926 00:50:23.759762 140035128160896 executors.py:381] [PAX STATUS]: Beginning step `1`.\n",
      "I0926 00:50:23.759824 140035128160896 py_utils.py:1015] [PAX STATUS]: Starting timer for <run> @ <.../paxml/executors.py:406>\n",
      "I0926 00:50:23.759853 140035128160896 programs.py:304] [PAX STATUS]:  Retrieving inputs.\n",
      "I0926 00:50:23.763221 140035128160896 programs.py:316] [PAX STATUS]:  Retrieved inputs.\n",
      "I0926 00:50:23.763342 140035128160896 programs.py:327] [PAX STATUS]:  Performing train_step().\n",
      "I0926 00:50:23.764239 140035128160896 programs.py:345] [PAX STATUS]: train_step() took 0.000825 seconds.\n",
      "I0926 00:50:23.764325 140035128160896 programs.py:357] [PAX STATUS]:  Writing summaries (attempt).\n",
      "I0926 00:50:23.764418 140035128160896 py_utils.py:1025] [PAX STATUS]: Elapsed time for <run>: 0.00 seconds  (@ <.../paxml/executors.py:406>)\n",
      "I0926 00:50:23.764468 140035128160896 executors.py:472] [PAX STATUS]: Step `1` completed.\n",
      "I0926 00:50:23.764501 140035128160896 executors.py:381] [PAX STATUS]: Beginning step `2`.\n",
      "I0926 00:50:23.764548 140035128160896 py_utils.py:1015] [PAX STATUS]: Starting timer for <run> @ <.../paxml/executors.py:406>\n",
      "I0926 00:50:23.764576 140035128160896 programs.py:304] [PAX STATUS]:  Retrieving inputs.\n",
      "I0926 00:50:23.767197 140035128160896 programs.py:316] [PAX STATUS]:  Retrieved inputs.\n",
      "I0926 00:50:23.767323 140035128160896 programs.py:327] [PAX STATUS]:  Performing train_step().\n",
      "I0926 00:50:23.768157 140035128160896 programs.py:345] [PAX STATUS]: train_step() took 0.000765 seconds.\n",
      "I0926 00:50:23.768233 140035128160896 programs.py:357] [PAX STATUS]:  Writing summaries (attempt).\n",
      "I0926 00:50:23.768328 140035128160896 py_utils.py:1025] [PAX STATUS]: Elapsed time for <run>: 0.00 seconds  (@ <.../paxml/executors.py:406>)\n",
      "I0926 00:50:23.768382 140035128160896 executors.py:472] [PAX STATUS]: Step `2` completed.\n",
      "I0926 00:50:23.768417 140035128160896 executors.py:381] [PAX STATUS]: Beginning step `3`.\n",
      "I0926 00:50:23.768466 140035128160896 py_utils.py:1015] [PAX STATUS]: Starting timer for <run> @ <.../paxml/executors.py:406>\n",
      "I0926 00:50:23.768496 140035128160896 programs.py:304] [PAX STATUS]:  Retrieving inputs.\n",
      "I0926 00:50:23.810469 140035128160896 programs.py:316] [PAX STATUS]:  Retrieved inputs.\n",
      "I0926 00:50:23.810633 140035128160896 programs.py:327] [PAX STATUS]:  Performing train_step().\n",
      "I0926 00:50:23.811740 140035128160896 programs.py:345] [PAX STATUS]: train_step() took 0.001019 seconds.\n",
      "I0926 00:50:23.811843 140035128160896 programs.py:357] [PAX STATUS]:  Writing summaries (attempt).\n",
      "I0926 00:50:23.811954 140035128160896 py_utils.py:1025] [PAX STATUS]: Elapsed time for <run>: 0.04 seconds  (@ <.../paxml/executors.py:406>)\n",
      "I0926 00:50:23.812010 140035128160896 executors.py:472] [PAX STATUS]: Step `3` completed.\n",
      "I0926 00:50:23.812045 140035128160896 executors.py:381] [PAX STATUS]: Beginning step `4`.\n",
      "I0926 00:50:23.812096 140035128160896 py_utils.py:1015] [PAX STATUS]: Starting timer for <run> @ <.../paxml/executors.py:406>\n",
      "I0926 00:50:23.812126 140035128160896 programs.py:304] [PAX STATUS]:  Retrieving inputs.\n",
      "I0926 00:50:23.827901 140035128160896 programs.py:316] [PAX STATUS]:  Retrieved inputs.\n",
      "I0926 00:50:23.828058 140035128160896 programs.py:327] [PAX STATUS]:  Performing train_step().\n",
      "I0926 00:50:23.829480 140035128160896 programs.py:345] [PAX STATUS]: train_step() took 0.001320 seconds.\n",
      "I0926 00:50:23.829596 140035128160896 programs.py:357] [PAX STATUS]:  Writing summaries (attempt).\n",
      "I0926 00:50:23.829711 140035128160896 py_utils.py:1025] [PAX STATUS]: Elapsed time for <run>: 0.02 seconds  (@ <.../paxml/executors.py:406>)\n",
      "I0926 00:50:23.829768 140035128160896 executors.py:472] [PAX STATUS]: Step `4` completed.\n",
      "I0926 00:50:23.829829 140035128160896 py_utils.py:1015] [PAX STATUS]: Starting timer for <run> @ <.../paxml/executors.py:406>\n",
      "I0926 00:50:23.852833 140035128160896 programs.py:345] [PAX STATUS]: train_step() took 0.000953 seconds.\n",
      "I0926 00:50:23.853037 140035128160896 py_utils.py:1025] [PAX STATUS]: Elapsed time for <run>: 0.02 seconds  (@ <.../paxml/executors.py:406>)\n",
      "I0926 00:50:23.853125 140035128160896 py_utils.py:1015] [PAX STATUS]: Starting timer for <run> @ <.../paxml/executors.py:406>\n",
      "I0926 00:50:23.870164 140035128160896 programs.py:345] [PAX STATUS]: train_step() took 0.001409 seconds.\n",
      "I0926 00:50:23.870367 140035128160896 py_utils.py:1025] [PAX STATUS]: Elapsed time for <run>: 0.02 seconds  (@ <.../paxml/executors.py:406>)\n",
      "I0926 00:50:23.870461 140035128160896 py_utils.py:1015] [PAX STATUS]: Starting timer for <run> @ <.../paxml/executors.py:406>\n",
      "I0926 00:50:23.894124 140035128160896 programs.py:345] [PAX STATUS]: train_step() took 0.000993 seconds.\n",
      "I0926 00:50:23.894336 140035128160896 py_utils.py:1025] [PAX STATUS]: Elapsed time for <run>: 0.02 seconds  (@ <.../paxml/executors.py:406>)\n",
      "I0926 00:50:23.894424 140035128160896 py_utils.py:1015] [PAX STATUS]: Starting timer for <run> @ <.../paxml/executors.py:406>\n",
      "I0926 00:50:23.911487 140035128160896 programs.py:345] [PAX STATUS]: train_step() took 0.001004 seconds.\n",
      "I0926 00:50:23.911704 140035128160896 py_utils.py:1025] [PAX STATUS]: Elapsed time for <run>: 0.02 seconds  (@ <.../paxml/executors.py:406>)\n",
      "I0926 00:50:23.911790 140035128160896 py_utils.py:1015] [PAX STATUS]: Starting timer for <run> @ <.../paxml/executors.py:406>\n",
      "I0926 00:50:23.934610 140035128160896 programs.py:345] [PAX STATUS]: train_step() took 0.001013 seconds.\n",
      "I0926 00:50:23.934746 140035128160896 programs.py:468] steps/sec: 3.800553\n",
      "I0926 00:50:23.934864 140035128160896 py_utils.py:1025] [PAX STATUS]: Elapsed time for <run>: 0.02 seconds  (@ <.../paxml/executors.py:406>)\n",
      "I0926 00:50:23.934948 140035128160896 py_utils.py:1015] [PAX STATUS]: Starting timer for <run> @ <.../paxml/executors.py:406>\n",
      "I0926 00:50:23.950824 140035128160896 programs.py:345] [PAX STATUS]: train_step() took 0.001009 seconds.\n",
      "I0926 00:50:23.951026 140035128160896 py_utils.py:1025] [PAX STATUS]: Elapsed time for <run>: 0.02 seconds  (@ <.../paxml/executors.py:406>)\n",
      "I0926 00:50:23.951112 140035128160896 py_utils.py:1015] [PAX STATUS]: Starting timer for <run> @ <.../paxml/executors.py:406>\n",
      "I0926 00:50:23.975761 140035128160896 programs.py:345] [PAX STATUS]: train_step() took 0.000966 seconds.\n",
      "I0926 00:50:23.975961 140035128160896 py_utils.py:1025] [PAX STATUS]: Elapsed time for <run>: 0.02 seconds  (@ <.../paxml/executors.py:406>)\n",
      "I0926 00:50:23.976064 140035128160896 py_utils.py:1015] [PAX STATUS]: Starting timer for <run> @ <.../paxml/executors.py:406>\n",
      "I0926 00:50:23.992820 140035128160896 programs.py:345] [PAX STATUS]: train_step() took 0.001019 seconds.\n",
      "I0926 00:50:23.993023 140035128160896 py_utils.py:1025] [PAX STATUS]: Elapsed time for <run>: 0.02 seconds  (@ <.../paxml/executors.py:406>)\n",
      "I0926 00:50:23.993119 140035128160896 py_utils.py:1015] [PAX STATUS]: Starting timer for <run> @ <.../paxml/executors.py:406>\n",
      "I0926 00:50:24.018530 140035128160896 programs.py:345] [PAX STATUS]: train_step() took 0.000986 seconds.\n",
      "I0926 00:50:24.018722 140035128160896 py_utils.py:1025] [PAX STATUS]: Elapsed time for <run>: 0.03 seconds  (@ <.../paxml/executors.py:406>)\n",
      "I0926 00:50:24.018816 140035128160896 py_utils.py:1015] [PAX STATUS]: Starting timer for <run> @ <.../paxml/executors.py:406>\n",
      "I0926 00:50:24.034222 140035128160896 programs.py:345] [PAX STATUS]: train_step() took 0.001017 seconds.\n",
      "I0926 00:50:24.034429 140035128160896 py_utils.py:1025] [PAX STATUS]: Elapsed time for <run>: 0.02 seconds  (@ <.../paxml/executors.py:406>)\n",
      "I0926 00:50:24.034517 140035128160896 py_utils.py:1015] [PAX STATUS]: Starting timer for <run> @ <.../paxml/executors.py:406>\n",
      "I0926 00:50:24.059210 140035128160896 programs.py:345] [PAX STATUS]: train_step() took 0.000968 seconds.\n",
      "I0926 00:50:24.059407 140035128160896 py_utils.py:1025] [PAX STATUS]: Elapsed time for <run>: 0.02 seconds  (@ <.../paxml/executors.py:406>)\n",
      "I0926 00:50:24.059497 140035128160896 py_utils.py:1015] [PAX STATUS]: Starting timer for <run> @ <.../paxml/executors.py:406>\n",
      "I0926 00:50:24.078109 140035128160896 programs.py:345] [PAX STATUS]: train_step() took 0.000998 seconds.\n",
      "I0926 00:50:24.078318 140035128160896 py_utils.py:1025] [PAX STATUS]: Elapsed time for <run>: 0.02 seconds  (@ <.../paxml/executors.py:406>)\n",
      "I0926 00:50:24.078404 140035128160896 py_utils.py:1015] [PAX STATUS]: Starting timer for <run> @ <.../paxml/executors.py:406>\n",
      "I0926 00:50:24.101078 140035128160896 programs.py:345] [PAX STATUS]: train_step() took 0.000952 seconds.\n",
      "I0926 00:50:24.101283 140035128160896 py_utils.py:1025] [PAX STATUS]: Elapsed time for <run>: 0.02 seconds  (@ <.../paxml/executors.py:406>)\n",
      "I0926 00:50:24.101366 140035128160896 py_utils.py:1015] [PAX STATUS]: Starting timer for <run> @ <.../paxml/executors.py:406>\n",
      "I0926 00:50:24.117471 140035128160896 programs.py:345] [PAX STATUS]: train_step() took 0.000964 seconds.\n",
      "I0926 00:50:24.117664 140035128160896 py_utils.py:1025] [PAX STATUS]: Elapsed time for <run>: 0.02 seconds  (@ <.../paxml/executors.py:406>)\n",
      "I0926 00:50:24.117750 140035128160896 py_utils.py:1015] [PAX STATUS]: Starting timer for <run> @ <.../paxml/executors.py:406>\n",
      "I0926 00:50:24.141856 140035128160896 programs.py:345] [PAX STATUS]: train_step() took 0.000961 seconds.\n",
      "I0926 00:50:24.141989 140035128160896 programs.py:468] steps/sec: 48.262481\n",
      "I0926 00:50:24.142127 140035128160896 py_utils.py:1025] [PAX STATUS]: Elapsed time for <run>: 0.02 seconds  (@ <.../paxml/executors.py:406>)\n",
      "I0926 00:50:24.346549 140035128160896 programs.py:468] steps/sec: 48.899594\n",
      "I0926 00:50:24.547791 140035128160896 programs.py:468] steps/sec: 49.723411\n",
      "I0926 00:50:24.753746 140035128160896 programs.py:468] steps/sec: 48.584097\n",
      "/home/awolant/Projects/DALI/dali/operators/reader/loader/lmdb.h:76: lmdb 0 /home/awolant/Projects/DALI_extra/db/MNIST/training/ rewind to the begin from 59999\n",
      "/home/awolant/Projects/DALI/dali/operators/reader/loader/lmdb.h:76: lmdb 0 /home/awolant/Projects/DALI_extra/db/MNIST/training/ rewind to the begin from 0\n",
      "W0926 00:50:24.940204 140035128160896 base_iterator.py:437] DALI iterator does not support resetting while epoch is not finished.                              Ignoring...\n",
      "I0926 00:50:24.959594 140035128160896 programs.py:468] steps/sec: 48.614280\n",
      "I0926 00:50:25.164081 140035128160896 programs.py:468] steps/sec: 48.942789\n",
      "I0926 00:50:25.363831 140035128160896 programs.py:468] steps/sec: 50.093145\n",
      "I0926 00:50:25.564644 140035128160896 programs.py:468] steps/sec: 49.828145\n",
      "I0926 00:50:25.758505 140035128160896 programs.py:468] steps/sec: 51.616676\n",
      "I0926 00:50:25.759135 140035128160896 executors.py:422] [PAX STATUS]:  Starting eval_step().\n",
      "I0926 00:50:25.762228 138882247022144 summary_utils.py:668] [PAX STATUS] step_i: 100, training loss: 2.2580194\n",
      "I0926 00:50:25.762307 138882247022144 summary_utils.py:671] weighted_scalars: {'accuracy': (array(0.4091797, dtype=float32), array(1., dtype=float32)), 'loss': (array(2.2580194, dtype=float32), array(1., dtype=float32))}\n",
      "I0926 00:50:25.762824 138882247022144 summary_utils.py:673] per_example_out: {'predictions': array([[ 0.01765364,  0.05092727, -0.0286095 , ...,  0.07827096,\n",
      "         0.03682335,  0.03216754],\n",
      "       [-0.01540886,  0.01722307, -0.02678141, ...,  0.06670806,\n",
      "         0.04152134,  0.05551143],\n",
      "       [ 0.04155483,  0.04538516, -0.0167989 , ..., -0.02414525,\n",
      "         0.05802527,  0.0336822 ],\n",
      "       ...,\n",
      "       [-0.03940495,  0.04670978, -0.04799388, ...,  0.08356942,\n",
      "         0.05651307,  0.0862369 ],\n",
      "       [-0.01815973,  0.00804599,  0.00214278, ...,  0.02696434,\n",
      "         0.05913538,  0.03695675],\n",
      "       [-0.03946526,  0.04231804,  0.02596226, ...,  0.03965276,\n",
      "         0.02730173, -0.00959596]], dtype=float32)}\n",
      "I0926 00:50:25.763221 138882247022144 summary_utils.py:674] summary_tensors: {'learning/applied_grad_norm_aggregate_scalar': array(0.0026132, dtype=float32), 'learning/clipped_grad_norm_aggregate_scalar': array(0.2397781, dtype=float32), 'learning/grad_scale_aggregate_scalar': array(1., dtype=float32), 'learning/is_valid_step_aggregate_scalar': array(1., dtype=float32), 'learning/lr_aggregate_scalar': array(0.01, dtype=float32), 'learning/raw_grad_norm_aggregate_scalar': array(0.2397781, dtype=float32), 'learning/var_norm_aggregate_scalar': array(23.197987, dtype=float32), 'lr_aggregate_scalar': array(0.01, dtype=float32), 'vars/params/network/conv1/b_scalar': array(0.0049221, dtype=float32), 'vars/params/network/conv1/w_scalar': array(0.08445952, dtype=float32), 'vars/params/network/conv2/b_scalar': array(0.0098938, dtype=float32), 'vars/params/network/conv2/w_scalar': array(0.04830401, dtype=float32), 'vars/params/network/dense_0/bias/b_scalar': array(0.00375477, dtype=float32), 'vars/params/network/dense_0/linear/w_scalar': array(0.0242846, dtype=float32), 'vars/params/network/dense_1/w_scalar': array(0.08755321, dtype=float32), 'vars/params/network/dense_2/b_scalar': array(0.00470723, dtype=float32)}\n",
      "I0926 00:50:25.764640 138882247022144 summary_utils.py:340] summary tensor at step=100 loss 2.25801944732666\n",
      "I0926 00:50:25.765943 138882247022144 summary_utils.py:340] summary tensor at step=100 Steps/sec 51.61667612206447\n",
      "I0926 00:50:25.766371 138882247022144 summary_utils.py:427] Metrics values at step 100:\n",
      "I0926 00:50:25.766429 138882247022144 summary_utils.py:428]   loss=2.258019\n",
      "I0926 00:50:25.766512 138882247022144 summary_utils.py:440]   accuracy=0.409180 (weight=1.000000)\n",
      "I0926 00:50:25.766582 138882247022144 summary_utils.py:340] summary tensor at step=100 Metrics/accuracy 0.4091796875\n",
      "I0926 00:50:25.766983 138882247022144 summary_utils.py:340] summary tensor at step=100 Metrics/accuracy-weight 1.0\n",
      "I0926 00:50:25.767347 138882247022144 summary_utils.py:440]   loss=2.258019 (weight=1.000000)\n",
      "I0926 00:50:25.767431 138882247022144 summary_utils.py:340] summary tensor at step=100 Metrics/loss 2.25801944732666\n",
      "I0926 00:50:25.767766 138882247022144 summary_utils.py:340] summary tensor at step=100 Metrics/loss-weight 1.0\n",
      "I0926 00:50:25.768073 138882247022144 local.py:45] Setting task status: step = 100, loss= 2.25801944732666, steps/sec 51.61667612206447, accuracy=0.4091796875, loss=2.25801944732666\n",
      "I0926 00:50:25.768219 138882247022144 summary_utils.py:340] summary tensor at step=100 learning/applied_grad_norm_scalar 0.002613200806081295\n",
      "I0926 00:50:25.768617 138882247022144 summary_utils.py:340] summary tensor at step=100 learning/clipped_grad_norm_scalar 0.23977810144424438\n",
      "I0926 00:50:25.769008 138882247022144 summary_utils.py:340] summary tensor at step=100 learning/grad_scale_scalar 1.0\n",
      "I0926 00:50:25.769386 138882247022144 summary_utils.py:340] summary tensor at step=100 learning/is_valid_step_scalar 1.0\n",
      "I0926 00:50:25.769756 138882247022144 summary_utils.py:340] summary tensor at step=100 learning/lr_scalar 0.009999999776482582\n",
      "I0926 00:50:25.770241 138882247022144 summary_utils.py:340] summary tensor at step=100 learning/raw_grad_norm_scalar 0.23977810144424438\n",
      "I0926 00:50:25.770644 138882247022144 summary_utils.py:340] summary tensor at step=100 learning/var_norm_scalar 23.197986602783203\n",
      "I0926 00:50:25.771061 138882247022144 summary_utils.py:340] summary tensor at step=100 lr_scalar 0.009999999776482582\n",
      "I0926 00:50:25.771474 138882247022144 summary_utils.py:340] summary tensor at step=100 vars/params/network/conv1/b_scalar 0.004922100342810154\n",
      "I0926 00:50:25.771866 138882247022144 summary_utils.py:340] summary tensor at step=100 vars/params/network/conv1/w_scalar 0.08445952087640762\n",
      "I0926 00:50:25.772258 138882247022144 summary_utils.py:340] summary tensor at step=100 vars/params/network/conv2/b_scalar 0.009893802925944328\n",
      "I0926 00:50:25.772670 138882247022144 summary_utils.py:340] summary tensor at step=100 vars/params/network/conv2/w_scalar 0.048304006457328796\n",
      "I0926 00:50:25.773063 138882247022144 summary_utils.py:340] summary tensor at step=100 vars/params/network/dense_0/bias/b_scalar 0.0037547717802226543\n",
      "I0926 00:50:25.773495 138882247022144 summary_utils.py:340] summary tensor at step=100 vars/params/network/dense_0/linear/w_scalar 0.024284595623612404\n",
      "I0926 00:50:25.773917 138882247022144 summary_utils.py:340] summary tensor at step=100 vars/params/network/dense_1/w_scalar 0.08755321055650711\n",
      "I0926 00:50:25.774361 138882247022144 summary_utils.py:340] summary tensor at step=100 vars/params/network/dense_2/b_scalar 0.004707226529717445\n",
      "I0926 00:50:25.774811 138882247022144 summary_utils.py:457] Wrote summary entry at step `100` (loss=`2.258019`).\n",
      "I0926 00:50:25.966376 140035128160896 programs.py:468] steps/sec: 48.136802\n",
      "/home/awolant/Projects/DALI/dali/operators/reader/loader/lmdb.h:76: lmdb 0 /home/awolant/Projects/DALI_extra/db/MNIST/training/ rewind to the begin from 59999\n",
      "/home/awolant/Projects/DALI/dali/operators/reader/loader/lmdb.h:76: lmdb 0 /home/awolant/Projects/DALI_extra/db/MNIST/training/ rewind to the begin from 0\n",
      "W0926 00:50:26.130404 140035128160896 base_iterator.py:437] DALI iterator does not support resetting while epoch is not finished.                              Ignoring...\n",
      "I0926 00:50:26.172232 140035128160896 programs.py:468] steps/sec: 48.606224\n",
      "I0926 00:50:26.371433 140035128160896 programs.py:468] steps/sec: 50.232389\n",
      "I0926 00:50:26.572546 140035128160896 programs.py:468] steps/sec: 49.753433\n",
      "I0926 00:50:26.779532 140035128160896 programs.py:468] steps/sec: 48.344143\n",
      "I0926 00:50:26.980736 140035128160896 programs.py:468] steps/sec: 49.731253\n",
      "I0926 00:50:27.178891 140035128160896 programs.py:468] steps/sec: 50.498130\n",
      "/home/awolant/Projects/DALI/dali/operators/reader/loader/lmdb.h:76: lmdb 0 /home/awolant/Projects/DALI_extra/db/MNIST/training/ rewind to the begin from 59999\n",
      "/home/awolant/Projects/DALI/dali/operators/reader/loader/lmdb.h:76: lmdb 0 /home/awolant/Projects/DALI_extra/db/MNIST/training/ rewind to the begin from 0\n",
      "W0926 00:50:27.307528 140035128160896 base_iterator.py:437] DALI iterator does not support resetting while epoch is not finished.                              Ignoring...\n",
      "I0926 00:50:27.385360 140035128160896 programs.py:468] steps/sec: 48.470012\n",
      "I0926 00:50:27.583381 140035128160896 programs.py:468] steps/sec: 50.532018\n",
      "I0926 00:50:27.778792 140035128160896 programs.py:468] steps/sec: 51.206688\n",
      "I0926 00:50:27.779195 140035128160896 executors.py:422] [PAX STATUS]:  Starting eval_step().\n",
      "I0926 00:50:27.782431 138882247022144 summary_utils.py:668] [PAX STATUS] step_i: 200, training loss: 2.147273\n",
      "I0926 00:50:27.782522 138882247022144 summary_utils.py:671] weighted_scalars: {'accuracy': (array(0.53808594, dtype=float32), array(1., dtype=float32)), 'loss': (array(2.147273, dtype=float32), array(1., dtype=float32))}\n",
      "I0926 00:50:27.782996 138882247022144 summary_utils.py:673] per_example_out: {'predictions': array([[ 0.47401738, -0.12255757,  0.20743345, ..., -0.20595123,\n",
      "         0.0585124 , -0.18898857],\n",
      "       [-0.11645474,  0.06347895, -0.10948217, ...,  0.06321173,\n",
      "         0.14657891,  0.15338454],\n",
      "       [-0.10347145,  0.08178113, -0.07860412, ...,  0.04348806,\n",
      "         0.20170797,  0.10140106],\n",
      "       ...,\n",
      "       [-0.06146809,  0.21365534, -0.02987398, ..., -0.03098005,\n",
      "         0.08108064,  0.02566466],\n",
      "       [-0.03461876,  0.00330551,  0.13189083, ..., -0.10069753,\n",
      "         0.19381143, -0.04462395],\n",
      "       [ 0.14187774, -0.06118935,  0.02825469, ..., -0.06895892,\n",
      "         0.08114249,  0.02862388]], dtype=float32)}\n",
      "I0926 00:50:27.783381 138882247022144 summary_utils.py:674] summary_tensors: {'learning/applied_grad_norm_aggregate_scalar': array(0.00516251, dtype=float32), 'learning/clipped_grad_norm_aggregate_scalar': array(0.46498886, dtype=float32), 'learning/grad_scale_aggregate_scalar': array(1., dtype=float32), 'learning/is_valid_step_aggregate_scalar': array(1., dtype=float32), 'learning/lr_aggregate_scalar': array(0.01, dtype=float32), 'learning/raw_grad_norm_aggregate_scalar': array(0.46498886, dtype=float32), 'learning/var_norm_aggregate_scalar': array(23.213446, dtype=float32), 'lr_aggregate_scalar': array(0.01, dtype=float32), 'vars/params/network/conv1/b_scalar': array(0.01004957, dtype=float32), 'vars/params/network/conv1/w_scalar': array(0.08774003, dtype=float32), 'vars/params/network/conv2/b_scalar': array(0.01927748, dtype=float32), 'vars/params/network/conv2/w_scalar': array(0.04839664, dtype=float32), 'vars/params/network/dense_0/bias/b_scalar': array(0.00629174, dtype=float32), 'vars/params/network/dense_0/linear/w_scalar': array(0.02428929, dtype=float32), 'vars/params/network/dense_1/w_scalar': array(0.08797316, dtype=float32), 'vars/params/network/dense_2/b_scalar': array(0.00997496, dtype=float32)}\n",
      "I0926 00:50:27.784746 138882247022144 summary_utils.py:340] summary tensor at step=200 loss 2.147273063659668\n",
      "I0926 00:50:27.785464 138882247022144 summary_utils.py:340] summary tensor at step=200 Steps/sec 51.206688373603484\n",
      "I0926 00:50:27.785842 138882247022144 summary_utils.py:427] Metrics values at step 200:\n",
      "I0926 00:50:27.785895 138882247022144 summary_utils.py:428]   loss=2.147273\n",
      "I0926 00:50:27.785962 138882247022144 summary_utils.py:440]   accuracy=0.538086 (weight=1.000000)\n",
      "I0926 00:50:27.786033 138882247022144 summary_utils.py:340] summary tensor at step=200 Metrics/accuracy 0.5380859375\n",
      "I0926 00:50:27.786411 138882247022144 summary_utils.py:340] summary tensor at step=200 Metrics/accuracy-weight 1.0\n",
      "I0926 00:50:27.786775 138882247022144 summary_utils.py:440]   loss=2.147273 (weight=1.000000)\n",
      "I0926 00:50:27.786861 138882247022144 summary_utils.py:340] summary tensor at step=200 Metrics/loss 2.147273063659668\n",
      "I0926 00:50:27.787227 138882247022144 summary_utils.py:340] summary tensor at step=200 Metrics/loss-weight 1.0\n",
      "I0926 00:50:27.787534 138882247022144 local.py:45] Setting task status: step = 200, loss= 2.147273063659668, steps/sec 51.206688373603484, accuracy=0.5380859375, loss=2.147273063659668\n",
      "I0926 00:50:27.787693 138882247022144 summary_utils.py:340] summary tensor at step=200 learning/applied_grad_norm_scalar 0.005162510089576244\n",
      "I0926 00:50:27.788104 138882247022144 summary_utils.py:340] summary tensor at step=200 learning/clipped_grad_norm_scalar 0.4649888575077057\n",
      "I0926 00:50:27.788515 138882247022144 summary_utils.py:340] summary tensor at step=200 learning/grad_scale_scalar 1.0\n",
      "I0926 00:50:27.788918 138882247022144 summary_utils.py:340] summary tensor at step=200 learning/is_valid_step_scalar 1.0\n",
      "I0926 00:50:27.789283 138882247022144 summary_utils.py:340] summary tensor at step=200 learning/lr_scalar 0.009999999776482582\n",
      "I0926 00:50:27.789883 138882247022144 summary_utils.py:340] summary tensor at step=200 learning/raw_grad_norm_scalar 0.4649888575077057\n",
      "I0926 00:50:27.790266 138882247022144 summary_utils.py:340] summary tensor at step=200 learning/var_norm_scalar 23.21344566345215\n",
      "I0926 00:50:27.790656 138882247022144 summary_utils.py:340] summary tensor at step=200 lr_scalar 0.009999999776482582\n",
      "I0926 00:50:27.791028 138882247022144 summary_utils.py:340] summary tensor at step=200 vars/params/network/conv1/b_scalar 0.01004957128316164\n",
      "I0926 00:50:27.791479 138882247022144 summary_utils.py:340] summary tensor at step=200 vars/params/network/conv1/w_scalar 0.08774003386497498\n",
      "I0926 00:50:27.791894 138882247022144 summary_utils.py:340] summary tensor at step=200 vars/params/network/conv2/b_scalar 0.019277481362223625\n",
      "I0926 00:50:27.792309 138882247022144 summary_utils.py:340] summary tensor at step=200 vars/params/network/conv2/w_scalar 0.04839663952589035\n",
      "I0926 00:50:27.792722 138882247022144 summary_utils.py:340] summary tensor at step=200 vars/params/network/dense_0/bias/b_scalar 0.00629173731431365\n",
      "I0926 00:50:27.793109 138882247022144 summary_utils.py:340] summary tensor at step=200 vars/params/network/dense_0/linear/w_scalar 0.024289285764098167\n",
      "I0926 00:50:27.793495 138882247022144 summary_utils.py:340] summary tensor at step=200 vars/params/network/dense_1/w_scalar 0.08797315508127213\n",
      "I0926 00:50:27.793880 138882247022144 summary_utils.py:340] summary tensor at step=200 vars/params/network/dense_2/b_scalar 0.009974963031709194\n",
      "I0926 00:50:27.794301 138882247022144 summary_utils.py:457] Wrote summary entry at step `200` (loss=`2.147273`).\n",
      "I0926 00:50:27.976253 140035128160896 programs.py:468] steps/sec: 50.676471\n",
      "I0926 00:50:28.175321 140035128160896 programs.py:468] steps/sec: 50.268872\n",
      "I0926 00:50:28.380868 140035128160896 programs.py:468] steps/sec: 48.681426\n",
      "/home/awolant/Projects/DALI/dali/operators/reader/loader/lmdb.h:76: lmdb 0 /home/awolant/Projects/DALI_extra/db/MNIST/training/ rewind to the begin from 59999\n",
      "/home/awolant/Projects/DALI/dali/operators/reader/loader/lmdb.h:76: lmdb 0 /home/awolant/Projects/DALI_extra/db/MNIST/training/ rewind to the begin from 0\n",
      "W0926 00:50:28.487474 140035128160896 base_iterator.py:437] DALI iterator does not support resetting while epoch is not finished.                              Ignoring...\n",
      "I0926 00:50:28.584389 140035128160896 programs.py:468] steps/sec: 49.164176\n",
      "I0926 00:50:28.784863 140035128160896 programs.py:468] steps/sec: 49.913889\n",
      "I0926 00:50:28.983533 140035128160896 programs.py:468] steps/sec: 50.367389\n",
      "I0926 00:50:29.177281 140035128160896 programs.py:468] steps/sec: 51.646676\n",
      "I0926 00:50:29.374995 140035128160896 programs.py:468] steps/sec: 50.612385\n",
      "/home/awolant/Projects/DALI/dali/operators/reader/loader/lmdb.h:76: lmdb 0 /home/awolant/Projects/DALI_extra/db/MNIST/training/ rewind to the begin from 59999\n",
      "/home/awolant/Projects/DALI/dali/operators/reader/loader/lmdb.h:76: lmdb 0 /home/awolant/Projects/DALI_extra/db/MNIST/training/ rewind to the begin from 0\n",
      "I0926 00:50:29.576160 140035128160896 programs.py:468] steps/sec: 49.748949\n",
      "W0926 00:50:29.634626 140035128160896 base_iterator.py:437] DALI iterator does not support resetting while epoch is not finished.                              Ignoring...\n",
      "I0926 00:50:29.774249 140035128160896 programs.py:468] steps/sec: 50.513517\n",
      "I0926 00:50:29.774653 140035128160896 executors.py:422] [PAX STATUS]:  Starting eval_step().\n",
      "I0926 00:50:29.777905 138882247022144 summary_utils.py:668] [PAX STATUS] step_i: 300, training loss: 1.4150331\n",
      "I0926 00:50:29.777992 138882247022144 summary_utils.py:671] weighted_scalars: {'accuracy': (array(0.74609375, dtype=float32), array(1., dtype=float32)), 'loss': (array(1.4150331, dtype=float32), array(1., dtype=float32))}\n",
      "I0926 00:50:29.778523 138882247022144 summary_utils.py:673] per_example_out: {'predictions': array([[-0.84301776,  0.56653047, -0.22413322, ..., -0.23407637,\n",
      "         0.51802343,  0.33375597],\n",
      "       [ 0.4480598 , -0.26119122, -0.78968436, ...,  0.38266805,\n",
      "         0.7003367 ,  0.63973695],\n",
      "       [ 0.02362194, -0.22282872, -0.14617015, ...,  0.064908  ,\n",
      "         0.37040862,  0.28217578],\n",
      "       ...,\n",
      "       [-0.78331697, -0.41009226, -0.09127519, ..., -0.2926035 ,\n",
      "         0.43093762,  0.53871846],\n",
      "       [ 1.6455216 , -0.762514  ,  0.1404155 , ..., -0.6234619 ,\n",
      "        -0.01955272, -0.17945693],\n",
      "       [ 0.33622026, -1.2648083 ,  0.9072785 , ..., -0.08916648,\n",
      "         0.05131119, -0.24584557]], dtype=float32)}\n",
      "I0926 00:50:29.778921 138882247022144 summary_utils.py:674] summary_tensors: {'learning/applied_grad_norm_aggregate_scalar': array(0.01349964, dtype=float32), 'learning/clipped_grad_norm_aggregate_scalar': array(1.2478863, dtype=float32), 'learning/grad_scale_aggregate_scalar': array(1., dtype=float32), 'learning/is_valid_step_aggregate_scalar': array(1., dtype=float32), 'learning/lr_aggregate_scalar': array(0.01, dtype=float32), 'learning/raw_grad_norm_aggregate_scalar': array(1.2478863, dtype=float32), 'learning/var_norm_aggregate_scalar': array(23.280052, dtype=float32), 'lr_aggregate_scalar': array(0.01, dtype=float32), 'vars/params/network/conv1/b_scalar': array(0.02118929, dtype=float32), 'vars/params/network/conv1/w_scalar': array(0.10125327, dtype=float32), 'vars/params/network/conv2/b_scalar': array(0.03248928, dtype=float32), 'vars/params/network/conv2/w_scalar': array(0.04881322, dtype=float32), 'vars/params/network/dense_0/bias/b_scalar': array(0.00881792, dtype=float32), 'vars/params/network/dense_0/linear/w_scalar': array(0.02430957, dtype=float32), 'vars/params/network/dense_1/w_scalar': array(0.08972653, dtype=float32), 'vars/params/network/dense_2/b_scalar': array(0.0201032, dtype=float32)}\n",
      "I0926 00:50:29.780166 138882247022144 summary_utils.py:340] summary tensor at step=300 loss 1.4150331020355225\n",
      "I0926 00:50:29.780913 138882247022144 summary_utils.py:340] summary tensor at step=300 Steps/sec 50.513516866124796\n",
      "I0926 00:50:29.781293 138882247022144 summary_utils.py:427] Metrics values at step 300:\n",
      "I0926 00:50:29.781345 138882247022144 summary_utils.py:428]   loss=1.415033\n",
      "I0926 00:50:29.781414 138882247022144 summary_utils.py:440]   accuracy=0.746094 (weight=1.000000)\n",
      "I0926 00:50:29.781476 138882247022144 summary_utils.py:340] summary tensor at step=300 Metrics/accuracy 0.74609375\n",
      "I0926 00:50:29.781828 138882247022144 summary_utils.py:340] summary tensor at step=300 Metrics/accuracy-weight 1.0\n",
      "I0926 00:50:29.782177 138882247022144 summary_utils.py:440]   loss=1.415033 (weight=1.000000)\n",
      "I0926 00:50:29.782257 138882247022144 summary_utils.py:340] summary tensor at step=300 Metrics/loss 1.4150331020355225\n",
      "I0926 00:50:29.782585 138882247022144 summary_utils.py:340] summary tensor at step=300 Metrics/loss-weight 1.0\n",
      "I0926 00:50:29.782886 138882247022144 local.py:45] Setting task status: step = 300, loss= 1.4150331020355225, steps/sec 50.513516866124796, accuracy=0.74609375, loss=1.4150331020355225\n",
      "I0926 00:50:29.783020 138882247022144 summary_utils.py:340] summary tensor at step=300 learning/applied_grad_norm_scalar 0.013499635271728039\n",
      "I0926 00:50:29.783400 138882247022144 summary_utils.py:340] summary tensor at step=300 learning/clipped_grad_norm_scalar 1.247886300086975\n",
      "I0926 00:50:29.783782 138882247022144 summary_utils.py:340] summary tensor at step=300 learning/grad_scale_scalar 1.0\n",
      "I0926 00:50:29.784147 138882247022144 summary_utils.py:340] summary tensor at step=300 learning/is_valid_step_scalar 1.0\n",
      "I0926 00:50:29.784508 138882247022144 summary_utils.py:340] summary tensor at step=300 learning/lr_scalar 0.009999999776482582\n",
      "I0926 00:50:29.784951 138882247022144 summary_utils.py:340] summary tensor at step=300 learning/raw_grad_norm_scalar 1.247886300086975\n",
      "I0926 00:50:29.785344 138882247022144 summary_utils.py:340] summary tensor at step=300 learning/var_norm_scalar 23.280052185058594\n",
      "I0926 00:50:29.785760 138882247022144 summary_utils.py:340] summary tensor at step=300 lr_scalar 0.009999999776482582\n",
      "I0926 00:50:29.786136 138882247022144 summary_utils.py:340] summary tensor at step=300 vars/params/network/conv1/b_scalar 0.021189285442233086\n",
      "I0926 00:50:29.786520 138882247022144 summary_utils.py:340] summary tensor at step=300 vars/params/network/conv1/w_scalar 0.10125327110290527\n",
      "I0926 00:50:29.786905 138882247022144 summary_utils.py:340] summary tensor at step=300 vars/params/network/conv2/b_scalar 0.03248927742242813\n",
      "I0926 00:50:29.788298 138882247022144 summary_utils.py:340] summary tensor at step=300 vars/params/network/conv2/w_scalar 0.048813216388225555\n",
      "I0926 00:50:29.790882 138882247022144 summary_utils.py:340] summary tensor at step=300 vars/params/network/dense_0/bias/b_scalar 0.008817917667329311\n",
      "I0926 00:50:29.791326 138882247022144 summary_utils.py:340] summary tensor at step=300 vars/params/network/dense_0/linear/w_scalar 0.024309571832418442\n",
      "I0926 00:50:29.792104 138882247022144 summary_utils.py:340] summary tensor at step=300 vars/params/network/dense_1/w_scalar 0.0897265300154686\n",
      "I0926 00:50:29.792531 138882247022144 summary_utils.py:340] summary tensor at step=300 vars/params/network/dense_2/b_scalar 0.020103197544813156\n",
      "I0926 00:50:29.792987 138882247022144 summary_utils.py:457] Wrote summary entry at step `300` (loss=`1.415033`).\n",
      "I0926 00:50:29.967206 140035128160896 programs.py:468] steps/sec: 51.859512\n",
      "I0926 00:50:30.168615 140035128160896 programs.py:468] steps/sec: 49.682536\n",
      "I0926 00:50:30.377715 140035128160896 programs.py:468] steps/sec: 47.852651\n",
      "I0926 00:50:30.575135 140035128160896 programs.py:468] steps/sec: 50.690496\n",
      "/home/awolant/Projects/DALI/dali/operators/reader/loader/lmdb.h:76: lmdb 0 /home/awolant/Projects/DALI_extra/db/MNIST/training/ rewind to the begin from 59999\n",
      "/home/awolant/Projects/DALI/dali/operators/reader/loader/lmdb.h:76: lmdb 0 /home/awolant/Projects/DALI_extra/db/MNIST/training/ rewind to the begin from 0\n",
      "I0926 00:50:30.781257 140035128160896 programs.py:468] steps/sec: 48.544511\n",
      "W0926 00:50:30.823221 140035128160896 base_iterator.py:437] DALI iterator does not support resetting while epoch is not finished.                              Ignoring...\n",
      "I0926 00:50:30.983730 140035128160896 programs.py:468] steps/sec: 49.419405\n",
      "I0926 00:50:31.184368 140035128160896 programs.py:468] steps/sec: 49.872225\n",
      "I0926 00:50:31.398164 140035128160896 programs.py:468] steps/sec: 46.804585\n",
      "I0926 00:50:31.595596 140035128160896 programs.py:468] steps/sec: 50.686024\n",
      "I0926 00:50:31.797254 140035128160896 programs.py:468] steps/sec: 49.622933\n",
      "I0926 00:50:31.797667 140035128160896 executors.py:422] [PAX STATUS]:  Starting eval_step().\n",
      "I0926 00:50:31.800450 138882247022144 summary_utils.py:668] [PAX STATUS] step_i: 400, training loss: 0.66080874\n",
      "I0926 00:50:31.800521 138882247022144 summary_utils.py:671] weighted_scalars: {'accuracy': (array(0.8232422, dtype=float32), array(1., dtype=float32)), 'loss': (array(0.66080874, dtype=float32), array(1., dtype=float32))}\n",
      "I0926 00:50:31.800984 138882247022144 summary_utils.py:673] per_example_out: {'predictions': array([[ 0.454017  ,  1.1866499 , -0.7041124 , ..., -1.2485008 ,\n",
      "         4.2560854 ,  0.4799356 ],\n",
      "       [ 1.093145  , -5.6225014 ,  0.8079049 , ..., -3.2594202 ,\n",
      "         3.093747  ,  1.1945986 ],\n",
      "       [ 0.8262023 , -2.3649147 ,  2.3392313 , ..., -3.4098225 ,\n",
      "         0.9743541 , -0.9992077 ],\n",
      "       ...,\n",
      "       [ 0.03770855,  0.76917607,  0.92245334, ...,  0.09219734,\n",
      "         0.30248252, -0.40580684],\n",
      "       [ 5.716487  , -4.2926416 , -0.01016915, ..., -0.28742242,\n",
      "         1.0483699 , -0.42943937],\n",
      "       [ 3.4588938 , -2.9640353 ,  0.46044797, ..., -1.6421311 ,\n",
      "         0.8354294 , -0.56103903]], dtype=float32)}\n",
      "I0926 00:50:31.801318 138882247022144 summary_utils.py:674] summary_tensors: {'learning/applied_grad_norm_aggregate_scalar': array(0.00969896, dtype=float32), 'learning/clipped_grad_norm_aggregate_scalar': array(0.9403692, dtype=float32), 'learning/grad_scale_aggregate_scalar': array(1., dtype=float32), 'learning/is_valid_step_aggregate_scalar': array(1., dtype=float32), 'learning/lr_aggregate_scalar': array(0.01, dtype=float32), 'learning/raw_grad_norm_aggregate_scalar': array(0.9403692, dtype=float32), 'learning/var_norm_aggregate_scalar': array(23.384424, dtype=float32), 'lr_aggregate_scalar': array(0.01, dtype=float32), 'vars/params/network/conv1/b_scalar': array(0.03040961, dtype=float32), 'vars/params/network/conv1/w_scalar': array(0.1195849, dtype=float32), 'vars/params/network/conv2/b_scalar': array(0.04014875, dtype=float32), 'vars/params/network/conv2/w_scalar': array(0.04946519, dtype=float32), 'vars/params/network/dense_0/bias/b_scalar': array(0.00994798, dtype=float32), 'vars/params/network/dense_0/linear/w_scalar': array(0.02434082, dtype=float32), 'vars/params/network/dense_1/w_scalar': array(0.09234481, dtype=float32), 'vars/params/network/dense_2/b_scalar': array(0.02656407, dtype=float32)}\n",
      "I0926 00:50:31.802604 138882247022144 summary_utils.py:340] summary tensor at step=400 loss 0.6608087420463562\n",
      "I0926 00:50:31.803330 138882247022144 summary_utils.py:340] summary tensor at step=400 Steps/sec 49.62293326707957\n",
      "I0926 00:50:31.803704 138882247022144 summary_utils.py:427] Metrics values at step 400:\n",
      "I0926 00:50:31.803756 138882247022144 summary_utils.py:428]   loss=0.660809\n",
      "I0926 00:50:31.803822 138882247022144 summary_utils.py:440]   accuracy=0.823242 (weight=1.000000)\n",
      "I0926 00:50:31.803882 138882247022144 summary_utils.py:340] summary tensor at step=400 Metrics/accuracy 0.8232421875\n",
      "I0926 00:50:31.804235 138882247022144 summary_utils.py:340] summary tensor at step=400 Metrics/accuracy-weight 1.0\n",
      "I0926 00:50:31.804587 138882247022144 summary_utils.py:440]   loss=0.660809 (weight=1.000000)\n",
      "I0926 00:50:31.804674 138882247022144 summary_utils.py:340] summary tensor at step=400 Metrics/loss 0.6608087420463562\n",
      "I0926 00:50:31.805026 138882247022144 summary_utils.py:340] summary tensor at step=400 Metrics/loss-weight 1.0\n",
      "I0926 00:50:31.805350 138882247022144 local.py:45] Setting task status: step = 400, loss= 0.6608087420463562, steps/sec 49.62293326707957, accuracy=0.8232421875, loss=0.6608087420463562\n",
      "I0926 00:50:31.805492 138882247022144 summary_utils.py:340] summary tensor at step=400 learning/applied_grad_norm_scalar 0.009698959067463875\n",
      "I0926 00:50:31.805899 138882247022144 summary_utils.py:340] summary tensor at step=400 learning/clipped_grad_norm_scalar 0.940369188785553\n",
      "I0926 00:50:31.806318 138882247022144 summary_utils.py:340] summary tensor at step=400 learning/grad_scale_scalar 1.0\n",
      "I0926 00:50:31.806711 138882247022144 summary_utils.py:340] summary tensor at step=400 learning/is_valid_step_scalar 1.0\n",
      "I0926 00:50:31.807104 138882247022144 summary_utils.py:340] summary tensor at step=400 learning/lr_scalar 0.009999999776482582\n",
      "I0926 00:50:31.807551 138882247022144 summary_utils.py:340] summary tensor at step=400 learning/raw_grad_norm_scalar 0.940369188785553\n",
      "I0926 00:50:31.807951 138882247022144 summary_utils.py:340] summary tensor at step=400 learning/var_norm_scalar 23.384424209594727\n",
      "I0926 00:50:31.808361 138882247022144 summary_utils.py:340] summary tensor at step=400 lr_scalar 0.009999999776482582\n",
      "I0926 00:50:31.808758 138882247022144 summary_utils.py:340] summary tensor at step=400 vars/params/network/conv1/b_scalar 0.030409613624215126\n",
      "I0926 00:50:31.809170 138882247022144 summary_utils.py:340] summary tensor at step=400 vars/params/network/conv1/w_scalar 0.11958490312099457\n",
      "I0926 00:50:31.809584 138882247022144 summary_utils.py:340] summary tensor at step=400 vars/params/network/conv2/b_scalar 0.040148746222257614\n",
      "I0926 00:50:31.809967 138882247022144 summary_utils.py:340] summary tensor at step=400 vars/params/network/conv2/w_scalar 0.04946519061923027\n",
      "I0926 00:50:31.810409 138882247022144 summary_utils.py:340] summary tensor at step=400 vars/params/network/dense_0/bias/b_scalar 0.00994797982275486\n",
      "I0926 00:50:31.810820 138882247022144 summary_utils.py:340] summary tensor at step=400 vars/params/network/dense_0/linear/w_scalar 0.024340815842151642\n",
      "I0926 00:50:31.811244 138882247022144 summary_utils.py:340] summary tensor at step=400 vars/params/network/dense_1/w_scalar 0.09234480559825897\n",
      "I0926 00:50:31.811627 138882247022144 summary_utils.py:340] summary tensor at step=400 vars/params/network/dense_2/b_scalar 0.02656407281756401\n",
      "I0926 00:50:31.812026 138882247022144 summary_utils.py:457] Wrote summary entry at step `400` (loss=`0.660809`).\n",
      "/home/awolant/Projects/DALI/dali/operators/reader/loader/lmdb.h:76: lmdb 0 /home/awolant/Projects/DALI_extra/db/MNIST/training/ rewind to the begin from 59999\n",
      "/home/awolant/Projects/DALI/dali/operators/reader/loader/lmdb.h:76: lmdb 0 /home/awolant/Projects/DALI_extra/db/MNIST/training/ rewind to the begin from 0\n",
      "I0926 00:50:31.993911 140035128160896 programs.py:468] steps/sec: 50.883099\n",
      "W0926 00:50:32.011698 140035128160896 base_iterator.py:437] DALI iterator does not support resetting while epoch is not finished.                              Ignoring...\n",
      "I0926 00:50:32.204231 140035128160896 programs.py:468] steps/sec: 47.577015\n",
      "I0926 00:50:32.407294 140035128160896 programs.py:468] steps/sec: 49.280341\n",
      "I0926 00:50:32.604726 140035128160896 programs.py:468] steps/sec: 50.685779\n",
      "I0926 00:50:32.805792 140035128160896 programs.py:468] steps/sec: 49.767070\n",
      "I0926 00:50:33.007793 140035128160896 programs.py:468] steps/sec: 49.535728\n",
      "/home/awolant/Projects/DALI/dali/operators/reader/loader/lmdb.h:76: lmdb 0 /home/awolant/Projects/DALI_extra/db/MNIST/training/ rewind to the begin from 59999\n",
      "/home/awolant/Projects/DALI/dali/operators/reader/loader/lmdb.h:76: lmdb 0 /home/awolant/Projects/DALI_extra/db/MNIST/training/ rewind to the begin from 0\n",
      "W0926 00:50:33.186963 140035128160896 base_iterator.py:437] DALI iterator does not support resetting while epoch is not finished.                              Ignoring...\n",
      "I0926 00:50:33.206062 140035128160896 programs.py:468] steps/sec: 50.469146\n",
      "I0926 00:50:33.412297 140035128160896 programs.py:468] steps/sec: 48.518960\n",
      "I0926 00:50:33.620557 140035128160896 programs.py:468] steps/sec: 48.045765\n",
      "I0926 00:50:33.815870 140035128160896 programs.py:468] steps/sec: 51.233960\n",
      "I0926 00:50:33.816255 140035128160896 executors.py:422] [PAX STATUS]:  Starting eval_step().\n",
      "I0926 00:50:33.819317 138882247022144 summary_utils.py:668] [PAX STATUS] step_i: 500, training loss: 0.42806652\n",
      "I0926 00:50:33.819408 138882247022144 summary_utils.py:671] weighted_scalars: {'accuracy': (array(0.88671875, dtype=float32), array(1., dtype=float32)), 'loss': (array(0.42806652, dtype=float32), array(1., dtype=float32))}\n",
      "I0926 00:50:33.819900 138882247022144 summary_utils.py:673] per_example_out: {'predictions': array([[ -1.8303779 ,   2.694547  ,   2.099266  , ...,  -0.95828533,\n",
      "          2.3787978 ,  -0.15613472],\n",
      "       [ -0.08836366,  -1.6089131 ,  -0.67650497, ...,   7.5681415 ,\n",
      "          0.37599432,   3.0847244 ],\n",
      "       [ -1.0004075 ,   3.7235613 ,   0.47011065, ...,  -1.1876954 ,\n",
      "          1.2179188 ,  -0.6100445 ],\n",
      "       ...,\n",
      "       [ -3.1305635 ,   4.120971  ,   0.4522845 , ...,  -1.2578833 ,\n",
      "          1.3403683 ,  -0.33978865],\n",
      "       [  4.681666  ,  -6.6972585 ,  -0.4420655 , ...,  -2.631207  ,\n",
      "          2.6407297 ,  -0.7948081 ],\n",
      "       [ 11.612543  , -11.912582  ,   2.632494  , ...,  -6.017209  ,\n",
      "          3.6840496 ,  -4.3283668 ]], dtype=float32)}\n",
      "I0926 00:50:33.820260 138882247022144 summary_utils.py:674] summary_tensors: {'learning/applied_grad_norm_aggregate_scalar': array(0.00627031, dtype=float32), 'learning/clipped_grad_norm_aggregate_scalar': array(0.643354, dtype=float32), 'learning/grad_scale_aggregate_scalar': array(1., dtype=float32), 'learning/is_valid_step_aggregate_scalar': array(1., dtype=float32), 'learning/lr_aggregate_scalar': array(0.01, dtype=float32), 'learning/raw_grad_norm_aggregate_scalar': array(0.643354, dtype=float32), 'learning/var_norm_aggregate_scalar': array(23.437784, dtype=float32), 'lr_aggregate_scalar': array(0.01, dtype=float32), 'vars/params/network/conv1/b_scalar': array(0.03436889, dtype=float32), 'vars/params/network/conv1/w_scalar': array(0.12798181, dtype=float32), 'vars/params/network/conv2/b_scalar': array(0.04317851, dtype=float32), 'vars/params/network/conv2/w_scalar': array(0.04979692, dtype=float32), 'vars/params/network/dense_0/bias/b_scalar': array(0.01038756, dtype=float32), 'vars/params/network/dense_0/linear/w_scalar': array(0.02435689, dtype=float32), 'vars/params/network/dense_1/w_scalar': array(0.09365918, dtype=float32), 'vars/params/network/dense_2/b_scalar': array(0.02964311, dtype=float32)}\n",
      "I0926 00:50:33.821483 138882247022144 summary_utils.py:340] summary tensor at step=500 loss 0.42806652188301086\n",
      "I0926 00:50:33.822232 138882247022144 summary_utils.py:340] summary tensor at step=500 Steps/sec 51.233960010114124\n",
      "I0926 00:50:33.822629 138882247022144 summary_utils.py:427] Metrics values at step 500:\n",
      "I0926 00:50:33.822685 138882247022144 summary_utils.py:428]   loss=0.428067\n",
      "I0926 00:50:33.822758 138882247022144 summary_utils.py:440]   accuracy=0.886719 (weight=1.000000)\n",
      "I0926 00:50:33.822826 138882247022144 summary_utils.py:340] summary tensor at step=500 Metrics/accuracy 0.88671875\n",
      "I0926 00:50:33.823220 138882247022144 summary_utils.py:340] summary tensor at step=500 Metrics/accuracy-weight 1.0\n",
      "I0926 00:50:33.823554 138882247022144 summary_utils.py:440]   loss=0.428067 (weight=1.000000)\n",
      "I0926 00:50:33.823636 138882247022144 summary_utils.py:340] summary tensor at step=500 Metrics/loss 0.42806652188301086\n",
      "I0926 00:50:33.823962 138882247022144 summary_utils.py:340] summary tensor at step=500 Metrics/loss-weight 1.0\n",
      "I0926 00:50:33.824266 138882247022144 local.py:45] Setting task status: step = 500, loss= 0.42806652188301086, steps/sec 51.233960010114124, accuracy=0.88671875, loss=0.42806652188301086\n",
      "I0926 00:50:33.824401 138882247022144 summary_utils.py:340] summary tensor at step=500 learning/applied_grad_norm_scalar 0.00627031410112977\n",
      "I0926 00:50:33.824782 138882247022144 summary_utils.py:340] summary tensor at step=500 learning/clipped_grad_norm_scalar 0.6433539986610413\n",
      "I0926 00:50:33.825163 138882247022144 summary_utils.py:340] summary tensor at step=500 learning/grad_scale_scalar 1.0\n",
      "I0926 00:50:33.825526 138882247022144 summary_utils.py:340] summary tensor at step=500 learning/is_valid_step_scalar 1.0\n",
      "I0926 00:50:33.825887 138882247022144 summary_utils.py:340] summary tensor at step=500 learning/lr_scalar 0.009999999776482582\n",
      "I0926 00:50:33.826328 138882247022144 summary_utils.py:340] summary tensor at step=500 learning/raw_grad_norm_scalar 0.6433539986610413\n",
      "I0926 00:50:33.826721 138882247022144 summary_utils.py:340] summary tensor at step=500 learning/var_norm_scalar 23.43778419494629\n",
      "I0926 00:50:33.827131 138882247022144 summary_utils.py:340] summary tensor at step=500 lr_scalar 0.009999999776482582\n",
      "I0926 00:50:33.827528 138882247022144 summary_utils.py:340] summary tensor at step=500 vars/params/network/conv1/b_scalar 0.03436889126896858\n",
      "I0926 00:50:33.828809 138882247022144 summary_utils.py:340] summary tensor at step=500 vars/params/network/conv1/w_scalar 0.12798181176185608\n",
      "I0926 00:50:33.831175 138882247022144 summary_utils.py:340] summary tensor at step=500 vars/params/network/conv2/b_scalar 0.04317851364612579\n",
      "I0926 00:50:33.831598 138882247022144 summary_utils.py:340] summary tensor at step=500 vars/params/network/conv2/w_scalar 0.04979692026972771\n",
      "I0926 00:50:33.832336 138882247022144 summary_utils.py:340] summary tensor at step=500 vars/params/network/dense_0/bias/b_scalar 0.010387563146650791\n",
      "I0926 00:50:33.832723 138882247022144 summary_utils.py:340] summary tensor at step=500 vars/params/network/dense_0/linear/w_scalar 0.024356886744499207\n",
      "I0926 00:50:33.833146 138882247022144 summary_utils.py:340] summary tensor at step=500 vars/params/network/dense_1/w_scalar 0.0936591774225235\n",
      "I0926 00:50:33.833557 138882247022144 summary_utils.py:340] summary tensor at step=500 vars/params/network/dense_2/b_scalar 0.02964310720562935\n",
      "I0926 00:50:33.834030 138882247022144 summary_utils.py:457] Wrote summary entry at step `500` (loss=`0.428067`).\n",
      "I0926 00:50:34.015341 140035128160896 programs.py:468] steps/sec: 50.163420\n",
      "I0926 00:50:34.210436 140035128160896 programs.py:468] steps/sec: 51.290284\n",
      "/home/awolant/Projects/DALI/dali/operators/reader/loader/lmdb.h:76: lmdb 0 /home/awolant/Projects/DALI_extra/db/MNIST/training/ rewind to the begin from 59999\n",
      "/home/awolant/Projects/DALI/dali/operators/reader/loader/lmdb.h:76: lmdb 0 /home/awolant/Projects/DALI_extra/db/MNIST/training/ rewind to the begin from 0\n",
      "W0926 00:50:34.363863 140035128160896 base_iterator.py:437] DALI iterator does not support resetting while epoch is not finished.                              Ignoring...\n",
      "I0926 00:50:34.400565 140035128160896 programs.py:468] steps/sec: 52.639093\n",
      "I0926 00:50:34.597830 140035128160896 programs.py:468] steps/sec: 50.726483\n",
      "I0926 00:50:34.806065 140035128160896 programs.py:468] steps/sec: 48.051765\n",
      "I0926 00:50:35.006313 140035128160896 programs.py:468] steps/sec: 49.970799\n",
      "I0926 00:50:35.208298 140035128160896 programs.py:468] steps/sec: 49.541052\n",
      "I0926 00:50:35.405689 140035128160896 programs.py:468] steps/sec: 50.694295\n",
      "/home/awolant/Projects/DALI/dali/operators/reader/loader/lmdb.h:76: lmdb 0 /home/awolant/Projects/DALI_extra/db/MNIST/training/ rewind to the begin from 59999\n",
      "/home/awolant/Projects/DALI/dali/operators/reader/loader/lmdb.h:76: lmdb 0 /home/awolant/Projects/DALI_extra/db/MNIST/training/ rewind to the begin from 0\n",
      "W0926 00:50:35.529241 140035128160896 base_iterator.py:437] DALI iterator does not support resetting while epoch is not finished.                              Ignoring...\n",
      "I0926 00:50:35.606041 140035128160896 programs.py:468] steps/sec: 49.943844\n",
      "I0926 00:50:35.805420 140035128160896 programs.py:468] steps/sec: 50.191754\n",
      "I0926 00:50:35.807644 138882247022144 summary_utils.py:668] [PAX STATUS] step_i: 600, training loss: 0.41082305\n",
      "I0926 00:50:35.807748 138882247022144 summary_utils.py:671] weighted_scalars: {'accuracy': (array(0.8886719, dtype=float32), array(1., dtype=float32)), 'loss': (array(0.41082305, dtype=float32), array(1., dtype=float32))}\n",
      "I0926 00:50:35.808222 138882247022144 summary_utils.py:673] per_example_out: {'predictions': array([[-0.4826229 , -3.1663191 ,  2.5024607 , ..., -0.87868565,\n",
      "         6.3207693 ,  0.6692344 ],\n",
      "       [-3.0323708 ,  5.475057  ,  0.3707452 , ..., -1.6931043 ,\n",
      "         1.7166712 , -0.08828005],\n",
      "       [-1.4032032 , -0.37122813, -3.967759  , ...,  8.223743  ,\n",
      "         1.4445562 ,  4.971711  ],\n",
      "       ...,\n",
      "       [-2.1459942 , -4.7045608 , -2.5921285 , ...,  1.8271922 ,\n",
      "         4.9824367 ,  4.1762843 ],\n",
      "       [11.812929  , -9.724945  ,  3.5454829 , ..., -3.1598368 ,\n",
      "         2.2946854 , -4.200377  ],\n",
      "       [-1.2046487 , -2.460038  ,  0.04660188, ..., -0.5510571 ,\n",
      "         5.5316176 ,  1.5533912 ]], dtype=float32)}\n",
      "I0926 00:50:35.808562 138882247022144 summary_utils.py:674] summary_tensors: {'learning/applied_grad_norm_aggregate_scalar': array(0.00547564, dtype=float32), 'learning/clipped_grad_norm_aggregate_scalar': array(0.53643054, dtype=float32), 'learning/grad_scale_aggregate_scalar': array(1., dtype=float32), 'learning/is_valid_step_aggregate_scalar': array(1., dtype=float32), 'learning/lr_aggregate_scalar': array(0.01, dtype=float32), 'learning/raw_grad_norm_aggregate_scalar': array(0.53643054, dtype=float32), 'learning/var_norm_aggregate_scalar': array(23.46956, dtype=float32), 'lr_aggregate_scalar': array(0.01, dtype=float32), 'vars/params/network/conv1/b_scalar': array(0.03710219, dtype=float32), 'vars/params/network/conv1/w_scalar': array(0.13267979, dtype=float32), 'vars/params/network/conv2/b_scalar': array(0.0451161, dtype=float32), 'vars/params/network/conv2/w_scalar': array(0.0499921, dtype=float32), 'vars/params/network/dense_0/bias/b_scalar': array(0.01067107, dtype=float32), 'vars/params/network/dense_0/linear/w_scalar': array(0.0243665, dtype=float32), 'vars/params/network/dense_1/w_scalar': array(0.0944325, dtype=float32), 'vars/params/network/dense_2/b_scalar': array(0.03173339, dtype=float32)}\n",
      "I0926 00:50:35.809817 138882247022144 summary_utils.py:340] summary tensor at step=600 loss 0.4108230471611023\n",
      "I0926 00:50:35.810627 138882247022144 summary_utils.py:340] summary tensor at step=600 Steps/sec 50.191753544520715\n",
      "I0926 00:50:35.811054 138882247022144 summary_utils.py:427] Metrics values at step 600:\n",
      "I0926 00:50:35.811114 138882247022144 summary_utils.py:428]   loss=0.410823\n",
      "I0926 00:50:35.811191 138882247022144 summary_utils.py:440]   accuracy=0.888672 (weight=1.000000)\n",
      "I0926 00:50:35.811270 138882247022144 summary_utils.py:340] summary tensor at step=600 Metrics/accuracy 0.888671875\n",
      "I0926 00:50:35.811653 138882247022144 summary_utils.py:340] summary tensor at step=600 Metrics/accuracy-weight 1.0\n",
      "I0926 00:50:35.811994 138882247022144 summary_utils.py:440]   loss=0.410823 (weight=1.000000)\n",
      "I0926 00:50:35.812096 138882247022144 summary_utils.py:340] summary tensor at step=600 Metrics/loss 0.4108230471611023\n",
      "I0926 00:50:35.812454 138882247022144 summary_utils.py:340] summary tensor at step=600 Metrics/loss-weight 1.0\n",
      "I0926 00:50:35.812786 138882247022144 local.py:45] Setting task status: step = 600, loss= 0.4108230471611023, steps/sec 50.191753544520715, accuracy=0.888671875, loss=0.4108230471611023\n",
      "I0926 00:50:35.812934 138882247022144 summary_utils.py:340] summary tensor at step=600 learning/applied_grad_norm_scalar 0.00547564122825861\n",
      "I0926 00:50:35.813347 138882247022144 summary_utils.py:340] summary tensor at step=600 learning/clipped_grad_norm_scalar 0.5364305377006531\n",
      "I0926 00:50:35.813737 138882247022144 summary_utils.py:340] summary tensor at step=600 learning/grad_scale_scalar 1.0\n",
      "I0926 00:50:35.815018 138882247022144 summary_utils.py:340] summary tensor at step=600 learning/is_valid_step_scalar 1.0\n",
      "I0926 00:50:35.817292 138882247022144 summary_utils.py:340] summary tensor at step=600 learning/lr_scalar 0.009999999776482582\n",
      "I0926 00:50:35.817763 138882247022144 summary_utils.py:340] summary tensor at step=600 learning/raw_grad_norm_scalar 0.5364305377006531\n",
      "I0926 00:50:35.818485 138882247022144 summary_utils.py:340] summary tensor at step=600 learning/var_norm_scalar 23.469560623168945\n",
      "I0926 00:50:35.818904 138882247022144 summary_utils.py:340] summary tensor at step=600 lr_scalar 0.009999999776482582\n",
      "I0926 00:50:35.819311 138882247022144 summary_utils.py:340] summary tensor at step=600 vars/params/network/conv1/b_scalar 0.03710218518972397\n",
      "I0926 00:50:35.819721 138882247022144 summary_utils.py:340] summary tensor at step=600 vars/params/network/conv1/w_scalar 0.1326797902584076\n",
      "I0926 00:50:35.820140 138882247022144 summary_utils.py:340] summary tensor at step=600 vars/params/network/conv2/b_scalar 0.04511610418558121\n",
      "I0926 00:50:35.820554 138882247022144 summary_utils.py:340] summary tensor at step=600 vars/params/network/conv2/w_scalar 0.04999210312962532\n",
      "I0926 00:50:35.820967 138882247022144 summary_utils.py:340] summary tensor at step=600 vars/params/network/dense_0/bias/b_scalar 0.010671066120266914\n",
      "I0926 00:50:35.821381 138882247022144 summary_utils.py:340] summary tensor at step=600 vars/params/network/dense_0/linear/w_scalar 0.024366499856114388\n",
      "I0926 00:50:35.821790 138882247022144 summary_utils.py:340] summary tensor at step=600 vars/params/network/dense_1/w_scalar 0.09443250298500061\n",
      "I0926 00:50:35.822210 138882247022144 summary_utils.py:340] summary tensor at step=600 vars/params/network/dense_2/b_scalar 0.03173339366912842\n",
      "I0926 00:50:35.822630 138882247022144 summary_utils.py:457] Wrote summary entry at step `600` (loss=`0.410823`).\n",
      "I0926 00:50:35.995270 140035128160896 programs.py:468] steps/sec: 52.733998\n",
      "I0926 00:50:36.183997 140035128160896 programs.py:468] steps/sec: 53.024531\n",
      "I0926 00:50:36.378863 140035128160896 programs.py:468] steps/sec: 51.349749\n",
      "I0926 00:50:36.578838 140035128160896 programs.py:468] steps/sec: 50.036732\n",
      "/home/awolant/Projects/DALI/dali/operators/reader/loader/lmdb.h:76: lmdb 0 /home/awolant/Projects/DALI_extra/db/MNIST/training/ rewind to the begin from 59999\n",
      "/home/awolant/Projects/DALI/dali/operators/reader/loader/lmdb.h:76: lmdb 0 /home/awolant/Projects/DALI_extra/db/MNIST/training/ rewind to the begin from 0\n",
      "W0926 00:50:36.690253 140035128160896 base_iterator.py:437] DALI iterator does not support resetting while epoch is not finished.                              Ignoring...\n",
      "I0926 00:50:36.785194 140035128160896 programs.py:468] steps/sec: 48.489848\n",
      "I0926 00:50:36.996761 140035128160896 programs.py:468] steps/sec: 47.296055\n",
      "I0926 00:50:37.193333 140035128160896 programs.py:468] steps/sec: 50.913673\n",
      "I0926 00:50:37.408707 140035128160896 programs.py:468] steps/sec: 46.459971\n",
      "I0926 00:50:37.603145 140035128160896 programs.py:468] steps/sec: 51.467579\n",
      "I0926 00:50:37.797602 140035128160896 programs.py:468] steps/sec: 51.463032\n",
      "I0926 00:50:37.801326 138882247022144 summary_utils.py:668] [PAX STATUS] step_i: 700, training loss: 0.35430932\n",
      "I0926 00:50:37.801428 138882247022144 summary_utils.py:671] weighted_scalars: {'accuracy': (array(0.8886719, dtype=float32), array(1., dtype=float32)), 'loss': (array(0.35430932, dtype=float32), array(1., dtype=float32))}\n",
      "I0926 00:50:37.801892 138882247022144 summary_utils.py:673] per_example_out: {'predictions': array([[-5.6862574 , -0.24883085,  2.4124243 , ...,  2.6842606 ,\n",
      "         3.3227627 ,  2.827505  ],\n",
      "       [-0.50578636, -9.723265  ,  1.1859548 , ...,  2.7834256 ,\n",
      "         1.7609115 ,  6.528214  ],\n",
      "       [-6.4275374 ,  5.7458277 ,  0.45652094, ...,  1.0383484 ,\n",
      "         2.2044582 ,  0.9476298 ],\n",
      "       ...,\n",
      "       [-1.6942388 , -4.39884   , -4.1833014 , ...,  5.3851624 ,\n",
      "         2.017068  ,  8.287466  ],\n",
      "       [-4.6940265 , -0.5534821 , -1.8953416 , ...,  0.7529336 ,\n",
      "         2.367961  ,  5.890933  ],\n",
      "       [ 4.8015876 , -5.869839  ,  0.89153415, ...,  0.2933407 ,\n",
      "         0.22508475, -0.17873901]], dtype=float32)}\n",
      "I0926 00:50:37.802258 138882247022144 summary_utils.py:674] summary_tensors: {'learning/applied_grad_norm_aggregate_scalar': array(0.00776398, dtype=float32), 'learning/clipped_grad_norm_aggregate_scalar': array(0.79535884, dtype=float32), 'learning/grad_scale_aggregate_scalar': array(1., dtype=float32), 'learning/is_valid_step_aggregate_scalar': array(1., dtype=float32), 'learning/lr_aggregate_scalar': array(0.01, dtype=float32), 'learning/raw_grad_norm_aggregate_scalar': array(0.79535884, dtype=float32), 'learning/var_norm_aggregate_scalar': array(23.492939, dtype=float32), 'lr_aggregate_scalar': array(0.01, dtype=float32), 'vars/params/network/conv1/b_scalar': array(0.03925599, dtype=float32), 'vars/params/network/conv1/w_scalar': array(0.13603875, dtype=float32), 'vars/params/network/conv2/b_scalar': array(0.04656878, dtype=float32), 'vars/params/network/conv2/w_scalar': array(0.05013585, dtype=float32), 'vars/params/network/dense_0/bias/b_scalar': array(0.01087178, dtype=float32), 'vars/params/network/dense_0/linear/w_scalar': array(0.02437367, dtype=float32), 'vars/params/network/dense_1/w_scalar': array(0.09500172, dtype=float32), 'vars/params/network/dense_2/b_scalar': array(0.0332779, dtype=float32)}\n",
      "I0926 00:50:37.803577 138882247022144 summary_utils.py:340] summary tensor at step=700 loss 0.3543093204498291\n",
      "/home/awolant/Projects/DALI/dali/operators/reader/loader/lmdb.h:76: lmdb 0 /home/awolant/Projects/DALI_extra/db/MNIST/training/ rewind to the begin from 59999\n",
      "/home/awolant/Projects/DALI/dali/operators/reader/loader/lmdb.h:76: lmdb 0 /home/awolant/Projects/DALI_extra/db/MNIST/training/ rewind to the begin from 0\n",
      "I0926 00:50:37.804298 138882247022144 summary_utils.py:340] summary tensor at step=700 Steps/sec 51.46303187801912\n",
      "I0926 00:50:37.804686 138882247022144 summary_utils.py:427] Metrics values at step 700:\n",
      "I0926 00:50:37.804738 138882247022144 summary_utils.py:428]   loss=0.354309\n",
      "I0926 00:50:37.804806 138882247022144 summary_utils.py:440]   accuracy=0.888672 (weight=1.000000)\n",
      "I0926 00:50:37.804869 138882247022144 summary_utils.py:340] summary tensor at step=700 Metrics/accuracy 0.888671875\n",
      "I0926 00:50:37.805214 138882247022144 summary_utils.py:340] summary tensor at step=700 Metrics/accuracy-weight 1.0\n",
      "I0926 00:50:37.805554 138882247022144 summary_utils.py:440]   loss=0.354309 (weight=1.000000)\n",
      "I0926 00:50:37.805635 138882247022144 summary_utils.py:340] summary tensor at step=700 Metrics/loss 0.3543093204498291\n",
      "I0926 00:50:37.805994 138882247022144 summary_utils.py:340] summary tensor at step=700 Metrics/loss-weight 1.0\n",
      "I0926 00:50:37.806332 138882247022144 local.py:45] Setting task status: step = 700, loss= 0.3543093204498291, steps/sec 51.46303187801912, accuracy=0.888671875, loss=0.3543093204498291\n",
      "I0926 00:50:37.806477 138882247022144 summary_utils.py:340] summary tensor at step=700 learning/applied_grad_norm_scalar 0.007763983681797981\n",
      "I0926 00:50:37.806889 138882247022144 summary_utils.py:340] summary tensor at step=700 learning/clipped_grad_norm_scalar 0.7953588366508484\n",
      "I0926 00:50:37.807299 138882247022144 summary_utils.py:340] summary tensor at step=700 learning/grad_scale_scalar 1.0\n",
      "I0926 00:50:37.807694 138882247022144 summary_utils.py:340] summary tensor at step=700 learning/is_valid_step_scalar 1.0\n",
      "I0926 00:50:37.808064 138882247022144 summary_utils.py:340] summary tensor at step=700 learning/lr_scalar 0.009999999776482582\n",
      "I0926 00:50:37.808473 138882247022144 summary_utils.py:340] summary tensor at step=700 learning/raw_grad_norm_scalar 0.7953588366508484\n",
      "I0926 00:50:37.808843 138882247022144 summary_utils.py:340] summary tensor at step=700 learning/var_norm_scalar 23.492938995361328\n",
      "I0926 00:50:37.809224 138882247022144 summary_utils.py:340] summary tensor at step=700 lr_scalar 0.009999999776482582\n",
      "I0926 00:50:37.809628 138882247022144 summary_utils.py:340] summary tensor at step=700 vars/params/network/conv1/b_scalar 0.03925599157810211\n",
      "I0926 00:50:37.810083 138882247022144 summary_utils.py:340] summary tensor at step=700 vars/params/network/conv1/w_scalar 0.13603875041007996\n",
      "I0926 00:50:37.810603 138882247022144 summary_utils.py:340] summary tensor at step=700 vars/params/network/conv2/b_scalar 0.04656877741217613\n",
      "I0926 00:50:37.811034 138882247022144 summary_utils.py:340] summary tensor at step=700 vars/params/network/conv2/w_scalar 0.05013584718108177\n",
      "I0926 00:50:37.811474 138882247022144 summary_utils.py:340] summary tensor at step=700 vars/params/network/dense_0/bias/b_scalar 0.010871781967580318\n",
      "I0926 00:50:37.811889 138882247022144 summary_utils.py:340] summary tensor at step=700 vars/params/network/dense_0/linear/w_scalar 0.024373674765229225\n",
      "I0926 00:50:37.812302 138882247022144 summary_utils.py:340] summary tensor at step=700 vars/params/network/dense_1/w_scalar 0.095001719892025\n",
      "I0926 00:50:37.812724 138882247022144 summary_utils.py:340] summary tensor at step=700 vars/params/network/dense_2/b_scalar 0.033277902752161026\n",
      "I0926 00:50:37.813156 138882247022144 summary_utils.py:457] Wrote summary entry at step `700` (loss=`0.354309`).\n",
      "W0926 00:50:37.879681 140035128160896 base_iterator.py:437] DALI iterator does not support resetting while epoch is not finished.                              Ignoring...\n",
      "I0926 00:50:38.007647 140035128160896 programs.py:468] steps/sec: 47.639808\n",
      "I0926 00:50:38.216345 140035128160896 programs.py:468] steps/sec: 47.948109\n",
      "I0926 00:50:38.426587 140035128160896 programs.py:468] steps/sec: 47.598666\n",
      "I0926 00:50:38.646141 140035128160896 programs.py:468] steps/sec: 45.578965\n",
      "I0926 00:50:38.855442 140035128160896 programs.py:468] steps/sec: 47.817572\n",
      "/home/awolant/Projects/DALI/dali/operators/reader/loader/lmdb.h:76: lmdb 0 /home/awolant/Projects/DALI_extra/db/MNIST/training/ rewind to the begin from 59999\n",
      "/home/awolant/Projects/DALI/dali/operators/reader/loader/lmdb.h:76: lmdb 0 /home/awolant/Projects/DALI_extra/db/MNIST/training/ rewind to the begin from 0\n",
      "I0926 00:50:39.061429 140035128160896 programs.py:468] steps/sec: 48.580327\n",
      "W0926 00:50:39.103019 140035128160896 base_iterator.py:437] DALI iterator does not support resetting while epoch is not finished.                              Ignoring...\n",
      "I0926 00:50:39.261448 140035128160896 programs.py:468] steps/sec: 50.027958\n",
      "I0926 00:50:39.476860 140035128160896 programs.py:468] steps/sec: 46.452253\n",
      "I0926 00:50:39.671608 140035128160896 programs.py:468] steps/sec: 51.382335\n",
      "I0926 00:50:39.866951 140035128160896 programs.py:468] steps/sec: 51.224699\n",
      "I0926 00:50:39.869974 138882247022144 summary_utils.py:668] [PAX STATUS] step_i: 800, training loss: 0.32842743\n",
      "I0926 00:50:39.870082 138882247022144 summary_utils.py:671] weighted_scalars: {'accuracy': (array(0.8984375, dtype=float32), array(1., dtype=float32)), 'loss': (array(0.32842743, dtype=float32), array(1., dtype=float32))}\n",
      "I0926 00:50:39.870555 138882247022144 summary_utils.py:673] per_example_out: {'predictions': array([[ -5.1227036 ,   4.4794197 ,   4.3232007 , ...,  -6.9539814 ,\n",
      "          1.5973779 ,  -4.499459  ],\n",
      "       [ -3.6570773 ,   0.89442295,   0.8176304 , ...,  -0.89617   ,\n",
      "          4.291526  ,   0.27196306],\n",
      "       [ -3.3476486 ,   6.4938765 ,   1.029463  , ...,  -1.1762917 ,\n",
      "          1.8055655 ,  -0.36110568],\n",
      "       ...,\n",
      "       [ -0.7327453 ,  -9.256657  ,  -2.6181035 , ...,  16.772306  ,\n",
      "          1.0730044 ,   8.349898  ],\n",
      "       [ 11.196064  , -10.476696  ,   2.477157  , ...,  -2.452709  ,\n",
      "          3.6062005 ,  -0.63657176],\n",
      "       [ -1.7667786 ,  -5.537358  ,  -3.7697797 , ...,   5.2894716 ,\n",
      "          2.620956  ,   7.458773  ]], dtype=float32)}\n",
      "I0926 00:50:39.870915 138882247022144 summary_utils.py:674] summary_tensors: {'learning/applied_grad_norm_aggregate_scalar': array(0.01143813, dtype=float32), 'learning/clipped_grad_norm_aggregate_scalar': array(1.1700824, dtype=float32), 'learning/grad_scale_aggregate_scalar': array(1., dtype=float32), 'learning/is_valid_step_aggregate_scalar': array(1., dtype=float32), 'learning/lr_aggregate_scalar': array(0.01, dtype=float32), 'learning/raw_grad_norm_aggregate_scalar': array(1.1700824, dtype=float32), 'learning/var_norm_aggregate_scalar': array(23.51159, dtype=float32), 'lr_aggregate_scalar': array(0.01, dtype=float32), 'vars/params/network/conv1/b_scalar': array(0.04107426, dtype=float32), 'vars/params/network/conv1/w_scalar': array(0.13863878, dtype=float32), 'vars/params/network/conv2/b_scalar': array(0.04773587, dtype=float32), 'vars/params/network/conv2/w_scalar': array(0.05024958, dtype=float32), 'vars/params/network/dense_0/bias/b_scalar': array(0.01103197, dtype=float32), 'vars/params/network/dense_0/linear/w_scalar': array(0.02437943, dtype=float32), 'vars/params/network/dense_1/w_scalar': array(0.09545237, dtype=float32), 'vars/params/network/dense_2/b_scalar': array(0.03449022, dtype=float32)}\n",
      "I0926 00:50:39.872211 138882247022144 summary_utils.py:340] summary tensor at step=800 loss 0.32842743396759033\n",
      "I0926 00:50:39.872958 138882247022144 summary_utils.py:340] summary tensor at step=800 Steps/sec 51.22469940950531\n",
      "I0926 00:50:39.873364 138882247022144 summary_utils.py:427] Metrics values at step 800:\n",
      "I0926 00:50:39.873421 138882247022144 summary_utils.py:428]   loss=0.328427\n",
      "I0926 00:50:39.873493 138882247022144 summary_utils.py:440]   accuracy=0.898438 (weight=1.000000)\n",
      "I0926 00:50:39.873563 138882247022144 summary_utils.py:340] summary tensor at step=800 Metrics/accuracy 0.8984375\n",
      "I0926 00:50:39.873937 138882247022144 summary_utils.py:340] summary tensor at step=800 Metrics/accuracy-weight 1.0\n",
      "I0926 00:50:39.874301 138882247022144 summary_utils.py:440]   loss=0.328427 (weight=1.000000)\n",
      "I0926 00:50:39.874388 138882247022144 summary_utils.py:340] summary tensor at step=800 Metrics/loss 0.32842743396759033\n",
      "I0926 00:50:39.874737 138882247022144 summary_utils.py:340] summary tensor at step=800 Metrics/loss-weight 1.0\n",
      "I0926 00:50:39.875059 138882247022144 local.py:45] Setting task status: step = 800, loss= 0.32842743396759033, steps/sec 51.22469940950531, accuracy=0.8984375, loss=0.32842743396759033\n",
      "I0926 00:50:39.875200 138882247022144 summary_utils.py:340] summary tensor at step=800 learning/applied_grad_norm_scalar 0.01143813319504261\n",
      "I0926 00:50:39.875601 138882247022144 summary_utils.py:340] summary tensor at step=800 learning/clipped_grad_norm_scalar 1.170082449913025\n",
      "I0926 00:50:39.875981 138882247022144 summary_utils.py:340] summary tensor at step=800 learning/grad_scale_scalar 1.0\n",
      "I0926 00:50:39.876343 138882247022144 summary_utils.py:340] summary tensor at step=800 learning/is_valid_step_scalar 1.0\n",
      "I0926 00:50:39.876706 138882247022144 summary_utils.py:340] summary tensor at step=800 learning/lr_scalar 0.009999999776482582\n",
      "I0926 00:50:39.877137 138882247022144 summary_utils.py:340] summary tensor at step=800 learning/raw_grad_norm_scalar 1.170082449913025\n",
      "I0926 00:50:39.877521 138882247022144 summary_utils.py:340] summary tensor at step=800 learning/var_norm_scalar 23.51158905029297\n",
      "I0926 00:50:39.877932 138882247022144 summary_utils.py:340] summary tensor at step=800 lr_scalar 0.009999999776482582\n",
      "I0926 00:50:39.878335 138882247022144 summary_utils.py:340] summary tensor at step=800 vars/params/network/conv1/b_scalar 0.04107425734400749\n",
      "I0926 00:50:39.878746 138882247022144 summary_utils.py:340] summary tensor at step=800 vars/params/network/conv1/w_scalar 0.13863877952098846\n",
      "I0926 00:50:39.879158 138882247022144 summary_utils.py:340] summary tensor at step=800 vars/params/network/conv2/b_scalar 0.047735873609781265\n",
      "I0926 00:50:39.880520 138882247022144 summary_utils.py:340] summary tensor at step=800 vars/params/network/conv2/w_scalar 0.05024958401918411\n",
      "I0926 00:50:39.883163 138882247022144 summary_utils.py:340] summary tensor at step=800 vars/params/network/dense_0/bias/b_scalar 0.011031965725123882\n",
      "I0926 00:50:39.883599 138882247022144 summary_utils.py:340] summary tensor at step=800 vars/params/network/dense_0/linear/w_scalar 0.024379432201385498\n",
      "I0926 00:50:39.884331 138882247022144 summary_utils.py:340] summary tensor at step=800 vars/params/network/dense_1/w_scalar 0.09545236825942993\n",
      "I0926 00:50:39.884758 138882247022144 summary_utils.py:340] summary tensor at step=800 vars/params/network/dense_2/b_scalar 0.03449022397398949\n",
      "I0926 00:50:39.885725 138882247022144 summary_utils.py:457] Wrote summary entry at step `800` (loss=`0.328427`).\n",
      "I0926 00:50:40.074570 140035128160896 programs.py:468] steps/sec: 48.195323\n",
      "/home/awolant/Projects/DALI/dali/operators/reader/loader/lmdb.h:76: lmdb 0 /home/awolant/Projects/DALI_extra/db/MNIST/training/ rewind to the begin from 59999\n",
      "/home/awolant/Projects/DALI/dali/operators/reader/loader/lmdb.h:76: lmdb 0 /home/awolant/Projects/DALI_extra/db/MNIST/training/ rewind to the begin from 0\n",
      "I0926 00:50:40.279148 140035128160896 programs.py:468] steps/sec: 48.914763\n",
      "W0926 00:50:40.293114 140035128160896 base_iterator.py:437] DALI iterator does not support resetting while epoch is not finished.                              Ignoring...\n",
      "I0926 00:50:40.472313 140035128160896 programs.py:468] steps/sec: 51.804683\n",
      "I0926 00:50:40.670682 140035128160896 programs.py:468] steps/sec: 50.442379\n",
      "I0926 00:50:40.861664 140035128160896 programs.py:468] steps/sec: 52.398409\n",
      "I0926 00:50:41.051518 140035128160896 programs.py:468] steps/sec: 52.706232\n",
      "I0926 00:50:41.234607 140035128160896 programs.py:468] steps/sec: 54.654182\n",
      "/home/awolant/Projects/DALI/dali/operators/reader/loader/lmdb.h:76: lmdb 0 /home/awolant/Projects/DALI_extra/db/MNIST/training/ rewind to the begin from 59999\n",
      "/home/awolant/Projects/DALI/dali/operators/reader/loader/lmdb.h:76: lmdb 0 /home/awolant/Projects/DALI_extra/db/MNIST/training/ rewind to the begin from 0\n",
      "W0926 00:50:41.409329 140035128160896 base_iterator.py:437] DALI iterator does not support resetting while epoch is not finished.                              Ignoring...\n",
      "I0926 00:50:41.422524 140035128160896 programs.py:468] steps/sec: 53.250049\n",
      "I0926 00:50:41.615924 140035128160896 programs.py:468] steps/sec: 51.735927\n",
      "I0926 00:50:41.816147 140035128160896 programs.py:468] steps/sec: 49.976098\n",
      "I0926 00:50:41.819653 138882247022144 summary_utils.py:668] [PAX STATUS] step_i: 900, training loss: 0.2689798\n",
      "I0926 00:50:41.819741 138882247022144 summary_utils.py:671] weighted_scalars: {'accuracy': (array(0.91503906, dtype=float32), array(1., dtype=float32)), 'loss': (array(0.2689798, dtype=float32), array(1., dtype=float32))}\n",
      "I0926 00:50:41.820205 138882247022144 summary_utils.py:673] per_example_out: {'predictions': array([[ 0.67493296, -4.4391713 ,  5.4769716 , ..., -0.17580348,\n",
      "         4.027602  , -2.000413  ],\n",
      "       [ 2.349295  , -7.4410086 , -0.22201   , ..., -5.387679  ,\n",
      "         4.5830717 ,  0.5190076 ],\n",
      "       [-1.7284732 ,  4.6684623 ,  0.37455094, ..., -0.10116927,\n",
      "         1.187861  , -0.0722672 ],\n",
      "       ...,\n",
      "       [-2.670627  ,  1.1159879 ,  9.250742  , ..., -5.817384  ,\n",
      "         2.6483598 , -0.3134672 ],\n",
      "       [-3.822823  , -5.5024095 ,  0.4226734 , ...,  0.50340104,\n",
      "         0.25449532,  3.5616798 ],\n",
      "       [ 0.88630104, -9.6627865 ,  3.797984  , ...,  4.3739285 ,\n",
      "         0.11789136,  6.6573043 ]], dtype=float32)}\n",
      "I0926 00:50:41.820544 138882247022144 summary_utils.py:674] summary_tensors: {'learning/applied_grad_norm_aggregate_scalar': array(0.0066374, dtype=float32), 'learning/clipped_grad_norm_aggregate_scalar': array(0.6474206, dtype=float32), 'learning/grad_scale_aggregate_scalar': array(1., dtype=float32), 'learning/is_valid_step_aggregate_scalar': array(1., dtype=float32), 'learning/lr_aggregate_scalar': array(0.01, dtype=float32), 'learning/raw_grad_norm_aggregate_scalar': array(0.6474206, dtype=float32), 'learning/var_norm_aggregate_scalar': array(23.527723, dtype=float32), 'lr_aggregate_scalar': array(0.01, dtype=float32), 'vars/params/network/conv1/b_scalar': array(0.04258195, dtype=float32), 'vars/params/network/conv1/w_scalar': array(0.14086826, dtype=float32), 'vars/params/network/conv2/b_scalar': array(0.04866899, dtype=float32), 'vars/params/network/conv2/w_scalar': array(0.05034859, dtype=float32), 'vars/params/network/dense_0/bias/b_scalar': array(0.01115804, dtype=float32), 'vars/params/network/dense_0/linear/w_scalar': array(0.02438446, dtype=float32), 'vars/params/network/dense_1/w_scalar': array(0.09584288, dtype=float32), 'vars/params/network/dense_2/b_scalar': array(0.03544205, dtype=float32)}\n",
      "I0926 00:50:41.821775 138882247022144 summary_utils.py:340] summary tensor at step=900 loss 0.2689797878265381\n",
      "I0926 00:50:41.822625 138882247022144 summary_utils.py:340] summary tensor at step=900 Steps/sec 49.97609804804697\n",
      "I0926 00:50:41.823030 138882247022144 summary_utils.py:427] Metrics values at step 900:\n",
      "I0926 00:50:41.823083 138882247022144 summary_utils.py:428]   loss=0.268980\n",
      "I0926 00:50:41.823152 138882247022144 summary_utils.py:440]   accuracy=0.915039 (weight=1.000000)\n",
      "I0926 00:50:41.823214 138882247022144 summary_utils.py:340] summary tensor at step=900 Metrics/accuracy 0.9150390625\n",
      "I0926 00:50:41.823566 138882247022144 summary_utils.py:340] summary tensor at step=900 Metrics/accuracy-weight 1.0\n",
      "I0926 00:50:41.823902 138882247022144 summary_utils.py:440]   loss=0.268980 (weight=1.000000)\n",
      "I0926 00:50:41.823984 138882247022144 summary_utils.py:340] summary tensor at step=900 Metrics/loss 0.2689797878265381\n",
      "I0926 00:50:41.824311 138882247022144 summary_utils.py:340] summary tensor at step=900 Metrics/loss-weight 1.0\n",
      "I0926 00:50:41.824612 138882247022144 local.py:45] Setting task status: step = 900, loss= 0.2689797878265381, steps/sec 49.97609804804697, accuracy=0.9150390625, loss=0.2689797878265381\n",
      "I0926 00:50:41.824767 138882247022144 summary_utils.py:340] summary tensor at step=900 learning/applied_grad_norm_scalar 0.006637397687882185\n",
      "I0926 00:50:41.825175 138882247022144 summary_utils.py:340] summary tensor at step=900 learning/clipped_grad_norm_scalar 0.6474205851554871\n",
      "I0926 00:50:41.825583 138882247022144 summary_utils.py:340] summary tensor at step=900 learning/grad_scale_scalar 1.0\n",
      "I0926 00:50:41.825973 138882247022144 summary_utils.py:340] summary tensor at step=900 learning/is_valid_step_scalar 1.0\n",
      "I0926 00:50:41.826369 138882247022144 summary_utils.py:340] summary tensor at step=900 learning/lr_scalar 0.009999999776482582\n",
      "I0926 00:50:41.826807 138882247022144 summary_utils.py:340] summary tensor at step=900 learning/raw_grad_norm_scalar 0.6474205851554871\n",
      "I0926 00:50:41.827199 138882247022144 summary_utils.py:340] summary tensor at step=900 learning/var_norm_scalar 23.52772331237793\n",
      "I0926 00:50:41.827607 138882247022144 summary_utils.py:340] summary tensor at step=900 lr_scalar 0.009999999776482582\n",
      "I0926 00:50:41.828000 138882247022144 summary_utils.py:340] summary tensor at step=900 vars/params/network/conv1/b_scalar 0.0425819531083107\n",
      "I0926 00:50:41.828382 138882247022144 summary_utils.py:340] summary tensor at step=900 vars/params/network/conv1/w_scalar 0.14086826145648956\n",
      "I0926 00:50:41.828763 138882247022144 summary_utils.py:340] summary tensor at step=900 vars/params/network/conv2/b_scalar 0.0486689917743206\n",
      "I0926 00:50:41.829144 138882247022144 summary_utils.py:340] summary tensor at step=900 vars/params/network/conv2/w_scalar 0.05034859478473663\n",
      "I0926 00:50:41.829525 138882247022144 summary_utils.py:340] summary tensor at step=900 vars/params/network/dense_0/bias/b_scalar 0.011158039793372154\n",
      "I0926 00:50:41.829906 138882247022144 summary_utils.py:340] summary tensor at step=900 vars/params/network/dense_0/linear/w_scalar 0.024384455755352974\n",
      "I0926 00:50:41.830337 138882247022144 summary_utils.py:340] summary tensor at step=900 vars/params/network/dense_1/w_scalar 0.0958428829908371\n",
      "I0926 00:50:41.830758 138882247022144 summary_utils.py:340] summary tensor at step=900 vars/params/network/dense_2/b_scalar 0.0354420505464077\n",
      "I0926 00:50:41.831183 138882247022144 summary_utils.py:457] Wrote summary entry at step `900` (loss=`0.268980`).\n",
      "I0926 00:50:42.008223 140035128160896 programs.py:468] steps/sec: 52.095907\n",
      "I0926 00:50:42.195478 140035128160896 programs.py:468] steps/sec: 53.437636\n",
      "I0926 00:50:42.385552 140035128160896 programs.py:468] steps/sec: 52.644577\n",
      "/home/awolant/Projects/DALI/dali/operators/reader/loader/lmdb.h:76: lmdb 0 /home/awolant/Projects/DALI_extra/db/MNIST/training/ rewind to the begin from 59999\n",
      "/home/awolant/Projects/DALI/dali/operators/reader/loader/lmdb.h:76: lmdb 0 /home/awolant/Projects/DALI_extra/db/MNIST/training/ rewind to the begin from 0\n",
      "W0926 00:50:42.537916 140035128160896 base_iterator.py:437] DALI iterator does not support resetting while epoch is not finished.                              Ignoring...\n",
      "I0926 00:50:42.576738 140035128160896 programs.py:468] steps/sec: 52.337275\n",
      "I0926 00:50:42.770940 140035128160896 programs.py:468] steps/sec: 51.525620\n",
      "I0926 00:50:42.968265 140035128160896 programs.py:468] steps/sec: 50.710292\n",
      "I0926 00:50:43.163108 140035128160896 programs.py:468] steps/sec: 51.358363\n",
      "I0926 00:50:43.356773 140035128160896 programs.py:468] steps/sec: 51.669453\n",
      "I0926 00:50:43.547961 140035128160896 programs.py:468] steps/sec: 52.336361\n",
      "/home/awolant/Projects/DALI/dali/operators/reader/loader/lmdb.h:76: lmdb 0 /home/awolant/Projects/DALI_extra/db/MNIST/training/ rewind to the begin from 59999\n",
      "/home/awolant/Projects/DALI/dali/operators/reader/loader/lmdb.h:76: lmdb 0 /home/awolant/Projects/DALI_extra/db/MNIST/training/ rewind to the begin from 0\n",
      "W0926 00:50:43.700330 140035128160896 base_iterator.py:437] DALI iterator does not support resetting while epoch is not finished.                              Ignoring...\n",
      "I0926 00:50:43.754006 140035128160896 programs.py:468] steps/sec: 48.562159\n",
      "I0926 00:50:43.754494 140035128160896 executors.py:396] Training loop completed (step (`1000`) greater than num_train_step (`1000`).\n",
      "I0926 00:50:43.754655 140035128160896 executors.py:511] [PAX STATUS]: Saving checkpoint for final step.\n",
      "I0926 00:50:43.754698 140035128160896 checkpoint_creators.py:229] Saving a ckpt at final step: 1000\n",
      "I0926 00:50:43.759665 140035128160896 checkpointer.py:67] Saving item to /tmp/dali_pax_logs/checkpoints/checkpoint_00001000.orbax-checkpoint-tmp-1695682243755565/state.\n",
      "I0926 00:50:43.762192 138882247022144 summary_utils.py:668] [PAX STATUS] step_i: 1000, training loss: 0.33137667\n",
      "I0926 00:50:43.762379 138882247022144 summary_utils.py:671] weighted_scalars: {'accuracy': (array(0.90527344, dtype=float32), array(1., dtype=float32)), 'loss': (array(0.33137667, dtype=float32), array(1., dtype=float32))}\n",
      "I0926 00:50:43.762891 138882247022144 summary_utils.py:673] per_example_out: {'predictions': array([[ -3.4132054 ,  -0.25462258,   7.5962143 , ...,  -2.491316  ,\n",
      "          0.86392784,  -0.71072197],\n",
      "       [ -1.2533212 ,  -8.281456  ,   5.9131236 , ..., -12.553749  ,\n",
      "          3.3294034 ,  -4.3831806 ],\n",
      "       [  0.73628974,  -6.7575264 ,  -5.606839  , ...,  10.965409  ,\n",
      "          0.85650253,   7.297507  ],\n",
      "       ...,\n",
      "       [ -0.3647306 ,  -2.3444297 ,  -3.5519211 , ...,   7.2430844 ,\n",
      "          2.1039762 ,   5.7749777 ],\n",
      "       [  0.7306855 ,  -5.4713464 ,   9.793458  , ...,  -5.273402  ,\n",
      "         -0.18203443,  -4.395619  ],\n",
      "       [  2.222125  ,  -6.332886  ,   3.2444696 , ...,  -9.349411  ,\n",
      "          0.9577956 ,  -4.2262263 ]], dtype=float32)}\n",
      "I0926 00:50:43.763981 138882247022144 summary_utils.py:674] summary_tensors: {'learning/applied_grad_norm_aggregate_scalar': array(0.01018554, dtype=float32), 'learning/clipped_grad_norm_aggregate_scalar': array(1.0569961, dtype=float32), 'learning/grad_scale_aggregate_scalar': array(1., dtype=float32), 'learning/is_valid_step_aggregate_scalar': array(1., dtype=float32), 'learning/lr_aggregate_scalar': array(0.01, dtype=float32), 'learning/raw_grad_norm_aggregate_scalar': array(1.0569961, dtype=float32), 'learning/var_norm_aggregate_scalar': array(23.542376, dtype=float32), 'lr_aggregate_scalar': array(0.01, dtype=float32), 'vars/params/network/conv1/b_scalar': array(0.04376674, dtype=float32), 'vars/params/network/conv1/w_scalar': array(0.14284575, dtype=float32), 'vars/params/network/conv2/b_scalar': array(0.04939055, dtype=float32), 'vars/params/network/conv2/w_scalar': array(0.05043739, dtype=float32), 'vars/params/network/dense_0/bias/b_scalar': array(0.01125301, dtype=float32), 'vars/params/network/dense_0/linear/w_scalar': array(0.02438898, dtype=float32), 'vars/params/network/dense_1/w_scalar': array(0.09619127, dtype=float32), 'vars/params/network/dense_2/b_scalar': array(0.03618956, dtype=float32)}\n",
      "I0926 00:50:43.765361 138882247022144 summary_utils.py:340] summary tensor at step=1000 loss 0.33137667179107666\n",
      "I0926 00:50:43.767469 138882247022144 summary_utils.py:340] summary tensor at step=1000 Steps/sec 48.56215945851444\n",
      "I0926 00:50:43.768216 138882247022144 summary_utils.py:427] Metrics values at step 1000:\n",
      "I0926 00:50:43.768372 138882247022144 summary_utils.py:428]   loss=0.331377\n",
      "I0926 00:50:43.768463 138882247022144 summary_utils.py:440]   accuracy=0.905273 (weight=1.000000)\n",
      "I0926 00:50:43.768532 138882247022144 summary_utils.py:340] summary tensor at step=1000 Metrics/accuracy 0.9052734375\n",
      "I0926 00:50:43.769541 138882247022144 summary_utils.py:340] summary tensor at step=1000 Metrics/accuracy-weight 1.0\n",
      "I0926 00:50:43.771852 138882247022144 summary_utils.py:440]   loss=0.331377 (weight=1.000000)\n",
      "I0926 00:50:43.771970 138882247022144 summary_utils.py:340] summary tensor at step=1000 Metrics/loss 0.33137667179107666\n",
      "I0926 00:50:43.772727 138882247022144 summary_utils.py:340] summary tensor at step=1000 Metrics/loss-weight 1.0\n",
      "I0926 00:50:43.774258 138882247022144 local.py:45] Setting task status: step = 1000, loss= 0.33137667179107666, steps/sec 48.56215945851444, accuracy=0.9052734375, loss=0.33137667179107666\n",
      "I0926 00:50:43.775866 138882247022144 summary_utils.py:340] summary tensor at step=1000 learning/applied_grad_norm_scalar 0.010185536928474903\n",
      "I0926 00:50:43.779299 138882247022144 summary_utils.py:340] summary tensor at step=1000 learning/clipped_grad_norm_scalar 1.0569961071014404\n",
      "I0926 00:50:43.780007 138882247022144 summary_utils.py:340] summary tensor at step=1000 learning/grad_scale_scalar 1.0\n",
      "I0926 00:50:43.780850 138882247022144 summary_utils.py:340] summary tensor at step=1000 learning/is_valid_step_scalar 1.0\n",
      "I0926 00:50:43.781515 138882247022144 summary_utils.py:340] summary tensor at step=1000 learning/lr_scalar 0.009999999776482582\n",
      "I0926 00:50:43.781987 138882247022144 summary_utils.py:340] summary tensor at step=1000 learning/raw_grad_norm_scalar 1.0569961071014404\n",
      "I0926 00:50:43.782460 138882247022144 summary_utils.py:340] summary tensor at step=1000 learning/var_norm_scalar 23.542375564575195\n",
      "I0926 00:50:43.782885 138882247022144 summary_utils.py:340] summary tensor at step=1000 lr_scalar 0.009999999776482582\n",
      "I0926 00:50:43.783301 138882247022144 summary_utils.py:340] summary tensor at step=1000 vars/params/network/conv1/b_scalar 0.04376673698425293\n",
      "I0926 00:50:43.783786 138882247022144 summary_utils.py:340] summary tensor at step=1000 vars/params/network/conv1/w_scalar 0.1428457498550415\n",
      "I0926 00:50:43.784776 138882247022144 summary_utils.py:340] summary tensor at step=1000 vars/params/network/conv2/b_scalar 0.049390554428100586\n",
      "I0926 00:50:43.785515 138882247022144 summary_utils.py:340] summary tensor at step=1000 vars/params/network/conv2/w_scalar 0.05043738707900047\n",
      "I0926 00:50:43.786180 138882247022144 summary_utils.py:340] summary tensor at step=1000 vars/params/network/dense_0/bias/b_scalar 0.011253008618950844\n",
      "I0926 00:50:43.786837 138882247022144 summary_utils.py:340] summary tensor at step=1000 vars/params/network/dense_0/linear/w_scalar 0.024388983845710754\n",
      "I0926 00:50:43.787336 138882247022144 summary_utils.py:340] summary tensor at step=1000 vars/params/network/dense_1/w_scalar 0.09619127213954926\n",
      "I0926 00:50:43.787764 138882247022144 summary_utils.py:340] summary tensor at step=1000 vars/params/network/dense_2/b_scalar 0.03618956357240677\n",
      "I0926 00:50:43.788219 138882247022144 summary_utils.py:457] Wrote summary entry at step `1000` (loss=`0.331377`).\n",
      "I0926 00:50:43.827108 140035128160896 utils.py:518] Renaming /tmp/dali_pax_logs/checkpoints/checkpoint_00001000.orbax-checkpoint-tmp-1695682243755565/state.orbax-checkpoint-tmp-1695682243759794 to /tmp/dali_pax_logs/checkpoints/checkpoint_00001000.orbax-checkpoint-tmp-1695682243755565/state\n",
      "I0926 00:50:43.827319 140035128160896 utils.py:562] Finished saving checkpoint to `/tmp/dali_pax_logs/checkpoints/checkpoint_00001000.orbax-checkpoint-tmp-1695682243755565/state`.\n",
      "I0926 00:50:43.828403 140035128160896 checkpointer.py:67] Saving item to /tmp/dali_pax_logs/checkpoints/checkpoint_00001000.orbax-checkpoint-tmp-1695682243755565/metadata.\n",
      "I0926 00:50:43.833500 140035128160896 utils.py:518] Renaming /tmp/dali_pax_logs/checkpoints/checkpoint_00001000.orbax-checkpoint-tmp-1695682243755565/metadata.orbax-checkpoint-tmp-1695682243828507 to /tmp/dali_pax_logs/checkpoints/checkpoint_00001000.orbax-checkpoint-tmp-1695682243755565/metadata\n",
      "I0926 00:50:43.833626 140035128160896 utils.py:562] Finished saving checkpoint to `/tmp/dali_pax_logs/checkpoints/checkpoint_00001000.orbax-checkpoint-tmp-1695682243755565/metadata`.\n",
      "I0926 00:50:43.834805 140035128160896 utils.py:518] Renaming /tmp/dali_pax_logs/checkpoints/checkpoint_00001000.orbax-checkpoint-tmp-1695682243755565 to /tmp/dali_pax_logs/checkpoints/checkpoint_00001000\n",
      "I0926 00:50:43.835899 140035128160896 executors.py:521] [PAX STATUS]: Final checkpoint saved.\n",
      "I0926 00:50:43.836101 140035128160896 executors.py:290] [PAX STATUS]: Shutting down executor.\n",
      "I0926 00:50:43.836503 140035128160896 summary_utils.py:294] Closed SummaryWriter `/tmp/dali_pax_logs/summaries/train`.\n",
      "I0926 00:50:43.836570 140035128160896 executors.py:296] [PAX STATUS]: Executor shutdown complete.\n",
      "I0926 00:50:43.871587 140035128160896 py_utils.py:1025] [PAX STATUS]: Elapsed time for <train_and_evaluate>: 24.73 seconds  (@ <.../paxml/main.py:285>)\n",
      "I0926 00:50:43.871711 140035128160896 py_utils.py:339] Starting sync_global_devices All tasks finish. across 1 devices globally\n",
      "I0926 00:50:43.873192 140035128160896 py_utils.py:342] Finished sync_global_devices All tasks finish. across 1 devices globally\n",
      "I0926 00:50:43.873272 140035128160896 py_utils.py:1025] [PAX STATUS]: Elapsed time for <run_experiment>: 24.75 seconds  (@ <.../paxml/main.py:420>)\n",
      "I0926 00:50:43.873305 140035128160896 py_utils.py:1025] [PAX STATUS]: Elapsed time for <run>: 24.75 seconds  (@ <.../paxml/main.py:487>)\n",
      "I0926 00:50:43.873331 140035128160896 py_utils.py:1025] [PAX STATUS]: E2E time: Elapsed time for <_main>: 25.33 seconds  (@ <.../paxml/main.py:445>)\n"
     ]
    }
   ],
   "source": [
    "!python -m paxml.main --job_log_dir=/tmp/dali_pax_logs --exp pax_examples.dali_pax_example.MnistExperiment"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next we create a helper function to print training accuracy from the logs. Logs created in this example are comaptible with TensorBoard and can be visualized using this tool."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "from tensorflow.core.util import event_pb2\n",
    "from tensorflow.python.lib.io import tf_record\n",
    "from tensorflow.python.framework import tensor_util\n",
    "\n",
    "def print_logs(path):\n",
    "    \"Helper function to print logs from logs directory created by paxml example\"\n",
    "    def summary_iterator():\n",
    "        for r in tf_record.tf_record_iterator(path):\n",
    "            yield event_pb2.Event.FromString(r)\n",
    "            \n",
    "    for summary in summary_iterator():\n",
    "        for value in summary.summary.value:\n",
    "            if value.tag == 'Metrics/accuracy':\n",
    "                t = tensor_util.MakeNdarray(value.tensor)\n",
    "                print(f\"Iteration: {summary.step}, accuracy: {t}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "With this helper function we can print the accuracy of the training inside Python code."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration: 100, accuracy: 0.4091796875\n",
      "Iteration: 200, accuracy: 0.5380859375\n",
      "Iteration: 300, accuracy: 0.74609375\n",
      "Iteration: 400, accuracy: 0.8232421875\n",
      "Iteration: 500, accuracy: 0.88671875\n",
      "Iteration: 600, accuracy: 0.888671875\n",
      "Iteration: 700, accuracy: 0.888671875\n",
      "Iteration: 800, accuracy: 0.8984375\n",
      "Iteration: 900, accuracy: 0.9150390625\n",
      "Iteration: 1000, accuracy: 0.9052734375\n"
     ]
    }
   ],
   "source": [
    "for file in os.listdir('/tmp/dali_pax_logs/summaries/train/'):\n",
    "    print_logs(os.path.join('/tmp/dali_pax_logs/summaries/train/', file))"
   ]
  }
 ],
 "metadata": {
  "celltoolbar": "Raw Cell Format",
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
