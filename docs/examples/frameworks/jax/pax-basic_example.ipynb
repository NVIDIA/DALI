{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Training neural network with DALI and Pax\n",
    "\n",
    "This simple example shows how to train a neural network implemented in JAX with DALI pipelines. It builds on MNIST training example from Pax codebse that can be found [here](https://github.com/google/paxml/blob/paxml-v1.1.0/paxml/tasks/vision/params/mnist.py).\n",
    "\n",
    "We will use MNIST in Caffe2 format from [DALI_extra](https://github.com/NVIDIA/DALI_extra).\n",
    "\n",
    "This example focuses on how to use DALI pipeline with Pax. For more information on DALI pipeline look into [Getting started](../../getting_started.ipynb) and [pipeline documentation](../../../pipeline.rst)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "I0925 23:48:12.906274 139756425134720 py_utils.py:1015] [PAX STATUS]: E2E time: Starting timer for <_main> @ <.../paxml/main.py:445>\n",
      "I0925 23:48:12.906384 139756425134720 main.py:450] [PAX STATUS]: Program start.\n",
      "I0925 23:48:12.906458 139756425134720 py_utils.py:1015] [PAX STATUS]: Starting timer for <setup_jax> @ <.../paxml/main.py:465>\n",
      "I0925 23:48:12.972933 139756425134720 xla_bridge.py:622] Unable to initialize backend 'rocm': NOT_FOUND: Could not find registered platform with name: \"rocm\". Available platform names are: CUDA Interpreter\n",
      "I0925 23:48:12.973237 139756425134720 xla_bridge.py:622] Unable to initialize backend 'tpu': module 'jaxlib.xla_extension' has no attribute 'get_tpu_client'\n",
      "I0925 23:48:12.973349 139756425134720 setup_jax.py:78] JAX process: 0 / 1\n",
      "I0925 23:48:12.973390 139756425134720 setup_jax.py:79] JAX devices: [gpu(id=0)]\n",
      "I0925 23:48:12.973586 139756425134720 setup_jax.py:80] jax.device_count(): 1\n",
      "I0925 23:48:12.973684 139756425134720 setup_jax.py:81] jax.local_device_count(): 1\n",
      "I0925 23:48:12.973711 139756425134720 setup_jax.py:82] jax.process_count(): 1\n",
      "I0925 23:48:12.973740 139756425134720 py_utils.py:1025] [PAX STATUS]: Elapsed time for <setup_jax>: 0.07 seconds  (@ <.../paxml/main.py:465>)\n",
      "I0925 23:48:12.973786 139756425134720 py_utils.py:1015] [PAX STATUS]: Starting timer for <get_experiment> @ <.../paxml/main.py:475>\n",
      "Registered experiment `dali_pax_example.MnistExperiment`\n",
      "I0925 23:48:13.475062 139756425134720 py_utils.py:1025] [PAX STATUS]: Elapsed time for <get_experiment>: 0.50 seconds  (@ <.../paxml/main.py:475>)\n",
      "I0925 23:48:13.475165 139756425134720 py_utils.py:1015] [PAX STATUS]: Starting timer for <run> @ <.../paxml/main.py:487>\n",
      "I0925 23:48:13.475203 139756425134720 py_utils.py:1015] [PAX STATUS]: Starting timer for <_setup_xm_work_unit> @ <.../paxml/main.py:404>\n",
      "I0925 23:48:13.475244 139756425134720 local.py:45] Setting task status: process_index: 0, process_count: 1\n",
      "I0925 23:48:13.475436 139756425134720 local.py:50] Created artifact job_log_dir of type ArtifactType.DIRECTORY and value /tmp/dali_pax_logs.\n",
      "I0925 23:48:13.475465 139756425134720 py_utils.py:1025] [PAX STATUS]: Elapsed time for <_setup_xm_work_unit>: 0.00 seconds  (@ <.../paxml/main.py:404>)\n",
      "I0925 23:48:13.475502 139756425134720 py_utils.py:1015] [PAX STATUS]: Starting timer for <get_search_space> @ <.../paxml/main.py:416>\n",
      "I0925 23:48:13.481298 139756425134720 py_utils.py:1025] [PAX STATUS]: Elapsed time for <get_search_space>: 0.01 seconds  (@ <.../paxml/main.py:416>)\n",
      "I0925 23:48:13.482688 139756425134720 py_utils.py:1015] [PAX STATUS]: Starting timer for <run_experiment> @ <.../paxml/main.py:420>\n",
      "I0925 23:48:13.483063 139756425134720 py_utils.py:1015] [PAX STATUS]: Starting timer for <write_hparams_file> @ <.../paxml/main.py:276>\n",
      "I0925 23:48:13.489831 139756425134720 py_utils.py:1025] [PAX STATUS]: Elapsed time for <write_hparams_file>: 0.01 seconds  (@ <.../paxml/main.py:276>)\n",
      "I0925 23:48:13.494071 139756425134720 local.py:45] Setting task status: Train experiment dali_pax_example.MnistExperiment at /tmp/dali_pax_logs\n",
      "I0925 23:48:13.494144 139756425134720 py_utils.py:1015] [PAX STATUS]: Starting timer for <train_and_evaluate> @ <.../paxml/main.py:285>\n",
      "I0925 23:48:13.498686 139756425134720 train.py:149] [PAX STATUS]: Getting dataset configurations.\n",
      "I0925 23:48:13.499943 139756425134720 train.py:158] [PAX STATUS]: Done getting dataset configurations.\n",
      "I0925 23:48:13.499989 139756425134720 train.py:160] train_input_p:\n",
      "I0925 23:48:13.500139 139756425134720 train.py:164]   batch_padding_size : 0\n",
      "I0925 23:48:13.500173 139756425134720 train.py:164]   batch_size : 1024\n",
      "I0925 23:48:13.500199 139756425134720 train.py:164]   cls : type/dali_pax_input/MnistDaliInput\n",
      "I0925 23:48:13.500222 139756425134720 train.py:164]   custom_device_order : NoneType\n",
      "I0925 23:48:13.500245 139756425134720 train.py:164]   eval_loop_num_batches : 1\n",
      "I0925 23:48:13.500267 139756425134720 train.py:164]   experimental_remote_input : False\n",
      "I0925 23:48:13.500288 139756425134720 train.py:164]   infeed_host_index : 0\n",
      "I0925 23:48:13.500310 139756425134720 train.py:164]   input_checkpointing_enabled : False\n",
      "I0925 23:48:13.500331 139756425134720 train.py:164]   input_random_seed : NoneType\n",
      "I0925 23:48:13.500353 139756425134720 train.py:164]   is_training : True\n",
      "I0925 23:48:13.500375 139756425134720 train.py:164]   name : ''\n",
      "I0925 23:48:13.500396 139756425134720 train.py:164]   num_infeed_hosts : 0\n",
      "I0925 23:48:13.500417 139756425134720 train.py:164]   reset_for_eval : False\n",
      "I0925 23:48:13.500439 139756425134720 train.py:164]   tf_data_service_address : NoneType\n",
      "I0925 23:48:13.500461 139756425134720 train.py:165] task_p:\n",
      "I0925 23:48:13.501570 139756425134720 train.py:167]   cls : type/paxml.tasks_lib/SingleTask\n",
      "I0925 23:48:13.501612 139756425134720 train.py:167]   decode.cls : type/paxml.tasks_lib/SingleTask.Decode\n",
      "I0925 23:48:13.501639 139756425134720 train.py:167]   decode.prng_key_fold_with_batch_index : False\n",
      "I0925 23:48:13.501664 139756425134720 train.py:167]   decode.profiler_capture_step : 1\n",
      "I0925 23:48:13.501686 139756425134720 train.py:167]   decode.profiler_max_num_hosts : NoneType\n",
      "I0925 23:48:13.501708 139756425134720 train.py:167]   decode.profiler_min_duration_sec : 1.0\n",
      "I0925 23:48:13.501730 139756425134720 train.py:167]   decode.profiler_num_steps : 0\n",
      "I0925 23:48:13.501752 139756425134720 train.py:167]   decode.random_seed : 1234\n",
      "I0925 23:48:13.501773 139756425134720 train.py:167]   early_stopping_fn : NoneType\n",
      "I0925 23:48:13.501795 139756425134720 train.py:167]   evaluate.apply_mutable_list : ['aux_loss', 'summaries', 'non_trainable']\n",
      "I0925 23:48:13.501816 139756425134720 train.py:167]   evaluate.cls : type/paxml.tasks_lib/SingleTask.Evaluate\n",
      "I0925 23:48:13.501837 139756425134720 train.py:167]   evaluate.random_seed : 1234\n",
      "I0925 23:48:13.501859 139756425134720 train.py:167]   infer.cls : type/paxml.tasks_lib/SingleTask.Infer\n",
      "I0925 23:48:13.501880 139756425134720 train.py:167]   infer.random_seed : 1234\n",
      "I0925 23:48:13.501902 139756425134720 train.py:167]   infer_writer : NoneType\n",
      "I0925 23:48:13.501923 139756425134720 train.py:167]   loss_aggregator : NoneType\n",
      "I0925 23:48:13.501944 139756425134720 train.py:167]   metrics : NoneType\n",
      "I0925 23:48:13.501965 139756425134720 train.py:167]   model.activation_split_dims_mapping.out : NoneType\n",
      "I0925 23:48:13.501987 139756425134720 train.py:167]   model.cls : type/dali_pax_example/CNNModel\n",
      "I0925 23:48:13.502013 139756425134720 train.py:167]   model.contiguous_submeshes : NoneType\n",
      "I0925 23:48:13.502034 139756425134720 train.py:167]   model.dcn_mesh_shape : NoneType\n",
      "I0925 23:48:13.502073 139756425134720 train.py:167]   model.dtype : type/jax.numpy/float32\n",
      "I0925 23:48:13.502096 139756425134720 train.py:167]   model.fprop_dtype : Float64DType\n",
      "I0925 23:48:13.502120 139756425134720 train.py:167]   model.ici_mesh_shape : [1, 1, 1]\n",
      "I0925 23:48:13.502144 139756425134720 train.py:167]   model.mesh_axis_names : ('replica', 'data', 'mdl')\n",
      "I0925 23:48:13.502167 139756425134720 train.py:167]   model.name : NoneType\n",
      "I0925 23:48:13.502191 139756425134720 train.py:167]   model.network_tpl.activation_split_dims_mapping.out : NoneType\n",
      "I0925 23:48:13.502214 139756425134720 train.py:167]   model.network_tpl.activation_tpl.activation_split_dims_mapping.out : NoneType\n",
      "I0925 23:48:13.502238 139756425134720 train.py:167]   model.network_tpl.activation_tpl.cls : type/praxis.layers.activations/ReLU\n",
      "I0925 23:48:13.502262 139756425134720 train.py:167]   model.network_tpl.activation_tpl.contiguous_submeshes : NoneType\n",
      "I0925 23:48:13.502286 139756425134720 train.py:167]   model.network_tpl.activation_tpl.dcn_mesh_shape : NoneType\n",
      "I0925 23:48:13.502310 139756425134720 train.py:167]   model.network_tpl.activation_tpl.dtype : type/jax.numpy/float32\n",
      "I0925 23:48:13.502333 139756425134720 train.py:167]   model.network_tpl.activation_tpl.fprop_dtype : NoneType\n",
      "I0925 23:48:13.502357 139756425134720 train.py:167]   model.network_tpl.activation_tpl.ici_mesh_shape : NoneType\n",
      "I0925 23:48:13.502381 139756425134720 train.py:167]   model.network_tpl.activation_tpl.mesh_axis_names : NoneType\n",
      "I0925 23:48:13.502405 139756425134720 train.py:167]   model.network_tpl.activation_tpl.name : NoneType\n",
      "I0925 23:48:13.502429 139756425134720 train.py:167]   model.network_tpl.activation_tpl.params_init.cls : type/praxis.base_layer/WeightInit\n",
      "I0925 23:48:13.502453 139756425134720 train.py:167]   model.network_tpl.activation_tpl.params_init.method : 'xavier'\n",
      "I0925 23:48:13.502476 139756425134720 train.py:167]   model.network_tpl.activation_tpl.params_init.scale : 1.000001\n",
      "I0925 23:48:13.502500 139756425134720 train.py:167]   model.network_tpl.activation_tpl.shared_weight_layer_id : NoneType\n",
      "I0925 23:48:13.502524 139756425134720 train.py:167]   model.network_tpl.activation_tpl.skip_lp_regularization : NoneType\n",
      "I0925 23:48:13.502548 139756425134720 train.py:167]   model.network_tpl.activation_tpl.weight_split_dims_mapping.wt : NoneType\n",
      "I0925 23:48:13.502572 139756425134720 train.py:167]   model.network_tpl.cls : type/dali_pax_example/CNN\n",
      "I0925 23:48:13.502595 139756425134720 train.py:167]   model.network_tpl.contiguous_submeshes : NoneType\n",
      "I0925 23:48:13.502619 139756425134720 train.py:167]   model.network_tpl.dcn_mesh_shape : NoneType\n",
      "I0925 23:48:13.502642 139756425134720 train.py:167]   model.network_tpl.dtype : type/jax.numpy/float32\n",
      "I0925 23:48:13.502665 139756425134720 train.py:167]   model.network_tpl.fprop_dtype : NoneType\n",
      "I0925 23:48:13.502688 139756425134720 train.py:167]   model.network_tpl.height : 28\n",
      "I0925 23:48:13.502711 139756425134720 train.py:167]   model.network_tpl.ici_mesh_shape : NoneType\n",
      "I0925 23:48:13.502734 139756425134720 train.py:167]   model.network_tpl.kernel_size : 3\n",
      "I0925 23:48:13.502757 139756425134720 train.py:167]   model.network_tpl.mesh_axis_names : NoneType\n",
      "I0925 23:48:13.502780 139756425134720 train.py:167]   model.network_tpl.name : 'mnist_model'\n",
      "I0925 23:48:13.502803 139756425134720 train.py:167]   model.network_tpl.num_classes : 10\n",
      "I0925 23:48:13.502826 139756425134720 train.py:167]   model.network_tpl.params_init.cls : type/praxis.base_layer/WeightInit\n",
      "I0925 23:48:13.502850 139756425134720 train.py:167]   model.network_tpl.params_init.method : 'xavier'\n",
      "I0925 23:48:13.502873 139756425134720 train.py:167]   model.network_tpl.params_init.scale : 1.000001\n",
      "I0925 23:48:13.502896 139756425134720 train.py:167]   model.network_tpl.shared_weight_layer_id : NoneType\n",
      "I0925 23:48:13.502919 139756425134720 train.py:167]   model.network_tpl.skip_lp_regularization : NoneType\n",
      "I0925 23:48:13.502954 139756425134720 train.py:167]   model.network_tpl.weight_split_dims_mapping.wt : NoneType\n",
      "I0925 23:48:13.502976 139756425134720 train.py:167]   model.network_tpl.width : 28\n",
      "I0925 23:48:13.502999 139756425134720 train.py:167]   model.params_init.cls : type/praxis.base_layer/WeightInit\n",
      "I0925 23:48:13.503022 139756425134720 train.py:167]   model.params_init.method : 'xavier'\n",
      "I0925 23:48:13.503044 139756425134720 train.py:167]   model.params_init.scale : 1.000001\n",
      "I0925 23:48:13.503067 139756425134720 train.py:167]   model.shared_weight_layer_id : NoneType\n",
      "I0925 23:48:13.503089 139756425134720 train.py:167]   model.skip_lp_regularization : NoneType\n",
      "I0925 23:48:13.503113 139756425134720 train.py:167]   model.weight_split_dims_mapping.wt : NoneType\n",
      "I0925 23:48:13.503136 139756425134720 train.py:167]   name : 'mnist_task'\n",
      "I0925 23:48:13.503159 139756425134720 train.py:167]   summary_verbosity : 3\n",
      "I0925 23:48:13.503182 139756425134720 train.py:167]   train.always_use_train_for_model_init : True\n",
      "I0925 23:48:13.503204 139756425134720 train.py:167]   train.apply_mutable_list : ['aux_loss', 'summaries', 'non_trainable', 'batch_stats', 'params_axes']\n",
      "I0925 23:48:13.503227 139756425134720 train.py:167]   train.async_summary_writing : True\n",
      "I0925 23:48:13.503251 139756425134720 train.py:167]   train.cls : type/paxml.tasks_lib/SingleTask.Train\n",
      "I0925 23:48:13.503274 139756425134720 train.py:167]   train.decode_interval_steps : NoneType\n",
      "I0925 23:48:13.503297 139756425134720 train.py:167]   train.decode_start_after_n_steps : 0\n",
      "I0925 23:48:13.503319 139756425134720 train.py:167]   train.decode_use_ema_states : False\n",
      "I0925 23:48:13.503353 139756425134720 train.py:167]   train.enable_input_checkpointing : False\n",
      "I0925 23:48:13.503373 139756425134720 train.py:167]   train.enforce_input_specs : True\n",
      "I0925 23:48:13.503393 139756425134720 train.py:167]   train.eval_interval_steps : 100\n",
      "I0925 23:48:13.503413 139756425134720 train.py:167]   train.eval_skip_train : True\n",
      "I0925 23:48:13.503449 139756425134720 train.py:167]   train.eval_use_ema_states : False\n",
      "I0925 23:48:13.503472 139756425134720 train.py:167]   train.external_checkpoint_handler : NoneType\n",
      "I0925 23:48:13.503494 139756425134720 train.py:167]   train.external_checkpoint_path : NoneType\n",
      "I0925 23:48:13.503517 139756425134720 train.py:167]   train.inputs_split_mapping : NoneType\n",
      "I0925 23:48:13.503539 139756425134720 train.py:167]   train.learner.check_valid_step : True\n",
      "I0925 23:48:13.503561 139756425134720 train.py:167]   train.learner.cls : type/paxml.learners/Learner\n",
      "I0925 23:48:13.503584 139756425134720 train.py:167]   train.learner.enable_skip_step_on_gradient_anomalies : True\n",
      "I0925 23:48:13.503607 139756425134720 train.py:167]   train.learner.force_repeat_prefix_structure : False\n",
      "I0925 23:48:13.503629 139756425134720 train.py:167]   train.learner.grad_norm_individual_vars : False\n",
      "I0925 23:48:13.503652 139756425134720 train.py:167]   train.learner.grad_norm_summary : True\n",
      "I0925 23:48:13.503674 139756425134720 train.py:167]   train.learner.keep_optimizer_state_for_excluded_vars : False\n",
      "I0925 23:48:13.503697 139756425134720 train.py:167]   train.learner.loss_name : 'loss'\n",
      "I0925 23:48:13.503719 139756425134720 train.py:167]   train.learner.name : ''\n",
      "I0925 23:48:13.503742 139756425134720 train.py:167]   train.learner.optimizer.clip_gradient_norm_to_value : 0.0\n",
      "I0925 23:48:13.503765 139756425134720 train.py:167]   train.learner.optimizer.clip_gradient_single_norm_to_value : 0.0\n",
      "I0925 23:48:13.503787 139756425134720 train.py:167]   train.learner.optimizer.cls : type/praxis.optimizers/ShardedSgd\n",
      "I0925 23:48:13.503810 139756425134720 train.py:167]   train.learner.optimizer.decoupled_weight_decay : NoneType\n",
      "I0925 23:48:13.503833 139756425134720 train.py:167]   train.learner.optimizer.ema_decay : 0.0\n",
      "I0925 23:48:13.503855 139756425134720 train.py:167]   train.learner.optimizer.ewc_regularizer_weight : 0.0\n",
      "I0925 23:48:13.503878 139756425134720 train.py:167]   train.learner.optimizer.ewc_weight_per_var : NoneType\n",
      "I0925 23:48:13.503901 139756425134720 train.py:167]   train.learner.optimizer.l1_regularizer_weight : NoneType\n",
      "I0925 23:48:13.503923 139756425134720 train.py:167]   train.learner.optimizer.l2_regularizer_weight : NoneType\n",
      "I0925 23:48:13.503945 139756425134720 train.py:167]   train.learner.optimizer.learning_rate : 0.01\n",
      "I0925 23:48:13.503967 139756425134720 train.py:167]   train.learner.optimizer.lr_schedule.cls : type/praxis.schedules/Constant\n",
      "I0925 23:48:13.503990 139756425134720 train.py:167]   train.learner.optimizer.lr_schedule.name : ''\n",
      "I0925 23:48:13.504012 139756425134720 train.py:167]   train.learner.optimizer.lr_schedule.value : 1\n",
      "I0925 23:48:13.504034 139756425134720 train.py:167]   train.learner.optimizer.momentum : 0.1\n",
      "I0925 23:48:13.504057 139756425134720 train.py:167]   train.learner.optimizer.name : ''\n",
      "I0925 23:48:13.504079 139756425134720 train.py:167]   train.learner.optimizer.nesterov : False\n",
      "I0925 23:48:13.504101 139756425134720 train.py:167]   train.learner.optimizer.skip_lp_1d_vectors : False\n",
      "I0925 23:48:13.504124 139756425134720 train.py:167]   train.learner.repeat_prefix_sep : '#'\n",
      "I0925 23:48:13.504146 139756425134720 train.py:167]   train.learner.scale_update_by_var_norm : False\n",
      "I0925 23:48:13.504168 139756425134720 train.py:167]   train.learner.skip_step_gradient_norm_value : 0.0\n",
      "I0925 23:48:13.504190 139756425134720 train.py:167]   train.learner.skip_zero_gradients : NoneType\n",
      "I0925 23:48:13.504213 139756425134720 train.py:167]   train.learner.stochastic_gradient : NoneType\n",
      "I0925 23:48:13.504236 139756425134720 train.py:167]   train.learner.var_norm_summary : True\n",
      "I0925 23:48:13.504258 139756425134720 train.py:167]   train.learner.vectorize_on_repeat_prefix : True\n",
      "I0925 23:48:13.504281 139756425134720 train.py:167]   train.log_train_output_interval_steps : NoneType\n",
      "I0925 23:48:13.504303 139756425134720 train.py:167]   train.max_inflight_steps : 2\n",
      "I0925 23:48:13.504326 139756425134720 train.py:167]   train.num_train_steps : 1000\n",
      "I0925 23:48:13.504348 139756425134720 train.py:167]   train.profiler_capture_step : NoneType\n",
      "I0925 23:48:13.504371 139756425134720 train.py:167]   train.profiler_max_num_hosts : NoneType\n",
      "I0925 23:48:13.504393 139756425134720 train.py:167]   train.profiler_min_duration_sec : 1.0\n",
      "I0925 23:48:13.504416 139756425134720 train.py:167]   train.profiler_num_steps : 2\n",
      "I0925 23:48:13.504438 139756425134720 train.py:167]   train.random_seed : 1234\n",
      "I0925 23:48:13.504461 139756425134720 train.py:167]   train.restore_transformations : NoneType\n",
      "I0925 23:48:13.504483 139756425134720 train.py:167]   train.save_interval_steps : 5000\n",
      "I0925 23:48:13.504505 139756425134720 train.py:167]   train.save_keep_interval_duration : '12h'\n",
      "I0925 23:48:13.504528 139756425134720 train.py:167]   train.save_max_to_keep : 10\n",
      "I0925 23:48:13.504550 139756425134720 train.py:167]   train.summary_accumulate_interval_steps : NoneType\n",
      "I0925 23:48:13.504574 139756425134720 train.py:167]   train.summary_interval_steps : 100\n",
      "I0925 23:48:13.504596 139756425134720 train.py:167]   train.tensorstore_metadata_key : NoneType\n",
      "I0925 23:48:13.504618 139756425134720 train.py:167]   train.variable_norm_summary : True\n",
      "I0925 23:48:13.504641 139756425134720 train.py:167]   vn.cls : type/paxml.tasks_lib/SingleTask.VariationalNoise\n",
      "I0925 23:48:13.504664 139756425134720 train.py:167]   vn.vn_regex : ''\n",
      "I0925 23:48:13.504686 139756425134720 train.py:167]   vn.vn_scale : 0.0\n",
      "I0925 23:48:13.504709 139756425134720 train.py:167]   vn.vn_start_step : 0\n",
      "I0925 23:48:13.504745 139756425134720 train.py:170] [PAX STATUS]: Creating task\n",
      "I0925 23:48:13.510623 139756425134720 py_utils.py:1015] [PAX STATUS]: Starting timer for <_create_checkpointer> @ <.../paxml/train.py:178>\n",
      "I0925 23:48:13.510684 139756425134720 checkpoint_creators.py:555] [PAX STATUS]: Creating checkpointer.\n",
      "I0925 23:48:13.510836 139756425134720 py_utils.py:339] Starting sync_global_devices checkpointer:makedirs:/tmp/dali_pax_logs/checkpoints across 1 devices globally\n",
      "W0925 23:48:13.684840 139756425134720 dispatch.py:271] Finished tracing + transforming _reduce_sum for pjit in 0.0005867481231689453 sec\n",
      "W0925 23:48:13.685304 139756425134720 dispatch.py:271] Finished tracing + transforming _psum for pjit in 0.0015795230865478516 sec\n",
      "W0925 23:48:13.687349 139756425134720 pxla.py:1764] Compiling _psum for with global shapes and types [ShapedArray(uint32[1])]. Argument mapping: (GSPMDSharding({replicated}),).\n",
      "W0925 23:48:13.689195 139756425134720 dispatch.py:271] Finished jaxpr to MLIR module conversion jit(_psum) in 0.001695394515991211 sec\n",
      "W0925 23:48:13.743861 139756425134720 dispatch.py:271] Finished XLA compilation of jit(_psum) in 0.05438947677612305 sec\n",
      "I0925 23:48:13.748855 139756425134720 py_utils.py:342] Finished sync_global_devices checkpointer:makedirs:/tmp/dali_pax_logs/checkpoints across 1 devices globally\n",
      "I0925 23:48:13.750697 139756425134720 utils.py:400] Cleaning up existing temporary directories at /tmp/dali_pax_logs/checkpoints.\n",
      "I0925 23:48:13.752090 139756425134720 py_utils.py:1025] [PAX STATUS]: Elapsed time for <_create_checkpointer>: 0.24 seconds  (@ <.../paxml/train.py:178>)\n",
      "I0925 23:48:13.752149 139756425134720 train.py:204] [PAX STATUS]: Initializing partitioner\n",
      "I0925 23:48:13.752193 139756425134720 partitioning.py:759] Using SPMD sharding for model parallelism.\n",
      "I0925 23:48:13.752219 139756425134720 partitioning.py:765] creating mesh with py_utils.create_device_mesh\n",
      "I0925 23:48:13.752290 139756425134720 py_utils.py:700] device_mesh: [[[gpu(id=0)]]]\n",
      "I0925 23:48:13.752561 139756425134720 partitioning.py:773] device_mesh: [[[gpu(id=0)]]]\n",
      "I0925 23:48:13.753704 139756425134720 partitioning.py:426] input_p.tf_data_service_address: None\n",
      "I0925 23:48:13.753800 139756425134720 train.py:227] [PAX STATUS]: Initializing train program.\n",
      "I0925 23:48:13.753867 139756425134720 train.py:230] [PAX STATUS]: Initializing eval programs.\n",
      "I0925 23:48:13.753897 139756425134720 train.py:239] [PAX STATUS]: Initializing decode programs.\n",
      "I0925 23:48:13.753924 139756425134720 train.py:255] [PAX STATUS]: Creating executor.\n",
      "I0925 23:48:13.753958 139756425134720 train.py:259] [PAX STATUS]: Setting up executor.\n",
      "/home/awolant/Projects/DALI/dali/operators/reader/loader/lmdb.h:61: lmdb 0 /home/awolant/Projects/DALI_extra/db/MNIST/training/ has 60000 entries\n",
      "/home/awolant/Projects/DALI/dali/operators/reader/loader/lmdb.h:76: lmdb 0 /home/awolant/Projects/DALI_extra/db/MNIST/training/ rewind to the begin from 0\n",
      "/home/awolant/Projects/DALI/dali/operators/decoder/nvjpeg/nvjpeg_decoder_decoupled_api.h:192: NVJPEG_BACKEND_HARDWARE is either disabled or not supported\n",
      "/home/awolant/Projects/DALI/dali/operators/reader/loader/lmdb.h:76: lmdb 0 /home/awolant/Projects/DALI_extra/db/MNIST/training/ rewind to the begin from 0\n",
      "W0925 23:48:14.062589 139756425134720 dispatch.py:271] Finished tracing + transforming jit(copy) in 0.0002002716064453125 sec\n",
      "W0925 23:48:14.062879 139756425134720 pxla.py:1764] Compiling copy for with global shapes and types [ShapedArray(float32[1024,28,28,1])]. Argument mapping: (GSPMDSharding({replicated}),).\n",
      "W0925 23:48:14.064079 139756425134720 dispatch.py:271] Finished jaxpr to MLIR module conversion jit(copy) in 0.0010700225830078125 sec\n",
      "W0925 23:48:14.077452 139756425134720 dispatch.py:271] Finished XLA compilation of jit(copy) in 0.013075590133666992 sec\n",
      "W0925 23:48:14.079048 139756425134720 dispatch.py:271] Finished tracing + transforming jit(copy) in 0.00018405914306640625 sec\n",
      "W0925 23:48:14.079285 139756425134720 pxla.py:1764] Compiling copy for with global shapes and types [ShapedArray(int32[1024])]. Argument mapping: (GSPMDSharding({replicated}),).\n",
      "W0925 23:48:14.080315 139756425134720 dispatch.py:271] Finished jaxpr to MLIR module conversion jit(copy) in 0.0009031295776367188 sec\n",
      "W0925 23:48:14.091496 139756425134720 dispatch.py:271] Finished XLA compilation of jit(copy) in 0.010910987854003906 sec\n",
      "W0925 23:48:14.092221 139756425134720 base_input.py:1108] b/292156360: The input specs is generated based on the first data batch. It is recommended to define an explicit input spec provider param in BaseExperiment.get_input_specs_provider_params(), which is more deterministic and efficient.\n",
      "I0925 23:48:14.092297 139756425134720 py_utils.py:1015] [PAX STATUS]: Starting timer for <get_next_padded> @ <.../praxis/base_input.py:1116>\n",
      "I0925 23:48:14.092333 139756425134720 py_utils.py:1025] [PAX STATUS]: Elapsed time for <get_next_padded>: 0.00 seconds  (@ <.../praxis/base_input.py:1116>)\n",
      "W0925 23:48:14.092905 139756425134720 dispatch.py:271] Finished tracing + transforming jit(convert_element_type) in 0.00020122528076171875 sec\n",
      "W0925 23:48:14.131645 139756425134720 dispatch.py:271] Finished tracing + transforming <lambda> for pjit in 0.03763723373413086 sec\n",
      "W0925 23:48:14.132576 139756425134720 dispatch.py:271] Finished tracing + transforming _threefry_seed for pjit in 0.03912091255187988 sec\n",
      "W0925 23:48:14.133086 139756425134720 pxla.py:1764] Compiling _threefry_seed for with global shapes and types [ShapedArray(int32[])]. Argument mapping: (GSPMDSharding({replicated}),).\n",
      "W0925 23:48:14.135540 139756425134720 dispatch.py:271] Finished jaxpr to MLIR module conversion jit(_threefry_seed) in 0.002330303192138672 sec\n",
      "W0925 23:48:14.185953 139756425134720 dispatch.py:271] Finished XLA compilation of jit(_threefry_seed) in 0.05014443397521973 sec\n",
      "W0925 23:48:14.187483 139756425134720 dispatch.py:271] Finished tracing + transforming jit(concatenate) in 0.0002639293670654297 sec\n",
      "W0925 23:48:14.187741 139756425134720 pxla.py:1764] Compiling concatenate for with global shapes and types [ShapedArray(uint32[2]), ShapedArray(uint32[2])]. Argument mapping: (GSPMDSharding({replicated}), GSPMDSharding({replicated})).\n",
      "W0925 23:48:14.188944 139756425134720 dispatch.py:271] Finished jaxpr to MLIR module conversion jit(concatenate) in 0.0010724067687988281 sec\n",
      "W0925 23:48:14.234786 139756425134720 dispatch.py:271] Finished XLA compilation of jit(concatenate) in 0.04557943344116211 sec\n",
      "I0925 23:48:14.236152 139756425134720 partitioning.py:426] input_p.tf_data_service_address: None\n",
      "I0925 23:48:14.236262 139756425134720 executors.py:142] [PAX STATUS]: Instantiating train input pipeline (<PaxConfig[MnistDaliInput(\n",
      "  batch_size=1024,\n",
      "  num_infeed_hosts=1,\n",
      "  infeed_host_index=0,\n",
      "  is_training=True,\n",
      "  tf_data_service_address=None)]>)\n",
      "/home/awolant/Projects/DALI/dali/operators/reader/loader/lmdb.h:61: lmdb 0 /home/awolant/Projects/DALI_extra/db/MNIST/training/ has 60000 entries\n",
      "/home/awolant/Projects/DALI/dali/operators/reader/loader/lmdb.h:76: lmdb 0 /home/awolant/Projects/DALI_extra/db/MNIST/training/ rewind to the begin from 0\n",
      "/home/awolant/Projects/DALI/dali/operators/decoder/nvjpeg/nvjpeg_decoder_decoupled_api.h:192: NVJPEG_BACKEND_HARDWARE is either disabled or not supported\n",
      "/home/awolant/Projects/DALI/dali/operators/reader/loader/lmdb.h:76: lmdb 0 /home/awolant/Projects/DALI_extra/db/MNIST/training/ rewind to the begin from 0\n",
      "I0925 23:48:14.334674 139756425134720 trainer_lib.py:1662] Spec yielded by InputSpecProvider for model init: {'inputs': ShapeDtypeStruct(shape=(1024, 28, 28, 1), dtype=float32),\n",
      " 'labels': ShapeDtypeStruct(shape=(1024,), dtype=int32)}\n",
      "I0925 23:48:14.334893 139756425134720 trainer_lib.py:1689] Modified spec for pjit global batch size: {'inputs': ShapeDtypeStruct(shape=(1024, 28, 28, 1), dtype=float32),\n",
      " 'labels': ShapeDtypeStruct(shape=(1024,), dtype=int32)}\n",
      "I0925 23:48:14.334942 139756425134720 executors.py:201] [PAX STATUS]: Setting up partitioner\n",
      "I0925 23:48:14.334977 139756425134720 partitioning.py:356] [PAX STATUS]: Getting input shapes from spec.\n",
      "I0925 23:48:14.335005 139756425134720 executors.py:208] [PAX STATUS]: Getting train state metadata.\n",
      "/home/awolant/.local/lib/python3.10/site-packages/jax/_src/numpy/array_methods.py:728: UserWarning: Explicitly requested dtype float64 requested in astype is not available, and will be truncated to dtype float32. To enable more dtypes, set the jax_enable_x64 configuration option or the JAX_ENABLE_X64 shell environment variable. See https://github.com/google/jax#current-gotchas for more.\n",
      "  return getattr(self.aval, name).fun(self, *args, **kwargs)\n",
      "I0925 23:48:14.445081 139756425134720 executors.py:212] [PAX STATUS]: Writing post init model hparams.\n",
      "I0925 23:48:14.445230 139756425134720 trainer_lib.py:212] post_init_model_params: /tmp/dali_pax_logs/post_init_model_params.txt\n",
      "I0925 23:48:14.678111 139756425134720 executors.py:220] [PAX STATUS]: Starting checkpoint load / variable init.\n",
      "W0925 23:48:14.678651 139756425134720 dispatch.py:271] Finished tracing + transforming jit(reshape) in 0.0001811981201171875 sec\n",
      "W0925 23:48:14.678922 139756425134720 pxla.py:1764] Compiling reshape for with global shapes and types [ShapedArray(uint32[4])]. Argument mapping: (GSPMDSharding({replicated}),).\n",
      "W0925 23:48:14.680143 139756425134720 dispatch.py:271] Finished jaxpr to MLIR module conversion jit(reshape) in 0.0011043548583984375 sec\n",
      "W0925 23:48:14.696693 139756425134720 dispatch.py:271] Finished XLA compilation of jit(reshape) in 0.016240596771240234 sec\n",
      "W0925 23:48:14.699280 139756425134720 dispatch.py:271] Finished tracing + transforming ravel for pjit in 0.00020241737365722656 sec\n",
      "W0925 23:48:14.700078 139756425134720 dispatch.py:271] Finished tracing + transforming threefry_2x32 for pjit in 0.0015156269073486328 sec\n",
      "W0925 23:48:14.701120 139756425134720 dispatch.py:271] Finished tracing + transforming _threefry_split_original for pjit in 0.0029141902923583984 sec\n",
      "W0925 23:48:14.703463 139756425134720 pxla.py:1764] Compiling _threefry_split_original for with global shapes and types [ShapedArray(uint32[2,2])]. Argument mapping: (GSPMDSharding({replicated}),).\n",
      "W0925 23:48:14.706483 139756425134720 dispatch.py:271] Finished jaxpr to MLIR module conversion jit(_threefry_split_original) in 0.0028786659240722656 sec\n",
      "W0925 23:48:14.775524 139756425134720 dispatch.py:271] Finished XLA compilation of jit(_threefry_split_original) in 0.06872916221618652 sec\n",
      "W0925 23:48:14.777434 139756425134720 dispatch.py:271] Finished tracing + transforming jit(transpose) in 0.0002827644348144531 sec\n",
      "W0925 23:48:14.777797 139756425134720 pxla.py:1764] Compiling transpose for with global shapes and types [ShapedArray(uint32[2,2,2])]. Argument mapping: (GSPMDSharding({replicated}),).\n",
      "W0925 23:48:14.779557 139756425134720 dispatch.py:271] Finished jaxpr to MLIR module conversion jit(transpose) in 0.0015938282012939453 sec\n",
      "W0925 23:48:14.826628 139756425134720 dispatch.py:271] Finished XLA compilation of jit(transpose) in 0.04670858383178711 sec\n",
      "W0925 23:48:14.828919 139756425134720 dispatch.py:271] Finished tracing + transforming jit(reshape) in 0.00020885467529296875 sec\n",
      "W0925 23:48:14.829172 139756425134720 pxla.py:1764] Compiling reshape for with global shapes and types [ShapedArray(uint32[2,2,2])]. Argument mapping: (GSPMDSharding({replicated}),).\n",
      "W0925 23:48:14.830398 139756425134720 dispatch.py:271] Finished jaxpr to MLIR module conversion jit(reshape) in 0.0011036396026611328 sec\n",
      "W0925 23:48:14.842380 139756425134720 dispatch.py:271] Finished XLA compilation of jit(reshape) in 0.011716604232788086 sec\n",
      "W0925 23:48:14.843687 139756425134720 dispatch.py:271] Finished tracing + transforming _unstack for pjit in 0.0004982948303222656 sec\n",
      "W0925 23:48:14.844178 139756425134720 pxla.py:1764] Compiling _unstack for with global shapes and types [ShapedArray(uint32[2,4])]. Argument mapping: (GSPMDSharding({replicated}),).\n",
      "W0925 23:48:14.845966 139756425134720 dispatch.py:271] Finished jaxpr to MLIR module conversion jit(_unstack) in 0.001672506332397461 sec\n",
      "W0925 23:48:14.895912 139756425134720 dispatch.py:271] Finished XLA compilation of jit(_unstack) in 0.04963111877441406 sec\n",
      "I0925 23:48:14.904065 139756425134720 trainer_lib.py:1490] unpadded_out_shape: TrainState(step=(), mdl_vars={'params': {'network': {'conv1': {'b': (32,), 'w': (3, 3, 1, 32)}, 'conv2': {'b': (64,), 'w': (3, 3, 32, 64)}, 'dense_0': {'bias': {'b': (256,)}, 'linear': {'w': (3136, 256)}}, 'dense_1': {'w': (256, 10)}, 'dense_2': {'b': (10,)}}}}, opt_states=[({'count': ()}, {'count': ()}, (TraceState(trace={'params': {'network': {'conv1': {'b': (32,), 'w': (3, 3, 1, 32)}, 'conv2': {'b': (64,), 'w': (3, 3, 32, 64)}, 'dense_0': {'bias': {'b': (256,)}, 'linear': {'w': (3136, 256)}}, 'dense_1': {'w': (256, 10)}, 'dense_2': {'b': (10,)}}}}), ScaleByScheduleState(count=())), {'count': ()})], extra_state=())\n",
      "I0925 23:48:14.904503 139756425134720 trainer_lib.py:1491] train_state_partition_specs: TrainState(step=PartitionSpec(), mdl_vars={'params': {'network': {'conv1': {'b': PartitionSpec(None,), 'w': PartitionSpec(None, None, None, None)}, 'conv2': {'b': PartitionSpec(None,), 'w': PartitionSpec(None, None, None, None)}, 'dense_0': {'bias': {'b': PartitionSpec(None,)}, 'linear': {'w': PartitionSpec(None, None)}}, 'dense_1': {'w': PartitionSpec(None, None)}, 'dense_2': {'b': PartitionSpec(None,)}}}}, opt_states=[({'count': PartitionSpec()}, {'count': PartitionSpec()}, (TraceState(trace={'params': {'network': {'conv1': {'b': PartitionSpec(None,), 'w': PartitionSpec(None, None, None, None)}, 'conv2': {'b': PartitionSpec(None,), 'w': PartitionSpec(None, None, None, None)}, 'dense_0': {'bias': {'b': PartitionSpec(None,)}, 'linear': {'w': PartitionSpec(None, None)}}, 'dense_1': {'w': PartitionSpec(None, None)}, 'dense_2': {'b': PartitionSpec(None,)}}}}), ScaleByScheduleState(count=PartitionSpec())), {'count': PartitionSpec()})], extra_state=())\n",
      "I0925 23:48:14.906272 139756425134720 trainer_lib.py:379] init_var prng_seed: {'params': Traced<ShapedArray(uint32[4])>with<DynamicJaxprTrace(level=1/0)>, 'random': Traced<ShapedArray(uint32[4])>with<DynamicJaxprTrace(level=1/0)>, 'dropout': Traced<ShapedArray(uint32[4])>with<DynamicJaxprTrace(level=1/0)>}\n",
      "I0925 23:48:14.906459 139756425134720 trainer_lib.py:380] var_weight_hparams: {'params': {'network': {'conv1': {'b': WeightHParams(shape=[32], init=WeightInit(method='constant', scale=0.0), dtype=<class 'jax.numpy.float32'>, collections=[], mesh_shape=None, tensor_split_dims_mapping=None, repeat_prefix=None, repeat_prefix_split_dims_mapping=None, repeat_optimizer_dims_mapping=None, fan_in_axes=None, fan_out_axes=None), 'w': WeightHParams(shape=(3, 3, 1, 32), init=WeightInit(method='xavier', scale=1.000001), dtype=<class 'jax.numpy.float32'>, collections=[], mesh_shape=(1, 1, 1), tensor_split_dims_mapping=(-1, -1, -1, -1), repeat_prefix=None, repeat_prefix_split_dims_mapping=None, repeat_optimizer_dims_mapping=None, fan_in_axes=None, fan_out_axes=None)}, 'conv2': {'b': WeightHParams(shape=[64], init=WeightInit(method='constant', scale=0.0), dtype=<class 'jax.numpy.float32'>, collections=[], mesh_shape=None, tensor_split_dims_mapping=None, repeat_prefix=None, repeat_prefix_split_dims_mapping=None, repeat_optimizer_dims_mapping=None, fan_in_axes=None, fan_out_axes=None), 'w': WeightHParams(shape=(3, 3, 32, 64), init=WeightInit(method='xavier', scale=1.000001), dtype=<class 'jax.numpy.float32'>, collections=[], mesh_shape=(1, 1, 1), tensor_split_dims_mapping=(-1, -1, -1, -1), repeat_prefix=None, repeat_prefix_split_dims_mapping=None, repeat_optimizer_dims_mapping=None, fan_in_axes=None, fan_out_axes=None)}, 'dense_0': {'bias': {'b': WeightHParams(shape=[256], init=WeightInit(method='constant', scale=0.0), dtype=<class 'jax.numpy.float32'>, collections=[], mesh_shape=(1, 1, 1), tensor_split_dims_mapping=(-1,), repeat_prefix=None, repeat_prefix_split_dims_mapping=None, repeat_optimizer_dims_mapping=None, fan_in_axes=None, fan_out_axes=None)}, 'linear': {'w': WeightHParams(shape=[3136, 256], init=WeightInit(method='xavier', scale=1.000001), dtype=<class 'jax.numpy.float32'>, collections=[], mesh_shape=(1, 1, 1), tensor_split_dims_mapping=(-1, -1), repeat_prefix=None, repeat_prefix_split_dims_mapping=None, repeat_optimizer_dims_mapping=None, fan_in_axes=None, fan_out_axes=None)}}, 'dense_1': {'w': WeightHParams(shape=[256, 10], init=WeightInit(method='xavier', scale=1.000001), dtype=<class 'jax.numpy.float32'>, collections=[], mesh_shape=(1, 1, 1), tensor_split_dims_mapping=(-1, -1), repeat_prefix=None, repeat_prefix_split_dims_mapping=None, repeat_optimizer_dims_mapping=None, fan_in_axes=None, fan_out_axes=None)}, 'dense_2': {'b': WeightHParams(shape=[10], init=WeightInit(method='constant', scale=0.0), dtype=<class 'jax.numpy.float32'>, collections=[], mesh_shape=(1, 1, 1), tensor_split_dims_mapping=(-1,), repeat_prefix=None, repeat_prefix_split_dims_mapping=None, repeat_optimizer_dims_mapping=None, fan_in_axes=None, fan_out_axes=None)}}}}\n",
      "I0925 23:48:14.932641 139756425134720 base_layer.py:530] Sets tensor_split_dims_mapping of a param of shape (3, 3, 1, 32) to (-1, -1, -1, -1)\n",
      "I0925 23:48:14.933264 139756425134720 base_layer.py:635] Creating var /network/conv1/w with shape=(3, 3, 1, 32), dtype=<class 'jax.numpy.float32'>, init method=xavier and scale=1.000001\n",
      "W0925 23:48:14.934730 139756425134720 dispatch.py:271] Finished tracing + transforming <lambda> for pjit in 0.00033020973205566406 sec\n",
      "W0925 23:48:14.935523 139756425134720 dispatch.py:271] Finished tracing + transforming <lambda> for pjit in 0.0003154277801513672 sec\n",
      "W0925 23:48:14.936151 139756425134720 dispatch.py:271] Finished tracing + transforming fn for pjit in 0.00026869773864746094 sec\n",
      "W0925 23:48:14.936930 139756425134720 dispatch.py:271] Finished tracing + transforming fn for pjit in 0.0004208087921142578 sec\n",
      "W0925 23:48:14.937463 139756425134720 dispatch.py:271] Finished tracing + transforming _uniform for pjit in 0.003824472427368164 sec\n",
      "W0925 23:48:14.938333 139756425134720 dispatch.py:271] Finished tracing + transforming fn for pjit in 0.0003814697265625 sec\n",
      "I0925 23:48:14.939286 139756425134720 base_layer.py:635] Creating var /network/conv1/b with shape=[32], dtype=<class 'jax.numpy.float32'>, init method=constant and scale=0.0\n",
      "W0925 23:48:14.939924 139756425134720 dispatch.py:271] Finished tracing + transforming fn for pjit in 0.0002956390380859375 sec\n",
      "W0925 23:48:14.944426 139756425134720 dispatch.py:271] Finished tracing + transforming fn for pjit in 0.0003364086151123047 sec\n",
      "W0925 23:48:14.945576 139756425134720 dispatch.py:271] Finished tracing + transforming <lambda> for pjit in 0.0003120899200439453 sec\n",
      "W0925 23:48:14.946022 139756425134720 dispatch.py:271] Finished tracing + transforming relu for pjit in 0.0009503364562988281 sec\n",
      "W0925 23:48:14.947643 139756425134720 dispatch.py:271] Finished tracing + transforming reciprocal for pjit in 0.0002353191375732422 sec\n",
      "W0925 23:48:14.948293 139756425134720 dispatch.py:271] Finished tracing + transforming fn for pjit in 0.00028514862060546875 sec\n",
      "I0925 23:48:14.948861 139756425134720 base_layer.py:530] Sets tensor_split_dims_mapping of a param of shape (3, 3, 32, 64) to (-1, -1, -1, -1)\n",
      "I0925 23:48:14.949408 139756425134720 base_layer.py:635] Creating var /network/conv2/w with shape=(3, 3, 32, 64), dtype=<class 'jax.numpy.float32'>, init method=xavier and scale=1.000001\n",
      "W0925 23:48:14.950689 139756425134720 dispatch.py:271] Finished tracing + transforming <lambda> for pjit in 0.00030112266540527344 sec\n",
      "W0925 23:48:14.951667 139756425134720 dispatch.py:271] Finished tracing + transforming fn for pjit in 0.0002923011779785156 sec\n",
      "W0925 23:48:14.952308 139756425134720 dispatch.py:271] Finished tracing + transforming fn for pjit in 0.0002644062042236328 sec\n",
      "W0925 23:48:14.952769 139756425134720 dispatch.py:271] Finished tracing + transforming _uniform for pjit in 0.0030629634857177734 sec\n",
      "W0925 23:48:14.953442 139756425134720 dispatch.py:271] Finished tracing + transforming fn for pjit in 0.00029540061950683594 sec\n",
      "I0925 23:48:14.954248 139756425134720 base_layer.py:635] Creating var /network/conv2/b with shape=[64], dtype=<class 'jax.numpy.float32'>, init method=constant and scale=0.0\n",
      "W0925 23:48:14.954976 139756425134720 dispatch.py:271] Finished tracing + transforming fn for pjit in 0.0003895759582519531 sec\n",
      "W0925 23:48:14.959354 139756425134720 dispatch.py:271] Finished tracing + transforming fn for pjit in 0.00035381317138671875 sec\n",
      "W0925 23:48:14.960512 139756425134720 dispatch.py:271] Finished tracing + transforming <lambda> for pjit in 0.0003445148468017578 sec\n",
      "W0925 23:48:14.960954 139756425134720 dispatch.py:271] Finished tracing + transforming relu for pjit in 0.0009624958038330078 sec\n",
      "W0925 23:48:14.962628 139756425134720 dispatch.py:271] Finished tracing + transforming reciprocal for pjit in 0.0003464221954345703 sec\n",
      "W0925 23:48:14.963253 139756425134720 dispatch.py:271] Finished tracing + transforming fn for pjit in 0.0002853870391845703 sec\n",
      "I0925 23:48:14.968492 139756425134720 base_layer.py:530] Sets tensor_split_dims_mapping of a param of shape [3136, 256] to (-1, -1)\n",
      "I0925 23:48:14.968970 139756425134720 base_layer.py:635] Creating var /network/dense_0/linear/w with shape=[3136, 256], dtype=<class 'jax.numpy.float32'>, init method=xavier and scale=1.000001\n",
      "W0925 23:48:14.970197 139756425134720 dispatch.py:271] Finished tracing + transforming <lambda> for pjit in 0.0003037452697753906 sec\n",
      "W0925 23:48:14.970929 139756425134720 dispatch.py:271] Finished tracing + transforming <lambda> for pjit in 0.00029969215393066406 sec\n",
      "W0925 23:48:14.971657 139756425134720 dispatch.py:271] Finished tracing + transforming fn for pjit in 0.0003216266632080078 sec\n",
      "W0925 23:48:14.972370 139756425134720 dispatch.py:271] Finished tracing + transforming fn for pjit in 0.0003135204315185547 sec\n",
      "W0925 23:48:14.972912 139756425134720 dispatch.py:271] Finished tracing + transforming _uniform for pjit in 0.003648996353149414 sec\n",
      "W0925 23:48:14.973773 139756425134720 dispatch.py:271] Finished tracing + transforming fn for pjit in 0.0004146099090576172 sec\n",
      "W0925 23:48:14.976747 139756425134720 dispatch.py:271] Finished tracing + transforming _einsum for pjit in 0.0004754066467285156 sec\n",
      "I0925 23:48:14.977315 139756425134720 base_layer.py:530] Sets tensor_split_dims_mapping of a param of shape [256] to (-1,)\n",
      "I0925 23:48:14.977798 139756425134720 base_layer.py:635] Creating var /network/dense_0/bias/b with shape=[256], dtype=<class 'jax.numpy.float32'>, init method=constant and scale=0.0\n",
      "W0925 23:48:14.978518 139756425134720 dispatch.py:271] Finished tracing + transforming fn for pjit in 0.00034880638122558594 sec\n",
      "W0925 23:48:14.979720 139756425134720 dispatch.py:271] Finished tracing + transforming fn for pjit in 0.0004703998565673828 sec\n",
      "W0925 23:48:14.981162 139756425134720 dispatch.py:271] Finished tracing + transforming <lambda> for pjit in 0.0003733634948730469 sec\n",
      "W0925 23:48:14.981606 139756425134720 dispatch.py:271] Finished tracing + transforming relu for pjit in 0.0011141300201416016 sec\n",
      "I0925 23:48:14.982108 139756425134720 base_layer.py:530] Sets tensor_split_dims_mapping of a param of shape [256, 10] to (-1, -1)\n",
      "I0925 23:48:14.982622 139756425134720 base_layer.py:635] Creating var /network/dense_1/w with shape=[256, 10], dtype=<class 'jax.numpy.float32'>, init method=xavier and scale=1.000001\n",
      "W0925 23:48:14.983951 139756425134720 dispatch.py:271] Finished tracing + transforming <lambda> for pjit in 0.0003139972686767578 sec\n",
      "W0925 23:48:14.984868 139756425134720 dispatch.py:271] Finished tracing + transforming fn for pjit in 0.0002789497375488281 sec\n",
      "W0925 23:48:14.985529 139756425134720 dispatch.py:271] Finished tracing + transforming fn for pjit in 0.0002827644348144531 sec\n",
      "W0925 23:48:14.986083 139756425134720 dispatch.py:271] Finished tracing + transforming _uniform for pjit in 0.003141164779663086 sec\n",
      "W0925 23:48:14.986881 139756425134720 dispatch.py:271] Finished tracing + transforming fn for pjit in 0.0003554821014404297 sec\n",
      "W0925 23:48:14.989796 139756425134720 dispatch.py:271] Finished tracing + transforming _einsum for pjit in 0.00046896934509277344 sec\n",
      "I0925 23:48:14.990427 139756425134720 base_layer.py:530] Sets tensor_split_dims_mapping of a param of shape [10] to (-1,)\n",
      "I0925 23:48:14.990893 139756425134720 base_layer.py:635] Creating var /network/dense_2/b with shape=[10], dtype=<class 'jax.numpy.float32'>, init method=constant and scale=0.0\n",
      "W0925 23:48:14.991545 139756425134720 dispatch.py:271] Finished tracing + transforming fn for pjit in 0.0003132820129394531 sec\n",
      "W0925 23:48:14.992617 139756425134720 dispatch.py:271] Finished tracing + transforming fn for pjit in 0.0004706382751464844 sec\n",
      "W0925 23:48:14.993841 139756425134720 dispatch.py:271] Finished tracing + transforming _reduce_max for pjit in 0.00047850608825683594 sec\n",
      "W0925 23:48:14.994747 139756425134720 dispatch.py:271] Finished tracing + transforming <lambda> for pjit in 0.0003724098205566406 sec\n",
      "W0925 23:48:14.995334 139756425134720 dispatch.py:271] Finished tracing + transforming <lambda> for pjit in 0.00022840499877929688 sec\n",
      "W0925 23:48:14.996135 139756425134720 dispatch.py:271] Finished tracing + transforming _reduce_sum for pjit in 0.0004527568817138672 sec\n",
      "W0925 23:48:14.996837 139756425134720 dispatch.py:271] Finished tracing + transforming <lambda> for pjit in 0.00024366378784179688 sec\n",
      "W0925 23:48:14.997417 139756425134720 dispatch.py:271] Finished tracing + transforming log_softmax for pjit in 0.004275321960449219 sec\n",
      "W0925 23:48:14.998442 139756425134720 dispatch.py:271] Finished tracing + transforming <lambda> for pjit in 0.00028967857360839844 sec\n",
      "W0925 23:48:14.998873 139756425134720 dispatch.py:271] Finished tracing + transforming _one_hot for pjit in 0.0010981559753417969 sec\n",
      "W0925 23:48:14.999462 139756425134720 dispatch.py:271] Finished tracing + transforming fn for pjit in 0.00028061866760253906 sec\n",
      "W0925 23:48:15.000218 139756425134720 dispatch.py:271] Finished tracing + transforming _reduce_sum for pjit in 0.00035762786865234375 sec\n",
      "W0925 23:48:15.000744 139756425134720 dispatch.py:271] Finished tracing + transforming <lambda> for pjit in 0.00018596649169921875 sec\n",
      "W0925 23:48:15.002266 139756425134720 dispatch.py:271] Finished tracing + transforming _reduce_sum for pjit in 0.0009779930114746094 sec\n",
      "W0925 23:48:15.002711 139756425134720 dispatch.py:271] Finished tracing + transforming _mean for pjit in 0.0016384124755859375 sec\n",
      "W0925 23:48:15.003444 139756425134720 dispatch.py:271] Finished tracing + transforming _argmax for pjit in 0.00023365020751953125 sec\n",
      "W0925 23:48:15.004086 139756425134720 dispatch.py:271] Finished tracing + transforming <lambda> for pjit in 0.00028896331787109375 sec\n",
      "W0925 23:48:15.005840 139756425134720 dispatch.py:271] Finished tracing + transforming init_fn for pjit in 0.09902286529541016 sec\n",
      "I0925 23:48:15.006710 139756425134720 trainer_lib.py:399] initial_vars: {'params': {'network': {'conv1': {'b': (32,), 'w': (3, 3, 1, 32)}, 'conv2': {'b': (64,), 'w': (3, 3, 32, 64)}, 'dense_0': {'bias': {'b': (256,)}, 'linear': {'w': (3136, 256)}}, 'dense_1': {'w': (256, 10)}, 'dense_2': {'b': (10,)}}}}\n",
      "W0925 23:48:15.009309 139756425134720 dispatch.py:271] Finished tracing + transforming init_model_from_seed for pjit in 0.10367107391357422 sec\n",
      "W0925 23:48:15.011691 139756425134720 pxla.py:1764] Compiling init_model_from_seed for with global shapes and types [ShapedArray(uint32[4])]. Argument mapping: (GSPMDSharding({replicated}),).\n",
      "W0925 23:48:15.014761 139756425134720 dispatch.py:271] Finished tracing + transforming ravel for pjit in 0.0001437664031982422 sec\n",
      "W0925 23:48:15.015401 139756425134720 dispatch.py:271] Finished tracing + transforming threefry_2x32 for pjit in 0.0011692047119140625 sec\n",
      "W0925 23:48:15.016078 139756425134720 dispatch.py:271] Finished tracing + transforming _threefry_split_original for pjit in 0.002113819122314453 sec\n",
      "W0925 23:48:15.022519 139756425134720 dispatch.py:271] Finished tracing + transforming <lambda> for pjit in 0.0002567768096923828 sec\n",
      "W0925 23:48:15.022996 139756425134720 dispatch.py:271] Finished tracing + transforming _threefry_seed for pjit in 0.001085042953491211 sec\n",
      "W0925 23:48:15.024205 139756425134720 dispatch.py:271] Finished tracing + transforming ravel for pjit in 0.00014066696166992188 sec\n",
      "W0925 23:48:15.024850 139756425134720 dispatch.py:271] Finished tracing + transforming threefry_2x32 for pjit in 0.001279592514038086 sec\n",
      "W0925 23:48:15.025641 139756425134720 dispatch.py:271] Finished tracing + transforming _threefry_fold_in for pjit in 0.003871440887451172 sec\n",
      "W0925 23:48:15.057166 139756425134720 dispatch.py:271] Finished jaxpr to MLIR module conversion pjit(init_model_from_seed) in 0.045342445373535156 sec\n",
      "W0925 23:48:15.409904 139756425134720 dispatch.py:271] Finished XLA compilation of pjit(init_model_from_seed) in 0.3524162769317627 sec\n",
      "I0925 23:48:15.414730 139756425134720 partitioning.py:849] partitioned train state shapes (global shape): TrainState(step=(), mdl_vars={'params': {'network': {'conv1': {'b': (32,), 'w': (3, 3, 1, 32)}, 'conv2': {'b': (64,), 'w': (3, 3, 32, 64)}, 'dense_0': {'bias': {'b': (256,)}, 'linear': {'w': (3136, 256)}}, 'dense_1': {'w': (256, 10)}, 'dense_2': {'b': (10,)}}}}, opt_states=[({'count': ()}, {'count': ()}, (TraceState(trace={'params': {'network': {'conv1': {'b': (32,), 'w': (3, 3, 1, 32)}, 'conv2': {'b': (64,), 'w': (3, 3, 32, 64)}, 'dense_0': {'bias': {'b': (256,)}, 'linear': {'w': (3136, 256)}}, 'dense_1': {'w': (256, 10)}, 'dense_2': {'b': (10,)}}}}), ScaleByScheduleState(count=())), {'count': ()})], extra_state=())\n",
      "I0925 23:48:15.414864 139756425134720 partitioning.py:857] root prng key: [2113592192 1902136347 2113592192 1902136347]\n",
      "I0925 23:48:15.415531 139756425134720 executors.py:233] [PAX STATUS]: Checkpoint load / variable init took 0 seconds\n",
      "W0925 23:48:15.417838 139756425134720 dispatch.py:271] Finished tracing + transforming ravel for pjit in 0.0001685619354248047 sec\n",
      "W0925 23:48:15.418610 139756425134720 dispatch.py:271] Finished tracing + transforming threefry_2x32 for pjit in 0.001432180404663086 sec\n",
      "W0925 23:48:15.419634 139756425134720 dispatch.py:271] Finished tracing + transforming _threefry_split_original for pjit in 0.002780914306640625 sec\n",
      "W0925 23:48:15.421711 139756425134720 pxla.py:1764] Compiling _threefry_split_original for with global shapes and types [ShapedArray(uint32[2,2])]. Argument mapping: (GSPMDSharding({replicated}),).\n",
      "W0925 23:48:15.424478 139756425134720 dispatch.py:271] Finished jaxpr to MLIR module conversion jit(_threefry_split_original) in 0.002638578414916992 sec\n",
      "W0925 23:48:15.491412 139756425134720 dispatch.py:271] Finished XLA compilation of jit(_threefry_split_original) in 0.06667590141296387 sec\n",
      "W0925 23:48:15.492782 139756425134720 dispatch.py:271] Finished tracing + transforming jit(transpose) in 0.00021910667419433594 sec\n",
      "W0925 23:48:15.493061 139756425134720 pxla.py:1764] Compiling transpose for with global shapes and types [ShapedArray(uint32[2,3,2])]. Argument mapping: (GSPMDSharding({replicated}),).\n",
      "W0925 23:48:15.494462 139756425134720 dispatch.py:271] Finished jaxpr to MLIR module conversion jit(transpose) in 0.001272439956665039 sec\n",
      "W0925 23:48:15.542198 139756425134720 dispatch.py:271] Finished XLA compilation of jit(transpose) in 0.04744720458984375 sec\n",
      "W0925 23:48:15.543775 139756425134720 dispatch.py:271] Finished tracing + transforming jit(reshape) in 0.0002110004425048828 sec\n",
      "W0925 23:48:15.544035 139756425134720 pxla.py:1764] Compiling reshape for with global shapes and types [ShapedArray(uint32[3,2,2])]. Argument mapping: (GSPMDSharding({replicated}),).\n",
      "W0925 23:48:15.545179 139756425134720 dispatch.py:271] Finished jaxpr to MLIR module conversion jit(reshape) in 0.0010230541229248047 sec\n",
      "W0925 23:48:15.557319 139756425134720 dispatch.py:271] Finished XLA compilation of jit(reshape) in 0.011870622634887695 sec\n",
      "W0925 23:48:15.558753 139756425134720 dispatch.py:271] Finished tracing + transforming _unstack for pjit in 0.0006248950958251953 sec\n",
      "W0925 23:48:15.559314 139756425134720 pxla.py:1764] Compiling _unstack for with global shapes and types [ShapedArray(uint32[3,4])]. Argument mapping: (GSPMDSharding({replicated}),).\n",
      "W0925 23:48:15.561166 139756425134720 dispatch.py:271] Finished jaxpr to MLIR module conversion jit(_unstack) in 0.0017344951629638672 sec\n",
      "W0925 23:48:15.620489 139756425134720 dispatch.py:271] Finished XLA compilation of jit(_unstack) in 0.059038400650024414 sec\n",
      "I0925 23:48:15.621668 139756425134720 executors.py:246] train prng seed: [3002578388 2149995687 3002578388 2149995687]\n",
      "I0925 23:48:15.622061 139756425134720 executors.py:247] eval prng seed: [2702578219 1490354220 2702578219 1490354220]\n",
      "I0925 23:48:15.622332 139756425134720 executors.py:248] decode prng seed: [ 117142398 1914648840  117142398 1914648840]\n",
      "W0925 23:48:15.623496 139756425134720 dispatch.py:271] Finished tracing + transforming _identity for pjit in 0.0003733634948730469 sec\n",
      "W0925 23:48:15.623955 139756425134720 pxla.py:1764] Compiling _identity for with global shapes and types [ShapedArray(uint32[4])]. Argument mapping: (GSPMDSharding({replicated}),).\n",
      "W0925 23:48:15.624988 139756425134720 dispatch.py:271] Finished jaxpr to MLIR module conversion pjit(_identity) in 0.0009119510650634766 sec\n",
      "W0925 23:48:15.637245 139756425134720 dispatch.py:271] Finished XLA compilation of pjit(_identity) in 0.011992454528808594 sec\n",
      "W0925 23:48:15.638774 139756425134720 dispatch.py:271] Finished tracing + transforming _identity for pjit in 0.00018095970153808594 sec\n",
      "W0925 23:48:15.639291 139756425134720 pxla.py:1764] Compiling _identity for with global shapes and types [ShapedArray(uint32[4])]. Argument mapping: (GSPMDSharding({replicated}),).\n",
      "W0925 23:48:15.640255 139756425134720 dispatch.py:271] Finished jaxpr to MLIR module conversion pjit(_identity) in 0.0008351802825927734 sec\n",
      "W0925 23:48:15.654180 139756425134720 dispatch.py:271] Finished XLA compilation of pjit(_identity) in 0.01368856430053711 sec\n",
      "W0925 23:48:15.655621 139756425134720 dispatch.py:271] Finished tracing + transforming _identity for pjit in 0.00016760826110839844 sec\n",
      "W0925 23:48:15.656116 139756425134720 pxla.py:1764] Compiling _identity for with global shapes and types [ShapedArray(uint32[4])]. Argument mapping: (GSPMDSharding({replicated}),).\n",
      "W0925 23:48:15.657061 139756425134720 dispatch.py:271] Finished jaxpr to MLIR module conversion pjit(_identity) in 0.0008234977722167969 sec\n",
      "W0925 23:48:15.668391 139756425134720 dispatch.py:271] Finished XLA compilation of pjit(_identity) in 0.011092901229858398 sec\n",
      "I0925 23:48:15.668960 139756425134720 executors.py:267] Starting executor.\n",
      "I0925 23:48:15.669229 139756425134720 executors.py:331] Model initial global_step=0\n",
      "I0925 23:48:15.669285 139756425134720 executors.py:338] [PAX STATUS]: Starting training loop.\n",
      "I0925 23:48:15.669317 139756425134720 programs.py:246] [PAX STATUS]: Setting up BaseTrainProgram.\n",
      "I0925 23:48:15.669819 139756425134720 summary_utils.py:281] Opening SummaryWriter `/tmp/dali_pax_logs/summaries/train`...\n",
      "I0925 23:48:15.679284 139756425134720 py_utils.py:339] Starting sync_global_devices Start training loop from step: 0 across 1 devices globally\n",
      "W0925 23:48:15.681041 139756425134720 dispatch.py:271] Finished tracing + transforming _reduce_sum for pjit in 0.0005803108215332031 sec\n",
      "W0925 23:48:15.681409 139756425134720 dispatch.py:271] Finished tracing + transforming _psum for pjit in 0.0012431144714355469 sec\n",
      "W0925 23:48:15.681838 139756425134720 pxla.py:1764] Compiling _psum for with global shapes and types [ShapedArray(uint32[1])]. Argument mapping: (GSPMDSharding({replicated}),).\n",
      "W0925 23:48:15.683357 139756425134720 dispatch.py:271] Finished jaxpr to MLIR module conversion jit(_psum) in 0.0014042854309082031 sec\n",
      "W0925 23:48:15.695917 139756425134720 dispatch.py:271] Finished XLA compilation of jit(_psum) in 0.012311697006225586 sec\n",
      "I0925 23:48:15.696871 139756425134720 py_utils.py:342] Finished sync_global_devices Start training loop from step: 0 across 1 devices globally\n",
      "I0925 23:48:15.855737 139756425134720 executors.py:381] [PAX STATUS]: Beginning step `0`.\n",
      "W0925 23:48:15.858577 139756425134720 dispatch.py:271] Finished tracing + transforming _reduce_sum for pjit in 0.0005762577056884766 sec\n",
      "W0925 23:48:15.859284 139756425134720 dispatch.py:271] Finished tracing + transforming _psum for pjit in 0.001562356948852539 sec\n",
      "W0925 23:48:15.859819 139756425134720 pxla.py:1764] Compiling _psum for with global shapes and types [ShapedArray(int32[1]), ShapedArray(int32[1])]. Argument mapping: (GSPMDSharding({replicated}), GSPMDSharding({replicated})).\n",
      "W0925 23:48:15.861690 139756425134720 dispatch.py:271] Finished jaxpr to MLIR module conversion jit(_psum) in 0.0017459392547607422 sec\n",
      "W0925 23:48:15.910193 139756425134720 dispatch.py:271] Finished XLA compilation of jit(_psum) in 0.04825425148010254 sec\n",
      "I0925 23:48:15.914139 139756425134720 checkpointer.py:67] Saving item to /tmp/dali_pax_logs/checkpoints/checkpoint_00000000.orbax-checkpoint-tmp-1695678495856706/state.\n",
      "I0925 23:48:15.978491 139756425134720 utils.py:518] Renaming /tmp/dali_pax_logs/checkpoints/checkpoint_00000000.orbax-checkpoint-tmp-1695678495856706/state.orbax-checkpoint-tmp-1695678495914262 to /tmp/dali_pax_logs/checkpoints/checkpoint_00000000.orbax-checkpoint-tmp-1695678495856706/state\n",
      "I0925 23:48:15.978702 139756425134720 utils.py:562] Finished saving checkpoint to `/tmp/dali_pax_logs/checkpoints/checkpoint_00000000.orbax-checkpoint-tmp-1695678495856706/state`.\n",
      "I0925 23:48:15.979804 139756425134720 checkpointer.py:67] Saving item to /tmp/dali_pax_logs/checkpoints/checkpoint_00000000.orbax-checkpoint-tmp-1695678495856706/metadata.\n",
      "I0925 23:48:15.987265 139756425134720 utils.py:518] Renaming /tmp/dali_pax_logs/checkpoints/checkpoint_00000000.orbax-checkpoint-tmp-1695678495856706/metadata.orbax-checkpoint-tmp-1695678495979917 to /tmp/dali_pax_logs/checkpoints/checkpoint_00000000.orbax-checkpoint-tmp-1695678495856706/metadata\n",
      "I0925 23:48:15.987414 139756425134720 utils.py:562] Finished saving checkpoint to `/tmp/dali_pax_logs/checkpoints/checkpoint_00000000.orbax-checkpoint-tmp-1695678495856706/metadata`.\n",
      "I0925 23:48:15.988582 139756425134720 utils.py:518] Renaming /tmp/dali_pax_logs/checkpoints/checkpoint_00000000.orbax-checkpoint-tmp-1695678495856706 to /tmp/dali_pax_logs/checkpoints/checkpoint_00000000\n",
      "I0925 23:48:15.989599 139756425134720 py_utils.py:1015] [PAX STATUS]: Starting timer for <run> @ <.../paxml/executors.py:406>\n",
      "I0925 23:48:15.989663 139756425134720 programs.py:304] [PAX STATUS]:  Retrieving inputs.\n",
      "I0925 23:48:15.989699 139756425134720 py_utils.py:1015] [PAX STATUS]: Starting timer for <get_next_padded> @ <.../paxml/programs.py:305>\n",
      "I0925 23:48:15.989734 139756425134720 py_utils.py:1025] [PAX STATUS]: Elapsed time for <get_next_padded>: 0.00 seconds  (@ <.../paxml/programs.py:305>)\n",
      "I0925 23:48:15.989787 139756425134720 partitioning.py:806] Checking input spec [pjit partitioner]\n",
      "I0925 23:48:15.989872 139756425134720 py_utils.py:1015] [PAX STATUS]: Starting timer for <partition> @ <.../paxml/programs.py:584>\n",
      "I0925 23:48:15.989910 139756425134720 trainer_lib.py:1606] get_input_partition_specs from mesh_axis_names=('replica', 'data', 'mdl') and inputs_shape_dtype={'inputs': ShapeDtypeStruct(shape=(1024, 28, 28, 1), dtype=float32), 'labels': ShapeDtypeStruct(shape=(1024,), dtype=int32)}\n",
      "I0925 23:48:15.989990 139756425134720 partitioning.py:931] step_fn inputs_partition_spec={'inputs': PartitionSpec(('replica', 'data', 'mdl'), None, None, None), 'labels': PartitionSpec(('replica', 'data', 'mdl'),)}\n",
      "I0925 23:48:15.990173 139756425134720 partitioning.py:1027] step_fn fn_in_partition_specs=(TrainState(step=PartitionSpec(), mdl_vars={'params': {'network': {'conv1': {'b': PartitionSpec(None,), 'w': PartitionSpec(None, None, None, None)}, 'conv2': {'b': PartitionSpec(None,), 'w': PartitionSpec(None, None, None, None)}, 'dense_0': {'bias': {'b': PartitionSpec(None,)}, 'linear': {'w': PartitionSpec(None, None)}}, 'dense_1': {'w': PartitionSpec(None, None)}, 'dense_2': {'b': PartitionSpec(None,)}}}}, opt_states=[({'count': PartitionSpec()}, {'count': PartitionSpec()}, (TraceState(trace={'params': {'network': {'conv1': {'b': PartitionSpec(None,), 'w': PartitionSpec(None, None, None, None)}, 'conv2': {'b': PartitionSpec(None,), 'w': PartitionSpec(None, None, None, None)}, 'dense_0': {'bias': {'b': PartitionSpec(None,)}, 'linear': {'w': PartitionSpec(None, None)}}, 'dense_1': {'w': PartitionSpec(None, None)}, 'dense_2': {'b': PartitionSpec(None,)}}}}), ScaleByScheduleState(count=PartitionSpec())), {'count': PartitionSpec()})], extra_state=()), PartitionSpec(), {'inputs': PartitionSpec(('replica', 'data', 'mdl'), None, None, None), 'labels': PartitionSpec(('replica', 'data', 'mdl'),)})\n",
      "I0925 23:48:15.990273 139756425134720 partitioning.py:1028] step_fn fn_out_partition_specs=(TrainState(step=PartitionSpec(), mdl_vars={'params': {'network': {'conv1': {'b': PartitionSpec(None,), 'w': PartitionSpec(None, None, None, None)}, 'conv2': {'b': PartitionSpec(None,), 'w': PartitionSpec(None, None, None, None)}, 'dense_0': {'bias': {'b': PartitionSpec(None,)}, 'linear': {'w': PartitionSpec(None, None)}}, 'dense_1': {'w': PartitionSpec(None, None)}, 'dense_2': {'b': PartitionSpec(None,)}}}}, opt_states=[({'count': PartitionSpec()}, {'count': PartitionSpec()}, (TraceState(trace={'params': {'network': {'conv1': {'b': PartitionSpec(None,), 'w': PartitionSpec(None, None, None, None)}, 'conv2': {'b': PartitionSpec(None,), 'w': PartitionSpec(None, None, None, None)}, 'dense_0': {'bias': {'b': PartitionSpec(None,)}, 'linear': {'w': PartitionSpec(None, None)}}, 'dense_1': {'w': PartitionSpec(None, None)}, 'dense_2': {'b': PartitionSpec(None,)}}}}), ScaleByScheduleState(count=PartitionSpec())), {'count': PartitionSpec()})], extra_state=()), PartitionSpec())\n",
      "I0925 23:48:15.990653 139756425134720 py_utils.py:1025] [PAX STATUS]: Elapsed time for <partition>: 0.00 seconds  (@ <.../paxml/programs.py:584>)\n",
      "W0925 23:48:15.991151 139756425134720 dispatch.py:271] Finished tracing + transforming swapaxes for pjit in 0.0002262592315673828 sec\n",
      "W0925 23:48:15.992334 139756425134720 dispatch.py:271] Finished tracing + transforming swapaxes for pjit in 0.00019621849060058594 sec\n",
      "I0925 23:48:15.993456 139756425134720 programs.py:316] [PAX STATUS]:  Retrieved inputs.\n",
      "I0925 23:48:15.993532 139756425134720 programs.py:327] [PAX STATUS]:  Performing train_step().\n",
      "I0925 23:48:15.995073 139756425134720 trainer_lib.py:1563] No resharding of input as mapping_dict is None.\n",
      "I0925 23:48:15.995129 139756425134720 trainer_lib.py:1563] No resharding of input as mapping_dict is None.\n",
      "I0925 23:48:15.995880 139756425134720 trainer_lib.py:945] Bprop included var: params/network/conv1/b\n",
      "I0925 23:48:15.995931 139756425134720 trainer_lib.py:945] Bprop included var: params/network/conv1/w\n",
      "I0925 23:48:15.995960 139756425134720 trainer_lib.py:945] Bprop included var: params/network/conv2/b\n",
      "I0925 23:48:15.995986 139756425134720 trainer_lib.py:945] Bprop included var: params/network/conv2/w\n",
      "I0925 23:48:15.996009 139756425134720 trainer_lib.py:945] Bprop included var: params/network/dense_0/bias/b\n",
      "I0925 23:48:15.996032 139756425134720 trainer_lib.py:945] Bprop included var: params/network/dense_0/linear/w\n",
      "I0925 23:48:15.996053 139756425134720 trainer_lib.py:945] Bprop included var: params/network/dense_1/w\n",
      "I0925 23:48:15.996075 139756425134720 trainer_lib.py:945] Bprop included var: params/network/dense_2/b\n",
      "I0925 23:48:16.021489 139756425134720 base_layer.py:530] Sets tensor_split_dims_mapping of a param of shape (3, 3, 1, 32) to (-1, -1, -1, -1)\n",
      "I0925 23:48:16.021798 139756425134720 base_layer.py:635] Creating var /network/conv1/w with shape=(3, 3, 1, 32), dtype=<class 'jax.numpy.float32'>, init method=xavier and scale=1.000001\n",
      "I0925 23:48:16.023114 139756425134720 base_layer.py:635] Creating var /network/conv1/b with shape=[32], dtype=<class 'jax.numpy.float32'>, init method=constant and scale=0.0\n",
      "W0925 23:48:16.030816 139756425134720 dispatch.py:271] Finished tracing + transforming fn for pjit in 0.00035858154296875 sec\n",
      "I0925 23:48:16.034605 139756425134720 base_layer.py:530] Sets tensor_split_dims_mapping of a param of shape (3, 3, 32, 64) to (-1, -1, -1, -1)\n",
      "I0925 23:48:16.034857 139756425134720 base_layer.py:635] Creating var /network/conv2/w with shape=(3, 3, 32, 64), dtype=<class 'jax.numpy.float32'>, init method=xavier and scale=1.000001\n",
      "I0925 23:48:16.035990 139756425134720 base_layer.py:635] Creating var /network/conv2/b with shape=[64], dtype=<class 'jax.numpy.float32'>, init method=constant and scale=0.0\n",
      "W0925 23:48:16.043203 139756425134720 dispatch.py:271] Finished tracing + transforming fn for pjit in 0.00034546852111816406 sec\n",
      "I0925 23:48:16.051873 139756425134720 base_layer.py:530] Sets tensor_split_dims_mapping of a param of shape [3136, 256] to (-1, -1)\n",
      "I0925 23:48:16.052134 139756425134720 base_layer.py:635] Creating var /network/dense_0/linear/w with shape=[3136, 256], dtype=<class 'jax.numpy.float32'>, init method=xavier and scale=1.000001\n",
      "I0925 23:48:16.057253 139756425134720 base_layer.py:530] Sets tensor_split_dims_mapping of a param of shape [256] to (-1,)\n",
      "I0925 23:48:16.057495 139756425134720 base_layer.py:635] Creating var /network/dense_0/bias/b with shape=[256], dtype=<class 'jax.numpy.float32'>, init method=constant and scale=0.0\n",
      "W0925 23:48:16.061138 139756425134720 dispatch.py:271] Finished tracing + transforming fn for pjit in 0.0003190040588378906 sec\n",
      "I0925 23:48:16.061972 139756425134720 base_layer.py:530] Sets tensor_split_dims_mapping of a param of shape [256, 10] to (-1, -1)\n",
      "I0925 23:48:16.062220 139756425134720 base_layer.py:635] Creating var /network/dense_1/w with shape=[256, 10], dtype=<class 'jax.numpy.float32'>, init method=xavier and scale=1.000001\n",
      "I0925 23:48:16.066978 139756425134720 base_layer.py:530] Sets tensor_split_dims_mapping of a param of shape [10] to (-1,)\n",
      "I0925 23:48:16.067217 139756425134720 base_layer.py:635] Creating var /network/dense_2/b with shape=[10], dtype=<class 'jax.numpy.float32'>, init method=constant and scale=0.0\n",
      "W0925 23:48:16.092633 139756425134720 dispatch.py:271] Finished tracing + transforming fn for pjit in 0.00035643577575683594 sec\n",
      "W0925 23:48:16.093526 139756425134720 dispatch.py:271] Finished tracing + transforming fn for pjit in 0.0002799034118652344 sec\n",
      "W0925 23:48:16.094388 139756425134720 dispatch.py:271] Finished tracing + transforming _reduce_sum for pjit in 0.0004482269287109375 sec\n",
      "W0925 23:48:16.095126 139756425134720 dispatch.py:271] Finished tracing + transforming fn for pjit in 0.0003211498260498047 sec\n",
      "W0925 23:48:16.095907 139756425134720 dispatch.py:271] Finished tracing + transforming _reduce_sum for pjit in 0.00034546852111816406 sec\n",
      "W0925 23:48:16.096519 139756425134720 dispatch.py:271] Finished tracing + transforming fn for pjit in 0.00027298927307128906 sec\n",
      "W0925 23:48:16.097262 139756425134720 dispatch.py:271] Finished tracing + transforming _reduce_sum for pjit in 0.0003464221954345703 sec\n",
      "W0925 23:48:16.097981 139756425134720 dispatch.py:271] Finished tracing + transforming fn for pjit in 0.0003783702850341797 sec\n",
      "W0925 23:48:16.098793 139756425134720 dispatch.py:271] Finished tracing + transforming _reduce_sum for pjit in 0.0003452301025390625 sec\n",
      "W0925 23:48:16.099409 139756425134720 dispatch.py:271] Finished tracing + transforming fn for pjit in 0.0002779960632324219 sec\n",
      "W0925 23:48:16.100088 139756425134720 dispatch.py:271] Finished tracing + transforming _reduce_sum for pjit in 0.0003161430358886719 sec\n",
      "W0925 23:48:16.100657 139756425134720 dispatch.py:271] Finished tracing + transforming fn for pjit in 0.0002536773681640625 sec\n",
      "W0925 23:48:16.101397 139756425134720 dispatch.py:271] Finished tracing + transforming _reduce_sum for pjit in 0.00034332275390625 sec\n",
      "W0925 23:48:16.102018 139756425134720 dispatch.py:271] Finished tracing + transforming fn for pjit in 0.0002808570861816406 sec\n",
      "W0925 23:48:16.102762 139756425134720 dispatch.py:271] Finished tracing + transforming _reduce_sum for pjit in 0.0003478527069091797 sec\n",
      "W0925 23:48:16.103373 139756425134720 dispatch.py:271] Finished tracing + transforming fn for pjit in 0.00027060508728027344 sec\n",
      "W0925 23:48:16.104588 139756425134720 dispatch.py:271] Finished tracing + transforming _reduce_sum for pjit in 0.0008263587951660156 sec\n",
      "W0925 23:48:16.105928 139756425134720 dispatch.py:271] Finished tracing + transforming _reduce_sum for pjit in 0.00033402442932128906 sec\n",
      "W0925 23:48:16.106506 139756425134720 dispatch.py:271] Finished tracing + transforming <lambda> for pjit in 0.00021767616271972656 sec\n",
      "W0925 23:48:16.107088 139756425134720 dispatch.py:271] Finished tracing + transforming isfinite for pjit in 0.00018787384033203125 sec\n",
      "W0925 23:48:16.107729 139756425134720 dispatch.py:271] Finished tracing + transforming _reduce_all for pjit in 0.0003402233123779297 sec\n",
      "W0925 23:48:16.113727 139756425134720 dispatch.py:271] Finished tracing + transforming fn for pjit in 0.00029778480529785156 sec\n",
      "W0925 23:48:16.114904 139756425134720 dispatch.py:271] Finished tracing + transforming fn for pjit in 0.000324249267578125 sec\n",
      "W0925 23:48:16.115608 139756425134720 dispatch.py:271] Finished tracing + transforming fn for pjit in 0.00027561187744140625 sec\n",
      "W0925 23:48:16.116614 139756425134720 dispatch.py:271] Finished tracing + transforming fn for pjit in 0.000354766845703125 sec\n",
      "W0925 23:48:16.117332 139756425134720 dispatch.py:271] Finished tracing + transforming fn for pjit in 0.0003376007080078125 sec\n",
      "W0925 23:48:16.118002 139756425134720 dispatch.py:271] Finished tracing + transforming fn for pjit in 0.0002696514129638672 sec\n",
      "W0925 23:48:16.118941 139756425134720 dispatch.py:271] Finished tracing + transforming fn for pjit in 0.00027489662170410156 sec\n",
      "W0925 23:48:16.119622 139756425134720 dispatch.py:271] Finished tracing + transforming fn for pjit in 0.00029850006103515625 sec\n",
      "W0925 23:48:16.120245 139756425134720 dispatch.py:271] Finished tracing + transforming fn for pjit in 0.00025391578674316406 sec\n",
      "W0925 23:48:16.121109 139756425134720 dispatch.py:271] Finished tracing + transforming fn for pjit in 0.000263214111328125 sec\n",
      "W0925 23:48:16.122020 139756425134720 dispatch.py:271] Finished tracing + transforming fn for pjit in 0.00029158592224121094 sec\n",
      "W0925 23:48:16.122772 139756425134720 dispatch.py:271] Finished tracing + transforming fn for pjit in 0.00037217140197753906 sec\n",
      "W0925 23:48:16.123446 139756425134720 dispatch.py:271] Finished tracing + transforming fn for pjit in 0.000270843505859375 sec\n",
      "W0925 23:48:16.124499 139756425134720 dispatch.py:271] Finished tracing + transforming fn for pjit in 0.00033926963806152344 sec\n",
      "W0925 23:48:16.125284 139756425134720 dispatch.py:271] Finished tracing + transforming fn for pjit in 0.000270843505859375 sec\n",
      "W0925 23:48:16.125972 139756425134720 dispatch.py:271] Finished tracing + transforming fn for pjit in 0.00027298927307128906 sec\n",
      "W0925 23:48:16.126695 139756425134720 dispatch.py:271] Finished tracing + transforming fn for pjit in 0.00027441978454589844 sec\n",
      "W0925 23:48:16.127385 139756425134720 dispatch.py:271] Finished tracing + transforming fn for pjit in 0.000274658203125 sec\n",
      "W0925 23:48:16.128062 139756425134720 dispatch.py:271] Finished tracing + transforming fn for pjit in 0.0002570152282714844 sec\n",
      "W0925 23:48:16.128790 139756425134720 dispatch.py:271] Finished tracing + transforming fn for pjit in 0.0003390312194824219 sec\n",
      "W0925 23:48:16.129486 139756425134720 dispatch.py:271] Finished tracing + transforming fn for pjit in 0.0002770423889160156 sec\n",
      "W0925 23:48:16.130221 139756425134720 dispatch.py:271] Finished tracing + transforming fn for pjit in 0.00031256675720214844 sec\n",
      "W0925 23:48:16.131137 139756425134720 dispatch.py:271] Finished tracing + transforming fn for pjit in 0.0003573894500732422 sec\n",
      "W0925 23:48:16.131864 139756425134720 dispatch.py:271] Finished tracing + transforming fn for pjit in 0.0002741813659667969 sec\n",
      "W0925 23:48:16.132704 139756425134720 dispatch.py:271] Finished tracing + transforming _broadcast_arrays for pjit in 0.00016260147094726562 sec\n",
      "W0925 23:48:16.133019 139756425134720 dispatch.py:271] Finished tracing + transforming _where for pjit in 0.0007679462432861328 sec\n",
      "W0925 23:48:16.134858 139756425134720 dispatch.py:271] Finished tracing + transforming _broadcast_arrays for pjit in 0.000392913818359375 sec\n",
      "W0925 23:48:16.135243 139756425134720 dispatch.py:271] Finished tracing + transforming _where for pjit in 0.0010223388671875 sec\n",
      "W0925 23:48:16.136269 139756425134720 dispatch.py:271] Finished tracing + transforming _broadcast_arrays for pjit in 0.00031375885009765625 sec\n",
      "W0925 23:48:16.136651 139756425134720 dispatch.py:271] Finished tracing + transforming _where for pjit in 0.0009307861328125 sec\n",
      "W0925 23:48:16.137614 139756425134720 dispatch.py:271] Finished tracing + transforming _broadcast_arrays for pjit in 0.0002734661102294922 sec\n",
      "W0925 23:48:16.137964 139756425134720 dispatch.py:271] Finished tracing + transforming _where for pjit in 0.0008406639099121094 sec\n",
      "W0925 23:48:16.138999 139756425134720 dispatch.py:271] Finished tracing + transforming _broadcast_arrays for pjit in 0.00030803680419921875 sec\n",
      "W0925 23:48:16.139380 139756425134720 dispatch.py:271] Finished tracing + transforming _where for pjit in 0.0009317398071289062 sec\n",
      "W0925 23:48:16.140419 139756425134720 dispatch.py:271] Finished tracing + transforming _broadcast_arrays for pjit in 0.0002741813659667969 sec\n",
      "W0925 23:48:16.140792 139756425134720 dispatch.py:271] Finished tracing + transforming _where for pjit in 0.0009441375732421875 sec\n",
      "W0925 23:48:16.141797 139756425134720 dispatch.py:271] Finished tracing + transforming _broadcast_arrays for pjit in 0.0002961158752441406 sec\n",
      "W0925 23:48:16.142189 139756425134720 dispatch.py:271] Finished tracing + transforming _where for pjit in 0.0009312629699707031 sec\n",
      "W0925 23:48:16.143257 139756425134720 dispatch.py:271] Finished tracing + transforming _broadcast_arrays for pjit in 0.0003247261047363281 sec\n",
      "W0925 23:48:16.143675 139756425134720 dispatch.py:271] Finished tracing + transforming _where for pjit in 0.0009970664978027344 sec\n",
      "W0925 23:48:16.144784 139756425134720 dispatch.py:271] Finished tracing + transforming _broadcast_arrays for pjit in 0.00036263465881347656 sec\n",
      "W0925 23:48:16.145173 139756425134720 dispatch.py:271] Finished tracing + transforming _where for pjit in 0.0009980201721191406 sec\n",
      "W0925 23:48:16.145948 139756425134720 dispatch.py:271] Finished tracing + transforming _where for pjit in 0.0004291534423828125 sec\n",
      "W0925 23:48:16.160613 139756425134720 dispatch.py:271] Finished tracing + transforming fn for pjit in 0.0003535747528076172 sec\n",
      "W0925 23:48:16.161354 139756425134720 dispatch.py:271] Finished tracing + transforming absolute for pjit in 0.00019502639770507812 sec\n",
      "W0925 23:48:16.162057 139756425134720 dispatch.py:271] Finished tracing + transforming _reduce_max for pjit in 0.00035858154296875 sec\n",
      "W0925 23:48:16.162734 139756425134720 dispatch.py:271] Finished tracing + transforming <lambda> for pjit in 0.00031065940856933594 sec\n",
      "W0925 23:48:16.163441 139756425134720 dispatch.py:271] Finished tracing + transforming true_divide for pjit in 0.0002765655517578125 sec\n",
      "W0925 23:48:16.164518 139756425134720 dispatch.py:271] Finished tracing + transforming fn for pjit in 0.0002753734588623047 sec\n",
      "W0925 23:48:16.165430 139756425134720 dispatch.py:271] Finished tracing + transforming absolute for pjit in 0.00017976760864257812 sec\n",
      "W0925 23:48:16.166094 139756425134720 dispatch.py:271] Finished tracing + transforming _reduce_max for pjit in 0.0003249645233154297 sec\n",
      "W0925 23:48:16.167056 139756425134720 dispatch.py:271] Finished tracing + transforming true_divide for pjit in 0.00030684471130371094 sec\n",
      "W0925 23:48:16.168681 139756425134720 dispatch.py:271] Finished tracing + transforming absolute for pjit in 0.00018310546875 sec\n",
      "W0925 23:48:16.169315 139756425134720 dispatch.py:271] Finished tracing + transforming _reduce_max for pjit in 0.0002834796905517578 sec\n",
      "W0925 23:48:16.170166 139756425134720 dispatch.py:271] Finished tracing + transforming true_divide for pjit in 0.00028514862060546875 sec\n",
      "W0925 23:48:16.171767 139756425134720 dispatch.py:271] Finished tracing + transforming absolute for pjit in 0.00019288063049316406 sec\n",
      "W0925 23:48:16.172458 139756425134720 dispatch.py:271] Finished tracing + transforming _reduce_max for pjit in 0.0003135204315185547 sec\n",
      "W0925 23:48:16.173409 139756425134720 dispatch.py:271] Finished tracing + transforming true_divide for pjit in 0.0003402233123779297 sec\n",
      "W0925 23:48:16.175025 139756425134720 dispatch.py:271] Finished tracing + transforming absolute for pjit in 0.00019478797912597656 sec\n",
      "W0925 23:48:16.175695 139756425134720 dispatch.py:271] Finished tracing + transforming _reduce_max for pjit in 0.000308990478515625 sec\n",
      "W0925 23:48:16.176585 139756425134720 dispatch.py:271] Finished tracing + transforming true_divide for pjit in 0.0002732276916503906 sec\n",
      "W0925 23:48:16.178206 139756425134720 dispatch.py:271] Finished tracing + transforming absolute for pjit in 0.00020933151245117188 sec\n",
      "W0925 23:48:16.179425 139756425134720 dispatch.py:271] Finished tracing + transforming _reduce_max for pjit in 0.0008509159088134766 sec\n",
      "W0925 23:48:16.180362 139756425134720 dispatch.py:271] Finished tracing + transforming true_divide for pjit in 0.0002751350402832031 sec\n",
      "W0925 23:48:16.181965 139756425134720 dispatch.py:271] Finished tracing + transforming absolute for pjit in 0.00019621849060058594 sec\n",
      "W0925 23:48:16.182686 139756425134720 dispatch.py:271] Finished tracing + transforming _reduce_max for pjit in 0.00033354759216308594 sec\n",
      "W0925 23:48:16.183617 139756425134720 dispatch.py:271] Finished tracing + transforming true_divide for pjit in 0.0002753734588623047 sec\n",
      "W0925 23:48:16.185268 139756425134720 dispatch.py:271] Finished tracing + transforming absolute for pjit in 0.0002541542053222656 sec\n",
      "W0925 23:48:16.185934 139756425134720 dispatch.py:271] Finished tracing + transforming _reduce_max for pjit in 0.00030159950256347656 sec\n",
      "W0925 23:48:16.186829 139756425134720 dispatch.py:271] Finished tracing + transforming true_divide for pjit in 0.00027060508728027344 sec\n",
      "W0925 23:48:16.188891 139756425134720 dispatch.py:271] Finished tracing + transforming true_divide for pjit in 0.0002677440643310547 sec\n",
      "W0925 23:48:16.199456 139756425134720 dispatch.py:271] Finished tracing + transforming _wrapped_step_fn for pjit in 0.20521306991577148 sec\n",
      "W0925 23:48:16.206932 139756425134720 pxla.py:1764] Compiling _wrapped_step_fn for with global shapes and types [ShapedArray(uint32[]), ShapedArray(float32[32]), ShapedArray(float32[3,3,1,32]), ShapedArray(float32[64]), ShapedArray(float32[3,3,32,64]), ShapedArray(float32[256]), ShapedArray(float32[3136,256]), ShapedArray(float32[256,10]), ShapedArray(float32[10]), ShapedArray(int32[]), ShapedArray(int32[]), ShapedArray(float32[32]), ShapedArray(float32[3,3,1,32]), ShapedArray(float32[64]), ShapedArray(float32[3,3,32,64]), ShapedArray(float32[256]), ShapedArray(float32[3136,256]), ShapedArray(float32[256,10]), ShapedArray(float32[10]), ShapedArray(int32[]), ShapedArray(int32[]), ShapedArray(float32[1024,28,28,1]), ShapedArray(int32[1024])]. Argument mapping: (GSPMDSharding({replicated}), GSPMDSharding({replicated}), GSPMDSharding({replicated}), GSPMDSharding({replicated}), GSPMDSharding({replicated}), GSPMDSharding({replicated}), GSPMDSharding({replicated}), GSPMDSharding({replicated}), GSPMDSharding({replicated}), GSPMDSharding({replicated}), GSPMDSharding({replicated}), GSPMDSharding({replicated}), GSPMDSharding({replicated}), GSPMDSharding({replicated}), GSPMDSharding({replicated}), GSPMDSharding({replicated}), GSPMDSharding({replicated}), GSPMDSharding({replicated}), GSPMDSharding({replicated}), GSPMDSharding({replicated}), GSPMDSharding({replicated}), GSPMDSharding({devices=[1,1,1,1]0}), GSPMDSharding({devices=[1]0})).\n",
      "W0925 23:48:16.292077 139756425134720 dispatch.py:271] Finished jaxpr to MLIR module conversion pjit(_wrapped_step_fn) in 0.08473491668701172 sec\n",
      "W0925 23:48:18.110601 139756425134720 dispatch.py:271] Finished XLA compilation of pjit(_wrapped_step_fn) in 1.818160057067871 sec\n",
      "I0925 23:48:18.114931 139756425134720 programs.py:345] [PAX STATUS]: train_step() took 2.121305 seconds.\n",
      "I0925 23:48:18.115055 139756425134720 programs.py:357] [PAX STATUS]:  Writing summaries (attempt).\n",
      "I0925 23:48:18.115191 139756425134720 py_utils.py:1025] [PAX STATUS]: Elapsed time for <run>: 2.13 seconds  (@ <.../paxml/executors.py:406>)\n",
      "I0925 23:48:18.115259 139756425134720 executors.py:472] [PAX STATUS]: Step `0` completed.\n",
      "I0925 23:48:18.115306 139756425134720 executors.py:381] [PAX STATUS]: Beginning step `1`.\n",
      "I0925 23:48:18.115360 139756425134720 py_utils.py:1015] [PAX STATUS]: Starting timer for <run> @ <.../paxml/executors.py:406>\n",
      "I0925 23:48:18.115388 139756425134720 programs.py:304] [PAX STATUS]:  Retrieving inputs.\n",
      "I0925 23:48:18.118648 139756425134720 programs.py:316] [PAX STATUS]:  Retrieved inputs.\n",
      "I0925 23:48:18.118759 139756425134720 programs.py:327] [PAX STATUS]:  Performing train_step().\n",
      "I0925 23:48:18.119563 139756425134720 programs.py:345] [PAX STATUS]: train_step() took 0.000737 seconds.\n",
      "I0925 23:48:18.119635 139756425134720 programs.py:357] [PAX STATUS]:  Writing summaries (attempt).\n",
      "I0925 23:48:18.119724 139756425134720 py_utils.py:1025] [PAX STATUS]: Elapsed time for <run>: 0.00 seconds  (@ <.../paxml/executors.py:406>)\n",
      "I0925 23:48:18.119773 139756425134720 executors.py:472] [PAX STATUS]: Step `1` completed.\n",
      "I0925 23:48:18.119804 139756425134720 executors.py:381] [PAX STATUS]: Beginning step `2`.\n",
      "I0925 23:48:18.119851 139756425134720 py_utils.py:1015] [PAX STATUS]: Starting timer for <run> @ <.../paxml/executors.py:406>\n",
      "I0925 23:48:18.119879 139756425134720 programs.py:304] [PAX STATUS]:  Retrieving inputs.\n",
      "I0925 23:48:18.122276 139756425134720 programs.py:316] [PAX STATUS]:  Retrieved inputs.\n",
      "I0925 23:48:18.122376 139756425134720 programs.py:327] [PAX STATUS]:  Performing train_step().\n",
      "I0925 23:48:18.123091 139756425134720 programs.py:345] [PAX STATUS]: train_step() took 0.000652 seconds.\n",
      "I0925 23:48:18.123161 139756425134720 programs.py:357] [PAX STATUS]:  Writing summaries (attempt).\n",
      "I0925 23:48:18.123247 139756425134720 py_utils.py:1025] [PAX STATUS]: Elapsed time for <run>: 0.00 seconds  (@ <.../paxml/executors.py:406>)\n",
      "I0925 23:48:18.123310 139756425134720 executors.py:472] [PAX STATUS]: Step `2` completed.\n",
      "I0925 23:48:18.123345 139756425134720 executors.py:381] [PAX STATUS]: Beginning step `3`.\n",
      "I0925 23:48:18.123389 139756425134720 py_utils.py:1015] [PAX STATUS]: Starting timer for <run> @ <.../paxml/executors.py:406>\n",
      "I0925 23:48:18.123420 139756425134720 programs.py:304] [PAX STATUS]:  Retrieving inputs.\n",
      "I0925 23:48:18.161105 139756425134720 programs.py:316] [PAX STATUS]:  Retrieved inputs.\n",
      "I0925 23:48:18.161267 139756425134720 programs.py:327] [PAX STATUS]:  Performing train_step().\n",
      "I0925 23:48:18.162369 139756425134720 programs.py:345] [PAX STATUS]: train_step() took 0.001017 seconds.\n",
      "I0925 23:48:18.162475 139756425134720 programs.py:357] [PAX STATUS]:  Writing summaries (attempt).\n",
      "I0925 23:48:18.162586 139756425134720 py_utils.py:1025] [PAX STATUS]: Elapsed time for <run>: 0.04 seconds  (@ <.../paxml/executors.py:406>)\n",
      "I0925 23:48:18.162640 139756425134720 executors.py:472] [PAX STATUS]: Step `3` completed.\n",
      "I0925 23:48:18.162676 139756425134720 executors.py:381] [PAX STATUS]: Beginning step `4`.\n",
      "I0925 23:48:18.162729 139756425134720 py_utils.py:1015] [PAX STATUS]: Starting timer for <run> @ <.../paxml/executors.py:406>\n",
      "I0925 23:48:18.162760 139756425134720 programs.py:304] [PAX STATUS]:  Retrieving inputs.\n",
      "I0925 23:48:18.178686 139756425134720 programs.py:316] [PAX STATUS]:  Retrieved inputs.\n",
      "I0925 23:48:18.178853 139756425134720 programs.py:327] [PAX STATUS]:  Performing train_step().\n",
      "I0925 23:48:18.180000 139756425134720 programs.py:345] [PAX STATUS]: train_step() took 0.001058 seconds.\n",
      "I0925 23:48:18.180111 139756425134720 programs.py:357] [PAX STATUS]:  Writing summaries (attempt).\n",
      "I0925 23:48:18.180225 139756425134720 py_utils.py:1025] [PAX STATUS]: Elapsed time for <run>: 0.02 seconds  (@ <.../paxml/executors.py:406>)\n",
      "I0925 23:48:18.180280 139756425134720 executors.py:472] [PAX STATUS]: Step `4` completed.\n",
      "I0925 23:48:18.180342 139756425134720 py_utils.py:1015] [PAX STATUS]: Starting timer for <run> @ <.../paxml/executors.py:406>\n",
      "I0925 23:48:18.205659 139756425134720 programs.py:345] [PAX STATUS]: train_step() took 0.000921 seconds.\n",
      "I0925 23:48:18.205861 139756425134720 py_utils.py:1025] [PAX STATUS]: Elapsed time for <run>: 0.03 seconds  (@ <.../paxml/executors.py:406>)\n",
      "I0925 23:48:18.205942 139756425134720 py_utils.py:1015] [PAX STATUS]: Starting timer for <run> @ <.../paxml/executors.py:406>\n",
      "I0925 23:48:18.220588 139756425134720 programs.py:345] [PAX STATUS]: train_step() took 0.001426 seconds.\n",
      "I0925 23:48:18.220782 139756425134720 py_utils.py:1025] [PAX STATUS]: Elapsed time for <run>: 0.01 seconds  (@ <.../paxml/executors.py:406>)\n",
      "I0925 23:48:18.220872 139756425134720 py_utils.py:1015] [PAX STATUS]: Starting timer for <run> @ <.../paxml/executors.py:406>\n",
      "I0925 23:48:18.249093 139756425134720 programs.py:345] [PAX STATUS]: train_step() took 0.000890 seconds.\n",
      "I0925 23:48:18.249287 139756425134720 py_utils.py:1025] [PAX STATUS]: Elapsed time for <run>: 0.03 seconds  (@ <.../paxml/executors.py:406>)\n",
      "I0925 23:48:18.249368 139756425134720 py_utils.py:1015] [PAX STATUS]: Starting timer for <run> @ <.../paxml/executors.py:406>\n",
      "I0925 23:48:18.265065 139756425134720 programs.py:345] [PAX STATUS]: train_step() took 0.000921 seconds.\n",
      "I0925 23:48:18.265264 139756425134720 py_utils.py:1025] [PAX STATUS]: Elapsed time for <run>: 0.02 seconds  (@ <.../paxml/executors.py:406>)\n",
      "I0925 23:48:18.265360 139756425134720 py_utils.py:1015] [PAX STATUS]: Starting timer for <run> @ <.../paxml/executors.py:406>\n",
      "I0925 23:48:18.290372 139756425134720 programs.py:345] [PAX STATUS]: train_step() took 0.001008 seconds.\n",
      "I0925 23:48:18.290516 139756425134720 programs.py:468] steps/sec: 3.824998\n",
      "I0925 23:48:18.290644 139756425134720 py_utils.py:1025] [PAX STATUS]: Elapsed time for <run>: 0.03 seconds  (@ <.../paxml/executors.py:406>)\n",
      "I0925 23:48:18.290734 139756425134720 py_utils.py:1015] [PAX STATUS]: Starting timer for <run> @ <.../paxml/executors.py:406>\n",
      "I0925 23:48:18.311077 139756425134720 programs.py:345] [PAX STATUS]: train_step() took 0.001065 seconds.\n",
      "I0925 23:48:18.311355 139756425134720 py_utils.py:1025] [PAX STATUS]: Elapsed time for <run>: 0.02 seconds  (@ <.../paxml/executors.py:406>)\n",
      "I0925 23:48:18.311459 139756425134720 py_utils.py:1015] [PAX STATUS]: Starting timer for <run> @ <.../paxml/executors.py:406>\n",
      "I0925 23:48:18.333975 139756425134720 programs.py:345] [PAX STATUS]: train_step() took 0.001194 seconds.\n",
      "I0925 23:48:18.334253 139756425134720 py_utils.py:1025] [PAX STATUS]: Elapsed time for <run>: 0.02 seconds  (@ <.../paxml/executors.py:406>)\n",
      "I0925 23:48:18.334370 139756425134720 py_utils.py:1015] [PAX STATUS]: Starting timer for <run> @ <.../paxml/executors.py:406>\n",
      "I0925 23:48:18.351169 139756425134720 programs.py:345] [PAX STATUS]: train_step() took 0.000976 seconds.\n",
      "I0925 23:48:18.351368 139756425134720 py_utils.py:1025] [PAX STATUS]: Elapsed time for <run>: 0.02 seconds  (@ <.../paxml/executors.py:406>)\n",
      "I0925 23:48:18.351451 139756425134720 py_utils.py:1015] [PAX STATUS]: Starting timer for <run> @ <.../paxml/executors.py:406>\n",
      "I0925 23:48:18.375014 139756425134720 programs.py:345] [PAX STATUS]: train_step() took 0.000943 seconds.\n",
      "I0925 23:48:18.375208 139756425134720 py_utils.py:1025] [PAX STATUS]: Elapsed time for <run>: 0.02 seconds  (@ <.../paxml/executors.py:406>)\n",
      "I0925 23:48:18.375300 139756425134720 py_utils.py:1015] [PAX STATUS]: Starting timer for <run> @ <.../paxml/executors.py:406>\n",
      "I0925 23:48:18.391416 139756425134720 programs.py:345] [PAX STATUS]: train_step() took 0.001023 seconds.\n",
      "I0925 23:48:18.391622 139756425134720 py_utils.py:1025] [PAX STATUS]: Elapsed time for <run>: 0.02 seconds  (@ <.../paxml/executors.py:406>)\n",
      "I0925 23:48:18.391706 139756425134720 py_utils.py:1015] [PAX STATUS]: Starting timer for <run> @ <.../paxml/executors.py:406>\n",
      "I0925 23:48:18.416884 139756425134720 programs.py:345] [PAX STATUS]: train_step() took 0.001012 seconds.\n",
      "I0925 23:48:18.417110 139756425134720 py_utils.py:1025] [PAX STATUS]: Elapsed time for <run>: 0.03 seconds  (@ <.../paxml/executors.py:406>)\n",
      "I0925 23:48:18.417209 139756425134720 py_utils.py:1015] [PAX STATUS]: Starting timer for <run> @ <.../paxml/executors.py:406>\n",
      "I0925 23:48:18.433073 139756425134720 programs.py:345] [PAX STATUS]: train_step() took 0.001029 seconds.\n",
      "I0925 23:48:18.433275 139756425134720 py_utils.py:1025] [PAX STATUS]: Elapsed time for <run>: 0.02 seconds  (@ <.../paxml/executors.py:406>)\n",
      "I0925 23:48:18.433362 139756425134720 py_utils.py:1015] [PAX STATUS]: Starting timer for <run> @ <.../paxml/executors.py:406>\n",
      "I0925 23:48:18.458313 139756425134720 programs.py:345] [PAX STATUS]: train_step() took 0.001009 seconds.\n",
      "I0925 23:48:18.458523 139756425134720 py_utils.py:1025] [PAX STATUS]: Elapsed time for <run>: 0.03 seconds  (@ <.../paxml/executors.py:406>)\n",
      "I0925 23:48:18.458611 139756425134720 py_utils.py:1015] [PAX STATUS]: Starting timer for <run> @ <.../paxml/executors.py:406>\n",
      "I0925 23:48:18.476695 139756425134720 programs.py:345] [PAX STATUS]: train_step() took 0.000968 seconds.\n",
      "I0925 23:48:18.476893 139756425134720 py_utils.py:1025] [PAX STATUS]: Elapsed time for <run>: 0.02 seconds  (@ <.../paxml/executors.py:406>)\n",
      "I0925 23:48:18.476973 139756425134720 py_utils.py:1015] [PAX STATUS]: Starting timer for <run> @ <.../paxml/executors.py:406>\n",
      "I0925 23:48:18.499246 139756425134720 programs.py:345] [PAX STATUS]: train_step() took 0.000953 seconds.\n",
      "I0925 23:48:18.499372 139756425134720 programs.py:468] steps/sec: 47.889476\n",
      "I0925 23:48:18.499496 139756425134720 py_utils.py:1025] [PAX STATUS]: Elapsed time for <run>: 0.02 seconds  (@ <.../paxml/executors.py:406>)\n",
      "I0925 23:48:18.700184 139756425134720 programs.py:468] steps/sec: 49.810156\n",
      "I0925 23:48:18.896327 139756425134720 programs.py:468] steps/sec: 51.016907\n",
      "I0925 23:48:19.097937 139756425134720 programs.py:468] steps/sec: 49.633444\n",
      "/home/awolant/Projects/DALI/dali/operators/reader/loader/lmdb.h:76: lmdb 0 /home/awolant/Projects/DALI_extra/db/MNIST/training/ rewind to the begin from 59999\n",
      "/home/awolant/Projects/DALI/dali/operators/reader/loader/lmdb.h:76: lmdb 0 /home/awolant/Projects/DALI_extra/db/MNIST/training/ rewind to the begin from 0\n",
      "W0925 23:48:19.277830 139756425134720 base_iterator.py:437] DALI iterator does not support resetting while epoch is not finished.                              Ignoring...\n",
      "I0925 23:48:19.304168 139756425134720 programs.py:468] steps/sec: 48.522833\n",
      "I0925 23:48:19.514456 139756425134720 programs.py:468] steps/sec: 47.582790\n",
      "I0925 23:48:19.717874 139756425134720 programs.py:468] steps/sec: 49.191738\n",
      "I0925 23:48:19.925325 139756425134720 programs.py:468] steps/sec: 48.238225\n",
      "I0925 23:48:20.130805 139756425134720 programs.py:468] steps/sec: 48.697704\n",
      "I0925 23:48:20.131616 139756425134720 executors.py:422] [PAX STATUS]:  Starting eval_step().\n",
      "I0925 23:48:20.133987 138603426477632 summary_utils.py:668] [PAX STATUS] step_i: 100, training loss: 2.2580194\n",
      "I0925 23:48:20.134068 138603426477632 summary_utils.py:671] weighted_scalars: {'accuracy': (array(0.4111328, dtype=float32), array(1., dtype=float32)), 'loss': (array(2.2580194, dtype=float32), array(1., dtype=float32))}\n",
      "I0925 23:48:20.134546 138603426477632 summary_utils.py:673] per_example_out: {'predictions': array([[ 0.01765719,  0.05093421, -0.02859303, ...,  0.07828381,\n",
      "         0.0368288 ,  0.0321532 ],\n",
      "       [-0.01537184,  0.01722503, -0.02678885, ...,  0.06671964,\n",
      "         0.0415201 ,  0.05551293],\n",
      "       [ 0.04157303,  0.04538595, -0.01678927, ..., -0.02414268,\n",
      "         0.05803378,  0.03367774],\n",
      "       ...,\n",
      "       [-0.03937948,  0.04670908, -0.04799585, ...,  0.08356304,\n",
      "         0.05652251,  0.08623599],\n",
      "       [-0.01813371,  0.0080601 ,  0.00214794, ...,  0.02695359,\n",
      "         0.0591314 ,  0.0369436 ],\n",
      "       [-0.03945521,  0.04232089,  0.02597373, ...,  0.03965427,\n",
      "         0.02730837, -0.00961193]], dtype=float32)}\n",
      "I0925 23:48:20.134905 138603426477632 summary_utils.py:674] summary_tensors: {'learning/applied_grad_norm_aggregate_scalar': array(0.00261324, dtype=float32), 'learning/clipped_grad_norm_aggregate_scalar': array(0.23977788, dtype=float32), 'learning/grad_scale_aggregate_scalar': array(1., dtype=float32), 'learning/is_valid_step_aggregate_scalar': array(1., dtype=float32), 'learning/lr_aggregate_scalar': array(0.01, dtype=float32), 'learning/raw_grad_norm_aggregate_scalar': array(0.23977788, dtype=float32), 'learning/var_norm_aggregate_scalar': array(23.197985, dtype=float32), 'lr_aggregate_scalar': array(0.01, dtype=float32), 'vars/params/network/conv1/b_scalar': array(0.00492226, dtype=float32), 'vars/params/network/conv1/w_scalar': array(0.08445955, dtype=float32), 'vars/params/network/conv2/b_scalar': array(0.009893, dtype=float32), 'vars/params/network/conv2/w_scalar': array(0.04830401, dtype=float32), 'vars/params/network/dense_0/bias/b_scalar': array(0.00375461, dtype=float32), 'vars/params/network/dense_0/linear/w_scalar': array(0.0242846, dtype=float32), 'vars/params/network/dense_1/w_scalar': array(0.08755321, dtype=float32), 'vars/params/network/dense_2/b_scalar': array(0.00470723, dtype=float32)}\n",
      "I0925 23:48:20.136436 138603426477632 summary_utils.py:340] summary tensor at step=100 loss 2.25801944732666\n",
      "I0925 23:48:20.137759 138603426477632 summary_utils.py:340] summary tensor at step=100 Steps/sec 48.697703687707104\n",
      "I0925 23:48:20.138183 138603426477632 summary_utils.py:427] Metrics values at step 100:\n",
      "I0925 23:48:20.138241 138603426477632 summary_utils.py:428]   loss=2.258019\n",
      "I0925 23:48:20.138324 138603426477632 summary_utils.py:440]   accuracy=0.411133 (weight=1.000000)\n",
      "I0925 23:48:20.138391 138603426477632 summary_utils.py:340] summary tensor at step=100 Metrics/accuracy 0.4111328125\n",
      "I0925 23:48:20.138780 138603426477632 summary_utils.py:340] summary tensor at step=100 Metrics/accuracy-weight 1.0\n",
      "I0925 23:48:20.139173 138603426477632 summary_utils.py:440]   loss=2.258019 (weight=1.000000)\n",
      "I0925 23:48:20.139261 138603426477632 summary_utils.py:340] summary tensor at step=100 Metrics/loss 2.25801944732666\n",
      "I0925 23:48:20.139617 138603426477632 summary_utils.py:340] summary tensor at step=100 Metrics/loss-weight 1.0\n",
      "I0925 23:48:20.139962 138603426477632 local.py:45] Setting task status: step = 100, loss= 2.25801944732666, steps/sec 48.697703687707104, accuracy=0.4111328125, loss=2.25801944732666\n",
      "I0925 23:48:20.140097 138603426477632 summary_utils.py:340] summary tensor at step=100 learning/applied_grad_norm_scalar 0.002613239688798785\n",
      "I0925 23:48:20.140477 138603426477632 summary_utils.py:340] summary tensor at step=100 learning/clipped_grad_norm_scalar 0.23977787792682648\n",
      "I0925 23:48:20.140862 138603426477632 summary_utils.py:340] summary tensor at step=100 learning/grad_scale_scalar 1.0\n",
      "I0925 23:48:20.141225 138603426477632 summary_utils.py:340] summary tensor at step=100 learning/is_valid_step_scalar 1.0\n",
      "I0925 23:48:20.141588 138603426477632 summary_utils.py:340] summary tensor at step=100 learning/lr_scalar 0.009999999776482582\n",
      "I0925 23:48:20.144757 138603426477632 summary_utils.py:340] summary tensor at step=100 learning/raw_grad_norm_scalar 0.23977787792682648\n",
      "I0925 23:48:20.145376 138603426477632 summary_utils.py:340] summary tensor at step=100 learning/var_norm_scalar 23.19798469543457\n",
      "I0925 23:48:20.145864 138603426477632 summary_utils.py:340] summary tensor at step=100 lr_scalar 0.009999999776482582\n",
      "I0925 23:48:20.146697 138603426477632 summary_utils.py:340] summary tensor at step=100 vars/params/network/conv1/b_scalar 0.004922256805002689\n",
      "I0925 23:48:20.147120 138603426477632 summary_utils.py:340] summary tensor at step=100 vars/params/network/conv1/w_scalar 0.08445955067873001\n",
      "I0925 23:48:20.147550 138603426477632 summary_utils.py:340] summary tensor at step=100 vars/params/network/conv2/b_scalar 0.009893003851175308\n",
      "I0925 23:48:20.147986 138603426477632 summary_utils.py:340] summary tensor at step=100 vars/params/network/conv2/w_scalar 0.048304006457328796\n",
      "I0925 23:48:20.148401 138603426477632 summary_utils.py:340] summary tensor at step=100 vars/params/network/dense_0/bias/b_scalar 0.0037546148523688316\n",
      "I0925 23:48:20.148823 138603426477632 summary_utils.py:340] summary tensor at step=100 vars/params/network/dense_0/linear/w_scalar 0.024284597486257553\n",
      "I0925 23:48:20.149245 138603426477632 summary_utils.py:340] summary tensor at step=100 vars/params/network/dense_1/w_scalar 0.08755321055650711\n",
      "I0925 23:48:20.149658 138603426477632 summary_utils.py:340] summary tensor at step=100 vars/params/network/dense_2/b_scalar 0.004707233514636755\n",
      "I0925 23:48:20.150121 138603426477632 summary_utils.py:457] Wrote summary entry at step `100` (loss=`2.258019`).\n",
      "I0925 23:48:20.334221 139756425134720 programs.py:468] steps/sec: 49.194738\n",
      "/home/awolant/Projects/DALI/dali/operators/reader/loader/lmdb.h:76: lmdb 0 /home/awolant/Projects/DALI_extra/db/MNIST/training/ rewind to the begin from 59999\n",
      "/home/awolant/Projects/DALI/dali/operators/reader/loader/lmdb.h:76: lmdb 0 /home/awolant/Projects/DALI_extra/db/MNIST/training/ rewind to the begin from 0\n",
      "W0925 23:48:20.498000 139756425134720 base_iterator.py:437] DALI iterator does not support resetting while epoch is not finished.                              Ignoring...\n",
      "I0925 23:48:20.537693 139756425134720 programs.py:468] steps/sec: 49.178011\n",
      "I0925 23:48:20.740860 139756425134720 programs.py:468] steps/sec: 49.250540\n",
      "I0925 23:48:20.944024 139756425134720 programs.py:468] steps/sec: 49.251754\n",
      "I0925 23:48:21.147352 139756425134720 programs.py:468] steps/sec: 49.217482\n",
      "I0925 23:48:21.355878 139756425134720 programs.py:468] steps/sec: 47.987552\n",
      "I0925 23:48:21.557047 139756425134720 programs.py:468] steps/sec: 49.742694\n",
      "/home/awolant/Projects/DALI/dali/operators/reader/loader/lmdb.h:76: lmdb 0 /home/awolant/Projects/DALI_extra/db/MNIST/training/ rewind to the begin from 59999\n",
      "/home/awolant/Projects/DALI/dali/operators/reader/loader/lmdb.h:76: lmdb 0 /home/awolant/Projects/DALI_extra/db/MNIST/training/ rewind to the begin from 0\n",
      "W0925 23:48:21.679304 139756425134720 base_iterator.py:437] DALI iterator does not support resetting while epoch is not finished.                              Ignoring...\n",
      "I0925 23:48:21.761788 139756425134720 programs.py:468] steps/sec: 48.873099\n",
      "I0925 23:48:21.963040 139756425134720 programs.py:468] steps/sec: 49.719875\n",
      "I0925 23:48:22.170762 139756425134720 programs.py:468] steps/sec: 48.171079\n",
      "I0925 23:48:22.171179 139756425134720 executors.py:422] [PAX STATUS]:  Starting eval_step().\n",
      "I0925 23:48:22.174270 138603426477632 summary_utils.py:668] [PAX STATUS] step_i: 200, training loss: 2.1472764\n",
      "I0925 23:48:22.174346 138603426477632 summary_utils.py:671] weighted_scalars: {'accuracy': (array(0.53808594, dtype=float32), array(1., dtype=float32)), 'loss': (array(2.1472764, dtype=float32), array(1., dtype=float32))}\n",
      "I0925 23:48:22.174800 138603426477632 summary_utils.py:673] per_example_out: {'predictions': array([[ 0.47404793, -0.12250548,  0.20743737, ..., -0.20594883,\n",
      "         0.05853971, -0.18898833],\n",
      "       [-0.11640404,  0.06352264, -0.10946636, ...,  0.06317379,\n",
      "         0.14659752,  0.15336378],\n",
      "       [-0.10345305,  0.0817857 , -0.07862944, ...,  0.04346912,\n",
      "         0.20173195,  0.10139043],\n",
      "       ...,\n",
      "       [-0.06143443,  0.21368378, -0.0299088 , ..., -0.03097491,\n",
      "         0.08110224,  0.02569573],\n",
      "       [-0.0345261 ,  0.00338606,  0.13188298, ..., -0.10068024,\n",
      "         0.19383012, -0.0446259 ],\n",
      "       [ 0.14193647, -0.06109748,  0.02827646, ..., -0.06891433,\n",
      "         0.08116934,  0.02863777]], dtype=float32)}\n",
      "I0925 23:48:22.175159 138603426477632 summary_utils.py:674] summary_tensors: {'learning/applied_grad_norm_aggregate_scalar': array(0.00516154, dtype=float32), 'learning/clipped_grad_norm_aggregate_scalar': array(0.46489292, dtype=float32), 'learning/grad_scale_aggregate_scalar': array(1., dtype=float32), 'learning/is_valid_step_aggregate_scalar': array(1., dtype=float32), 'learning/lr_aggregate_scalar': array(0.01, dtype=float32), 'learning/raw_grad_norm_aggregate_scalar': array(0.46489292, dtype=float32), 'learning/var_norm_aggregate_scalar': array(23.213446, dtype=float32), 'lr_aggregate_scalar': array(0.01, dtype=float32), 'vars/params/network/conv1/b_scalar': array(0.01004838, dtype=float32), 'vars/params/network/conv1/w_scalar': array(0.08773998, dtype=float32), 'vars/params/network/conv2/b_scalar': array(0.01927686, dtype=float32), 'vars/params/network/conv2/w_scalar': array(0.04839664, dtype=float32), 'vars/params/network/dense_0/bias/b_scalar': array(0.0062917, dtype=float32), 'vars/params/network/dense_0/linear/w_scalar': array(0.02428929, dtype=float32), 'vars/params/network/dense_1/w_scalar': array(0.08797315, dtype=float32), 'vars/params/network/dense_2/b_scalar': array(0.00997511, dtype=float32)}\n",
      "I0925 23:48:22.176362 138603426477632 summary_utils.py:340] summary tensor at step=200 loss 2.1472764015197754\n",
      "I0925 23:48:22.177074 138603426477632 summary_utils.py:340] summary tensor at step=200 Steps/sec 48.17107877479299\n",
      "I0925 23:48:22.177454 138603426477632 summary_utils.py:427] Metrics values at step 200:\n",
      "I0925 23:48:22.177506 138603426477632 summary_utils.py:428]   loss=2.147276\n",
      "I0925 23:48:22.177572 138603426477632 summary_utils.py:440]   accuracy=0.538086 (weight=1.000000)\n",
      "I0925 23:48:22.177632 138603426477632 summary_utils.py:340] summary tensor at step=200 Metrics/accuracy 0.5380859375\n",
      "I0925 23:48:22.177986 138603426477632 summary_utils.py:340] summary tensor at step=200 Metrics/accuracy-weight 1.0\n",
      "I0925 23:48:22.178350 138603426477632 summary_utils.py:440]   loss=2.147276 (weight=1.000000)\n",
      "I0925 23:48:22.178436 138603426477632 summary_utils.py:340] summary tensor at step=200 Metrics/loss 2.1472764015197754\n",
      "I0925 23:48:22.178789 138603426477632 summary_utils.py:340] summary tensor at step=200 Metrics/loss-weight 1.0\n",
      "I0925 23:48:22.179113 138603426477632 local.py:45] Setting task status: step = 200, loss= 2.1472764015197754, steps/sec 48.17107877479299, accuracy=0.5380859375, loss=2.1472764015197754\n",
      "I0925 23:48:22.179255 138603426477632 summary_utils.py:340] summary tensor at step=200 learning/applied_grad_norm_scalar 0.005161536857485771\n",
      "I0925 23:48:22.179670 138603426477632 summary_utils.py:340] summary tensor at step=200 learning/clipped_grad_norm_scalar 0.4648929238319397\n",
      "I0925 23:48:22.180052 138603426477632 summary_utils.py:340] summary tensor at step=200 learning/grad_scale_scalar 1.0\n",
      "I0925 23:48:22.180414 138603426477632 summary_utils.py:340] summary tensor at step=200 learning/is_valid_step_scalar 1.0\n",
      "I0925 23:48:22.180774 138603426477632 summary_utils.py:340] summary tensor at step=200 learning/lr_scalar 0.009999999776482582\n",
      "I0925 23:48:22.181192 138603426477632 summary_utils.py:340] summary tensor at step=200 learning/raw_grad_norm_scalar 0.4648929238319397\n",
      "I0925 23:48:22.181579 138603426477632 summary_utils.py:340] summary tensor at step=200 learning/var_norm_scalar 23.21344566345215\n",
      "I0925 23:48:22.181986 138603426477632 summary_utils.py:340] summary tensor at step=200 lr_scalar 0.009999999776482582\n",
      "I0925 23:48:22.182390 138603426477632 summary_utils.py:340] summary tensor at step=200 vars/params/network/conv1/b_scalar 0.010048380121588707\n",
      "I0925 23:48:22.182774 138603426477632 summary_utils.py:340] summary tensor at step=200 vars/params/network/conv1/w_scalar 0.0877399817109108\n",
      "I0925 23:48:22.183157 138603426477632 summary_utils.py:340] summary tensor at step=200 vars/params/network/conv2/b_scalar 0.01927686296403408\n",
      "I0925 23:48:22.183539 138603426477632 summary_utils.py:340] summary tensor at step=200 vars/params/network/conv2/w_scalar 0.04839663952589035\n",
      "I0925 23:48:22.183920 138603426477632 summary_utils.py:340] summary tensor at step=200 vars/params/network/dense_0/bias/b_scalar 0.00629169587045908\n",
      "I0925 23:48:22.184302 138603426477632 summary_utils.py:340] summary tensor at step=200 vars/params/network/dense_0/linear/w_scalar 0.024289285764098167\n",
      "I0925 23:48:22.184681 138603426477632 summary_utils.py:340] summary tensor at step=200 vars/params/network/dense_1/w_scalar 0.08797314763069153\n",
      "I0925 23:48:22.185063 138603426477632 summary_utils.py:340] summary tensor at step=200 vars/params/network/dense_2/b_scalar 0.009975113905966282\n",
      "I0925 23:48:22.186426 138603426477632 summary_utils.py:457] Wrote summary entry at step `200` (loss=`2.147276`).\n",
      "I0925 23:48:22.377208 139756425134720 programs.py:468] steps/sec: 48.468780\n",
      "I0925 23:48:22.576291 139756425134720 programs.py:468] steps/sec: 50.261403\n",
      "I0925 23:48:22.776307 139756425134720 programs.py:468] steps/sec: 50.027839\n",
      "/home/awolant/Projects/DALI/dali/operators/reader/loader/lmdb.h:76: lmdb 0 /home/awolant/Projects/DALI_extra/db/MNIST/training/ rewind to the begin from 59999\n",
      "/home/awolant/Projects/DALI/dali/operators/reader/loader/lmdb.h:76: lmdb 0 /home/awolant/Projects/DALI_extra/db/MNIST/training/ rewind to the begin from 0\n",
      "W0925 23:48:22.874166 139756425134720 base_iterator.py:437] DALI iterator does not support resetting while epoch is not finished.                              Ignoring...\n",
      "I0925 23:48:22.986757 139756425134720 programs.py:468] steps/sec: 47.546758\n",
      "I0925 23:48:23.194975 139756425134720 programs.py:468] steps/sec: 48.056830\n",
      "I0925 23:48:23.405689 139756425134720 programs.py:468] steps/sec: 47.487758\n",
      "I0925 23:48:23.611813 139756425134720 programs.py:468] steps/sec: 48.545354\n",
      "I0925 23:48:23.817574 139756425134720 programs.py:468] steps/sec: 48.630514\n",
      "/home/awolant/Projects/DALI/dali/operators/reader/loader/lmdb.h:76: lmdb 0 /home/awolant/Projects/DALI_extra/db/MNIST/training/ rewind to the begin from 59999\n",
      "/home/awolant/Projects/DALI/dali/operators/reader/loader/lmdb.h:76: lmdb 0 /home/awolant/Projects/DALI_extra/db/MNIST/training/ rewind to the begin from 0\n",
      "I0925 23:48:24.028769 139756425134720 programs.py:468] steps/sec: 47.378008\n",
      "W0925 23:48:24.090396 139756425134720 base_iterator.py:437] DALI iterator does not support resetting while epoch is not finished.                              Ignoring...\n",
      "I0925 23:48:24.231105 139756425134720 programs.py:468] steps/sec: 49.453667\n",
      "I0925 23:48:24.231497 139756425134720 executors.py:422] [PAX STATUS]:  Starting eval_step().\n",
      "I0925 23:48:24.234196 138603426477632 summary_utils.py:668] [PAX STATUS] step_i: 300, training loss: 1.4150704\n",
      "I0925 23:48:24.234266 138603426477632 summary_utils.py:671] weighted_scalars: {'accuracy': (array(0.7451172, dtype=float32), array(1., dtype=float32)), 'loss': (array(1.4150704, dtype=float32), array(1., dtype=float32))}\n",
      "I0925 23:48:24.234735 138603426477632 summary_utils.py:673] per_example_out: {'predictions': array([[-0.8427371 ,  0.5666051 , -0.22416571, ..., -0.23387598,\n",
      "         0.5180705 ,  0.3338988 ],\n",
      "       [ 0.44818798, -0.2610643 , -0.78965163, ...,  0.382759  ,\n",
      "         0.70040596,  0.63982266],\n",
      "       [ 0.02369272, -0.22271824, -0.14611685, ...,  0.06507813,\n",
      "         0.37055033,  0.28208795],\n",
      "       ...,\n",
      "       [-0.7833816 , -0.4099597 , -0.09126573, ..., -0.29248655,\n",
      "         0.4310693 ,  0.5387332 ],\n",
      "       [ 1.6454512 , -0.7623337 ,  0.14043134, ..., -0.623435  ,\n",
      "        -0.01939649, -0.17932102],\n",
      "       [ 0.33651507, -1.2644407 ,  0.90725577, ..., -0.08912277,\n",
      "         0.05149567, -0.24605782]], dtype=float32)}\n",
      "I0925 23:48:24.235094 138603426477632 summary_utils.py:674] summary_tensors: {'learning/applied_grad_norm_aggregate_scalar': array(0.01350006, dtype=float32), 'learning/clipped_grad_norm_aggregate_scalar': array(1.2479115, dtype=float32), 'learning/grad_scale_aggregate_scalar': array(1., dtype=float32), 'learning/is_valid_step_aggregate_scalar': array(1., dtype=float32), 'learning/lr_aggregate_scalar': array(0.01, dtype=float32), 'learning/raw_grad_norm_aggregate_scalar': array(1.2479115, dtype=float32), 'learning/var_norm_aggregate_scalar': array(23.28005, dtype=float32), 'lr_aggregate_scalar': array(0.01, dtype=float32), 'vars/params/network/conv1/b_scalar': array(0.02118796, dtype=float32), 'vars/params/network/conv1/w_scalar': array(0.10125273, dtype=float32), 'vars/params/network/conv2/b_scalar': array(0.03248831, dtype=float32), 'vars/params/network/conv2/w_scalar': array(0.0488132, dtype=float32), 'vars/params/network/dense_0/bias/b_scalar': array(0.00881791, dtype=float32), 'vars/params/network/dense_0/linear/w_scalar': array(0.02430957, dtype=float32), 'vars/params/network/dense_1/w_scalar': array(0.08972645, dtype=float32), 'vars/params/network/dense_2/b_scalar': array(0.02010236, dtype=float32)}\n",
      "I0925 23:48:24.236328 138603426477632 summary_utils.py:340] summary tensor at step=300 loss 1.4150704145431519\n",
      "I0925 23:48:24.237085 138603426477632 summary_utils.py:340] summary tensor at step=300 Steps/sec 49.453667370962876\n",
      "I0925 23:48:24.237484 138603426477632 summary_utils.py:427] Metrics values at step 300:\n",
      "I0925 23:48:24.237539 138603426477632 summary_utils.py:428]   loss=1.415070\n",
      "I0925 23:48:24.237610 138603426477632 summary_utils.py:440]   accuracy=0.745117 (weight=1.000000)\n",
      "I0925 23:48:24.237673 138603426477632 summary_utils.py:340] summary tensor at step=300 Metrics/accuracy 0.7451171875\n",
      "I0925 23:48:24.238047 138603426477632 summary_utils.py:340] summary tensor at step=300 Metrics/accuracy-weight 1.0\n",
      "I0925 23:48:24.238416 138603426477632 summary_utils.py:440]   loss=1.415070 (weight=1.000000)\n",
      "I0925 23:48:24.238496 138603426477632 summary_utils.py:340] summary tensor at step=300 Metrics/loss 1.4150704145431519\n",
      "I0925 23:48:24.238822 138603426477632 summary_utils.py:340] summary tensor at step=300 Metrics/loss-weight 1.0\n",
      "I0925 23:48:24.239124 138603426477632 local.py:45] Setting task status: step = 300, loss= 1.4150704145431519, steps/sec 49.453667370962876, accuracy=0.7451171875, loss=1.4150704145431519\n",
      "I0925 23:48:24.239255 138603426477632 summary_utils.py:340] summary tensor at step=300 learning/applied_grad_norm_scalar 0.013500062748789787\n",
      "I0925 23:48:24.239639 138603426477632 summary_utils.py:340] summary tensor at step=300 learning/clipped_grad_norm_scalar 1.2479114532470703\n",
      "I0925 23:48:24.240020 138603426477632 summary_utils.py:340] summary tensor at step=300 learning/grad_scale_scalar 1.0\n",
      "I0925 23:48:24.240382 138603426477632 summary_utils.py:340] summary tensor at step=300 learning/is_valid_step_scalar 1.0\n",
      "I0925 23:48:24.240744 138603426477632 summary_utils.py:340] summary tensor at step=300 learning/lr_scalar 0.009999999776482582\n",
      "I0925 23:48:24.241152 138603426477632 summary_utils.py:340] summary tensor at step=300 learning/raw_grad_norm_scalar 1.2479114532470703\n",
      "I0925 23:48:24.241517 138603426477632 summary_utils.py:340] summary tensor at step=300 learning/var_norm_scalar 23.28005027770996\n",
      "I0925 23:48:24.241897 138603426477632 summary_utils.py:340] summary tensor at step=300 lr_scalar 0.009999999776482582\n",
      "I0925 23:48:24.242268 138603426477632 summary_utils.py:340] summary tensor at step=300 vars/params/network/conv1/b_scalar 0.02118796482682228\n",
      "I0925 23:48:24.242698 138603426477632 summary_utils.py:340] summary tensor at step=300 vars/params/network/conv1/w_scalar 0.1012527346611023\n",
      "I0925 23:48:24.243109 138603426477632 summary_utils.py:340] summary tensor at step=300 vars/params/network/conv2/b_scalar 0.03248830512166023\n",
      "I0925 23:48:24.243521 138603426477632 summary_utils.py:340] summary tensor at step=300 vars/params/network/conv2/w_scalar 0.04881319776177406\n",
      "I0925 23:48:24.243936 138603426477632 summary_utils.py:340] summary tensor at step=300 vars/params/network/dense_0/bias/b_scalar 0.00881790742278099\n",
      "I0925 23:48:24.244316 138603426477632 summary_utils.py:340] summary tensor at step=300 vars/params/network/dense_0/linear/w_scalar 0.024309569969773293\n",
      "I0925 23:48:24.244696 138603426477632 summary_utils.py:340] summary tensor at step=300 vars/params/network/dense_1/w_scalar 0.08972644805908203\n",
      "I0925 23:48:24.245077 138603426477632 summary_utils.py:340] summary tensor at step=300 vars/params/network/dense_2/b_scalar 0.020102357491850853\n",
      "I0925 23:48:24.245470 138603426477632 summary_utils.py:457] Wrote summary entry at step `300` (loss=`1.415070`).\n",
      "I0925 23:48:24.433081 139756425134720 programs.py:468] steps/sec: 49.541871\n",
      "I0925 23:48:24.640955 139756425134720 programs.py:468] steps/sec: 48.134979\n",
      "I0925 23:48:24.837765 139756425134720 programs.py:468] steps/sec: 50.843376\n",
      "I0925 23:48:25.035741 139756425134720 programs.py:468] steps/sec: 50.543831\n",
      "/home/awolant/Projects/DALI/dali/operators/reader/loader/lmdb.h:76: lmdb 0 /home/awolant/Projects/DALI_extra/db/MNIST/training/ rewind to the begin from 59999\n",
      "/home/awolant/Projects/DALI/dali/operators/reader/loader/lmdb.h:76: lmdb 0 /home/awolant/Projects/DALI_extra/db/MNIST/training/ rewind to the begin from 0\n",
      "I0925 23:48:25.236923 139756425134720 programs.py:468] steps/sec: 49.739096\n",
      "W0925 23:48:25.277075 139756425134720 base_iterator.py:437] DALI iterator does not support resetting while epoch is not finished.                              Ignoring...\n",
      "I0925 23:48:25.438412 139756425134720 programs.py:468] steps/sec: 49.665005\n",
      "I0925 23:48:25.627346 139756425134720 programs.py:468] steps/sec: 52.965740\n",
      "I0925 23:48:25.826932 139756425134720 programs.py:468] steps/sec: 50.135118\n",
      "I0925 23:48:26.026103 139756425134720 programs.py:468] steps/sec: 50.240692\n",
      "I0925 23:48:26.224364 139756425134720 programs.py:468] steps/sec: 50.470482\n",
      "I0925 23:48:26.224786 139756425134720 executors.py:422] [PAX STATUS]:  Starting eval_step().\n",
      "I0925 23:48:26.227770 138603426477632 summary_utils.py:668] [PAX STATUS] step_i: 400, training loss: 0.6608458\n",
      "I0925 23:48:26.227846 138603426477632 summary_utils.py:671] weighted_scalars: {'accuracy': (array(0.8232422, dtype=float32), array(1., dtype=float32)), 'loss': (array(0.6608458, dtype=float32), array(1., dtype=float32))}\n",
      "I0925 23:48:26.228306 138603426477632 summary_utils.py:673] per_example_out: {'predictions': array([[ 0.45477292,  1.1871557 , -0.7035879 , ..., -1.2487562 ,\n",
      "         4.256291  ,  0.47952414],\n",
      "       [ 1.093693  , -5.621391  ,  0.8084887 , ..., -3.2590868 ,\n",
      "         3.0931594 ,  1.1948363 ],\n",
      "       [ 0.8273768 , -2.3654134 ,  2.3401768 , ..., -3.409637  ,\n",
      "         0.9745959 , -0.9993003 ],\n",
      "       ...,\n",
      "       [ 0.03803476,  0.7688107 ,  0.9229584 , ...,  0.0922604 ,\n",
      "         0.30235   , -0.40580013],\n",
      "       [ 5.7166553 , -4.2916517 , -0.00941462, ..., -0.28723773,\n",
      "         1.0482639 , -0.429532  ],\n",
      "       [ 3.4559903 , -2.9617813 ,  0.46090457, ..., -1.6423646 ,\n",
      "         0.8378321 , -0.5584724 ]], dtype=float32)}\n",
      "I0925 23:48:26.228663 138603426477632 summary_utils.py:674] summary_tensors: {'learning/applied_grad_norm_aggregate_scalar': array(0.00969568, dtype=float32), 'learning/clipped_grad_norm_aggregate_scalar': array(0.9400846, dtype=float32), 'learning/grad_scale_aggregate_scalar': array(1., dtype=float32), 'learning/is_valid_step_aggregate_scalar': array(1., dtype=float32), 'learning/lr_aggregate_scalar': array(0.01, dtype=float32), 'learning/raw_grad_norm_aggregate_scalar': array(0.9400846, dtype=float32), 'learning/var_norm_aggregate_scalar': array(23.384424, dtype=float32), 'lr_aggregate_scalar': array(0.01, dtype=float32), 'vars/params/network/conv1/b_scalar': array(0.03040592, dtype=float32), 'vars/params/network/conv1/w_scalar': array(0.11958512, dtype=float32), 'vars/params/network/conv2/b_scalar': array(0.04014616, dtype=float32), 'vars/params/network/conv2/w_scalar': array(0.04946519, dtype=float32), 'vars/params/network/dense_0/bias/b_scalar': array(0.00994822, dtype=float32), 'vars/params/network/dense_0/linear/w_scalar': array(0.02434082, dtype=float32), 'vars/params/network/dense_1/w_scalar': array(0.0923448, dtype=float32), 'vars/params/network/dense_2/b_scalar': array(0.02656354, dtype=float32)}\n",
      "I0925 23:48:26.229973 138603426477632 summary_utils.py:340] summary tensor at step=400 loss 0.6608458161354065\n",
      "I0925 23:48:26.230745 138603426477632 summary_utils.py:340] summary tensor at step=400 Steps/sec 50.470482202442476\n",
      "I0925 23:48:26.231150 138603426477632 summary_utils.py:427] Metrics values at step 400:\n",
      "I0925 23:48:26.231205 138603426477632 summary_utils.py:428]   loss=0.660846\n",
      "I0925 23:48:26.231287 138603426477632 summary_utils.py:440]   accuracy=0.823242 (weight=1.000000)\n",
      "I0925 23:48:26.231348 138603426477632 summary_utils.py:340] summary tensor at step=400 Metrics/accuracy 0.8232421875\n",
      "I0925 23:48:26.231714 138603426477632 summary_utils.py:340] summary tensor at step=400 Metrics/accuracy-weight 1.0\n",
      "I0925 23:48:26.232050 138603426477632 summary_utils.py:440]   loss=0.660846 (weight=1.000000)\n",
      "I0925 23:48:26.232131 138603426477632 summary_utils.py:340] summary tensor at step=400 Metrics/loss 0.6608458161354065\n",
      "I0925 23:48:26.232460 138603426477632 summary_utils.py:340] summary tensor at step=400 Metrics/loss-weight 1.0\n",
      "I0925 23:48:26.232764 138603426477632 local.py:45] Setting task status: step = 400, loss= 0.6608458161354065, steps/sec 50.470482202442476, accuracy=0.8232421875, loss=0.6608458161354065\n",
      "I0925 23:48:26.232898 138603426477632 summary_utils.py:340] summary tensor at step=400 learning/applied_grad_norm_scalar 0.009695683605968952\n",
      "I0925 23:48:26.233279 138603426477632 summary_utils.py:340] summary tensor at step=400 learning/clipped_grad_norm_scalar 0.9400845766067505\n",
      "I0925 23:48:26.233661 138603426477632 summary_utils.py:340] summary tensor at step=400 learning/grad_scale_scalar 1.0\n",
      "I0925 23:48:26.234033 138603426477632 summary_utils.py:340] summary tensor at step=400 learning/is_valid_step_scalar 1.0\n",
      "I0925 23:48:26.234396 138603426477632 summary_utils.py:340] summary tensor at step=400 learning/lr_scalar 0.009999999776482582\n",
      "I0925 23:48:26.234847 138603426477632 summary_utils.py:340] summary tensor at step=400 learning/raw_grad_norm_scalar 0.9400845766067505\n",
      "I0925 23:48:26.235238 138603426477632 summary_utils.py:340] summary tensor at step=400 learning/var_norm_scalar 23.384424209594727\n",
      "I0925 23:48:26.235645 138603426477632 summary_utils.py:340] summary tensor at step=400 lr_scalar 0.009999999776482582\n",
      "I0925 23:48:26.236037 138603426477632 summary_utils.py:340] summary tensor at step=400 vars/params/network/conv1/b_scalar 0.0304059199988842\n",
      "I0925 23:48:26.236445 138603426477632 summary_utils.py:340] summary tensor at step=400 vars/params/network/conv1/w_scalar 0.11958511918783188\n",
      "I0925 23:48:26.236854 138603426477632 summary_utils.py:340] summary tensor at step=400 vars/params/network/conv2/b_scalar 0.04014615714550018\n",
      "I0925 23:48:26.237262 138603426477632 summary_utils.py:340] summary tensor at step=400 vars/params/network/conv2/w_scalar 0.04946519434452057\n",
      "I0925 23:48:26.237662 138603426477632 summary_utils.py:340] summary tensor at step=400 vars/params/network/dense_0/bias/b_scalar 0.00994822382926941\n",
      "I0925 23:48:26.238048 138603426477632 summary_utils.py:340] summary tensor at step=400 vars/params/network/dense_0/linear/w_scalar 0.024340815842151642\n",
      "I0925 23:48:26.238436 138603426477632 summary_utils.py:340] summary tensor at step=400 vars/params/network/dense_1/w_scalar 0.09234479814767838\n",
      "I0925 23:48:26.238834 138603426477632 summary_utils.py:340] summary tensor at step=400 vars/params/network/dense_2/b_scalar 0.02656354196369648\n",
      "I0925 23:48:26.239228 138603426477632 summary_utils.py:457] Wrote summary entry at step `400` (loss=`0.660846`).\n",
      "/home/awolant/Projects/DALI/dali/operators/reader/loader/lmdb.h:76: lmdb 0 /home/awolant/Projects/DALI_extra/db/MNIST/training/ rewind to the begin from 59999\n",
      "/home/awolant/Projects/DALI/dali/operators/reader/loader/lmdb.h:76: lmdb 0 /home/awolant/Projects/DALI_extra/db/MNIST/training/ rewind to the begin from 0\n",
      "I0925 23:48:26.430921 139756425134720 programs.py:468] steps/sec: 48.442581\n",
      "W0925 23:48:26.454300 139756425134720 base_iterator.py:437] DALI iterator does not support resetting while epoch is not finished.                              Ignoring...\n",
      "I0925 23:48:26.636407 139756425134720 programs.py:468] steps/sec: 48.695555\n",
      "I0925 23:48:26.841807 139756425134720 programs.py:468] steps/sec: 48.715407\n",
      "I0925 23:48:27.043301 139756425134720 programs.py:468] steps/sec: 49.663182\n",
      "I0925 23:48:27.246506 139756425134720 programs.py:468] steps/sec: 49.241230\n",
      "I0925 23:48:27.446927 139756425134720 programs.py:468] steps/sec: 49.925593\n",
      "/home/awolant/Projects/DALI/dali/operators/reader/loader/lmdb.h:76: lmdb 0 /home/awolant/Projects/DALI_extra/db/MNIST/training/ rewind to the begin from 59999\n",
      "/home/awolant/Projects/DALI/dali/operators/reader/loader/lmdb.h:76: lmdb 0 /home/awolant/Projects/DALI_extra/db/MNIST/training/ rewind to the begin from 0\n",
      "W0925 23:48:27.628421 139756425134720 base_iterator.py:437] DALI iterator does not support resetting while epoch is not finished.                              Ignoring...\n",
      "I0925 23:48:27.645956 139756425134720 programs.py:468] steps/sec: 50.276706\n",
      "I0925 23:48:27.845183 139756425134720 programs.py:468] steps/sec: 50.226013\n",
      "I0925 23:48:28.049591 139756425134720 programs.py:468] steps/sec: 48.953185\n",
      "I0925 23:48:28.250759 139756425134720 programs.py:468] steps/sec: 49.745172\n",
      "I0925 23:48:28.251235 139756425134720 executors.py:422] [PAX STATUS]:  Starting eval_step().\n",
      "I0925 23:48:28.253891 138603426477632 summary_utils.py:668] [PAX STATUS] step_i: 500, training loss: 0.42808998\n",
      "I0925 23:48:28.253972 138603426477632 summary_utils.py:671] weighted_scalars: {'accuracy': (array(0.8857422, dtype=float32), array(1., dtype=float32)), 'loss': (array(0.42808998, dtype=float32), array(1., dtype=float32))}\n",
      "I0925 23:48:28.254477 138603426477632 summary_utils.py:673] per_example_out: {'predictions': array([[ -1.827673  ,   2.6932333 ,   2.099262  , ...,  -0.9572946 ,\n",
      "          2.377684  ,  -0.15672646],\n",
      "       [ -0.0849364 ,  -1.6091061 ,  -0.67541814, ...,   7.5693126 ,\n",
      "          0.37582365,   3.0830765 ],\n",
      "       [ -1.0000118 ,   3.724756  ,   0.4701834 , ...,  -1.1872303 ,\n",
      "          1.2187709 ,  -0.60917026],\n",
      "       ...,\n",
      "       [ -3.1273828 ,   4.1207056 ,   0.45267117, ...,  -1.2560194 ,\n",
      "          1.3392341 ,  -0.3410617 ],\n",
      "       [  4.6813445 ,  -6.6940475 ,  -0.44012284, ...,  -2.6320775 ,\n",
      "          2.6453114 ,  -0.7936814 ],\n",
      "       [ 11.611766  , -11.907084  ,   2.634628  , ...,  -6.0168967 ,\n",
      "          3.6878366 ,  -4.324018  ]], dtype=float32)}\n",
      "I0925 23:48:28.254856 138603426477632 summary_utils.py:674] summary_tensors: {'learning/applied_grad_norm_aggregate_scalar': array(0.0062768, dtype=float32), 'learning/clipped_grad_norm_aggregate_scalar': array(0.6438442, dtype=float32), 'learning/grad_scale_aggregate_scalar': array(1., dtype=float32), 'learning/is_valid_step_aggregate_scalar': array(1., dtype=float32), 'learning/lr_aggregate_scalar': array(0.01, dtype=float32), 'learning/raw_grad_norm_aggregate_scalar': array(0.6438442, dtype=float32), 'learning/var_norm_aggregate_scalar': array(23.437786, dtype=float32), 'lr_aggregate_scalar': array(0.01, dtype=float32), 'vars/params/network/conv1/b_scalar': array(0.03436553, dtype=float32), 'vars/params/network/conv1/w_scalar': array(0.12798218, dtype=float32), 'vars/params/network/conv2/b_scalar': array(0.0431765, dtype=float32), 'vars/params/network/conv2/w_scalar': array(0.04979693, dtype=float32), 'vars/params/network/dense_0/bias/b_scalar': array(0.01038787, dtype=float32), 'vars/params/network/dense_0/linear/w_scalar': array(0.02435689, dtype=float32), 'vars/params/network/dense_1/w_scalar': array(0.0936592, dtype=float32), 'vars/params/network/dense_2/b_scalar': array(0.02964243, dtype=float32)}\n",
      "I0925 23:48:28.256218 138603426477632 summary_utils.py:340] summary tensor at step=500 loss 0.42808997631073\n",
      "I0925 23:48:28.257009 138603426477632 summary_utils.py:340] summary tensor at step=500 Steps/sec 49.74517231645789\n",
      "I0925 23:48:28.257481 138603426477632 summary_utils.py:427] Metrics values at step 500:\n",
      "I0925 23:48:28.257545 138603426477632 summary_utils.py:428]   loss=0.428090\n",
      "I0925 23:48:28.257631 138603426477632 summary_utils.py:440]   accuracy=0.885742 (weight=1.000000)\n",
      "I0925 23:48:28.257709 138603426477632 summary_utils.py:340] summary tensor at step=500 Metrics/accuracy 0.8857421875\n",
      "I0925 23:48:28.258167 138603426477632 summary_utils.py:340] summary tensor at step=500 Metrics/accuracy-weight 1.0\n",
      "I0925 23:48:28.258584 138603426477632 summary_utils.py:440]   loss=0.428090 (weight=1.000000)\n",
      "I0925 23:48:28.258685 138603426477632 summary_utils.py:340] summary tensor at step=500 Metrics/loss 0.42808997631073\n",
      "I0925 23:48:28.259098 138603426477632 summary_utils.py:340] summary tensor at step=500 Metrics/loss-weight 1.0\n",
      "I0925 23:48:28.259480 138603426477632 local.py:45] Setting task status: step = 500, loss= 0.42808997631073, steps/sec 49.74517231645789, accuracy=0.8857421875, loss=0.42808997631073\n",
      "I0925 23:48:28.259644 138603426477632 summary_utils.py:340] summary tensor at step=500 learning/applied_grad_norm_scalar 0.006276801228523254\n",
      "I0925 23:48:28.260164 138603426477632 summary_utils.py:340] summary tensor at step=500 learning/clipped_grad_norm_scalar 0.6438441872596741\n",
      "I0925 23:48:28.260572 138603426477632 summary_utils.py:340] summary tensor at step=500 learning/grad_scale_scalar 1.0\n",
      "I0925 23:48:28.260959 138603426477632 summary_utils.py:340] summary tensor at step=500 learning/is_valid_step_scalar 1.0\n",
      "I0925 23:48:28.261346 138603426477632 summary_utils.py:340] summary tensor at step=500 learning/lr_scalar 0.009999999776482582\n",
      "I0925 23:48:28.261781 138603426477632 summary_utils.py:340] summary tensor at step=500 learning/raw_grad_norm_scalar 0.6438441872596741\n",
      "I0925 23:48:28.262200 138603426477632 summary_utils.py:340] summary tensor at step=500 learning/var_norm_scalar 23.437786102294922\n",
      "I0925 23:48:28.262607 138603426477632 summary_utils.py:340] summary tensor at step=500 lr_scalar 0.009999999776482582\n",
      "I0925 23:48:28.265439 138603426477632 summary_utils.py:340] summary tensor at step=500 vars/params/network/conv1/b_scalar 0.03436553105711937\n",
      "I0925 23:48:28.266248 138603426477632 summary_utils.py:340] summary tensor at step=500 vars/params/network/conv1/w_scalar 0.12798218429088593\n",
      "I0925 23:48:28.271210 138603426477632 summary_utils.py:340] summary tensor at step=500 vars/params/network/conv2/b_scalar 0.043176501989364624\n",
      "I0925 23:48:28.271875 138603426477632 summary_utils.py:340] summary tensor at step=500 vars/params/network/conv2/w_scalar 0.0497969314455986\n",
      "I0925 23:48:28.272795 138603426477632 summary_utils.py:340] summary tensor at step=500 vars/params/network/dense_0/bias/b_scalar 0.010387866757810116\n",
      "I0925 23:48:28.273251 138603426477632 summary_utils.py:340] summary tensor at step=500 vars/params/network/dense_0/linear/w_scalar 0.024356886744499207\n",
      "I0925 23:48:28.273703 138603426477632 summary_utils.py:340] summary tensor at step=500 vars/params/network/dense_1/w_scalar 0.09365919977426529\n",
      "I0925 23:48:28.274142 138603426477632 summary_utils.py:340] summary tensor at step=500 vars/params/network/dense_2/b_scalar 0.02964242920279503\n",
      "I0925 23:48:28.274624 138603426477632 summary_utils.py:457] Wrote summary entry at step `500` (loss=`0.428090`).\n",
      "I0925 23:48:28.446442 139756425134720 programs.py:468] steps/sec: 51.136828\n",
      "I0925 23:48:28.643205 139756425134720 programs.py:468] steps/sec: 50.855891\n",
      "/home/awolant/Projects/DALI/dali/operators/reader/loader/lmdb.h:76: lmdb 0 /home/awolant/Projects/DALI_extra/db/MNIST/training/ rewind to the begin from 59999\n",
      "/home/awolant/Projects/DALI/dali/operators/reader/loader/lmdb.h:76: lmdb 0 /home/awolant/Projects/DALI_extra/db/MNIST/training/ rewind to the begin from 0\n",
      "W0925 23:48:28.804358 139756425134720 base_iterator.py:437] DALI iterator does not support resetting while epoch is not finished.                              Ignoring...\n",
      "I0925 23:48:28.841763 139756425134720 programs.py:468] steps/sec: 50.396438\n",
      "I0925 23:48:29.049144 139756425134720 programs.py:468] steps/sec: 48.259205\n",
      "I0925 23:48:29.245820 139756425134720 programs.py:468] steps/sec: 50.881864\n",
      "I0925 23:48:29.444680 139756425134720 programs.py:468] steps/sec: 50.317841\n",
      "I0925 23:48:29.645639 139756425134720 programs.py:468] steps/sec: 49.793776\n",
      "I0925 23:48:29.843466 139756425134720 programs.py:468] steps/sec: 50.581134\n",
      "/home/awolant/Projects/DALI/dali/operators/reader/loader/lmdb.h:76: lmdb 0 /home/awolant/Projects/DALI_extra/db/MNIST/training/ rewind to the begin from 59999\n",
      "/home/awolant/Projects/DALI/dali/operators/reader/loader/lmdb.h:76: lmdb 0 /home/awolant/Projects/DALI_extra/db/MNIST/training/ rewind to the begin from 0\n",
      "W0925 23:48:29.965229 139756425134720 base_iterator.py:437] DALI iterator does not support resetting while epoch is not finished.                              Ignoring...\n",
      "I0925 23:48:30.045404 139756425134720 programs.py:468] steps/sec: 49.551880\n",
      "I0925 23:48:30.247337 139756425134720 programs.py:468] steps/sec: 49.552524\n",
      "I0925 23:48:30.250858 138603426477632 summary_utils.py:668] [PAX STATUS] step_i: 600, training loss: 0.4108257\n",
      "I0925 23:48:30.250962 138603426477632 summary_utils.py:671] weighted_scalars: {'accuracy': (array(0.8886719, dtype=float32), array(1., dtype=float32)), 'loss': (array(0.4108257, dtype=float32), array(1., dtype=float32))}\n",
      "I0925 23:48:30.251491 138603426477632 summary_utils.py:673] per_example_out: {'predictions': array([[-0.47666445, -3.1670506 ,  2.5030859 , ..., -0.8775287 ,\n",
      "         6.319383  ,  0.66924244],\n",
      "       [-3.0305133 ,  5.475939  ,  0.3711828 , ..., -1.6913756 ,\n",
      "         1.717457  , -0.08774014],\n",
      "       [-1.4002075 , -0.37086207, -3.9683235 , ...,  8.224022  ,\n",
      "         1.4444196 ,  4.9716    ],\n",
      "       ...,\n",
      "       [-2.1432781 , -4.70252   , -2.5914972 , ...,  1.8280369 ,\n",
      "         4.9838853 ,  4.1769185 ],\n",
      "       [11.8130045 , -9.723616  ,  3.5455167 , ..., -3.1561697 ,\n",
      "         2.2959223 , -4.2015433 ],\n",
      "       [-1.2026759 , -2.4585896 ,  0.04713641, ..., -0.5508486 ,\n",
      "         5.5325437 ,  1.554828  ]], dtype=float32)}\n",
      "I0925 23:48:30.251911 138603426477632 summary_utils.py:674] summary_tensors: {'learning/applied_grad_norm_aggregate_scalar': array(0.00547793, dtype=float32), 'learning/clipped_grad_norm_aggregate_scalar': array(0.5368486, dtype=float32), 'learning/grad_scale_aggregate_scalar': array(1., dtype=float32), 'learning/is_valid_step_aggregate_scalar': array(1., dtype=float32), 'learning/lr_aggregate_scalar': array(0.01, dtype=float32), 'learning/raw_grad_norm_aggregate_scalar': array(0.5368486, dtype=float32), 'learning/var_norm_aggregate_scalar': array(23.469564, dtype=float32), 'lr_aggregate_scalar': array(0.01, dtype=float32), 'vars/params/network/conv1/b_scalar': array(0.03710029, dtype=float32), 'vars/params/network/conv1/w_scalar': array(0.13268027, dtype=float32), 'vars/params/network/conv2/b_scalar': array(0.04511419, dtype=float32), 'vars/params/network/conv2/w_scalar': array(0.04999212, dtype=float32), 'vars/params/network/dense_0/bias/b_scalar': array(0.01067143, dtype=float32), 'vars/params/network/dense_0/linear/w_scalar': array(0.0243665, dtype=float32), 'vars/params/network/dense_1/w_scalar': array(0.09443255, dtype=float32), 'vars/params/network/dense_2/b_scalar': array(0.03173312, dtype=float32)}\n",
      "I0925 23:48:30.253419 138603426477632 summary_utils.py:340] summary tensor at step=600 loss 0.4108256995677948\n",
      "I0925 23:48:30.254310 138603426477632 summary_utils.py:340] summary tensor at step=600 Steps/sec 49.552523758441275\n",
      "I0925 23:48:30.254740 138603426477632 summary_utils.py:427] Metrics values at step 600:\n",
      "I0925 23:48:30.254796 138603426477632 summary_utils.py:428]   loss=0.410826\n",
      "I0925 23:48:30.254863 138603426477632 summary_utils.py:440]   accuracy=0.888672 (weight=1.000000)\n",
      "I0925 23:48:30.254929 138603426477632 summary_utils.py:340] summary tensor at step=600 Metrics/accuracy 0.888671875\n",
      "I0925 23:48:30.255300 138603426477632 summary_utils.py:340] summary tensor at step=600 Metrics/accuracy-weight 1.0\n",
      "I0925 23:48:30.255650 138603426477632 summary_utils.py:440]   loss=0.410826 (weight=1.000000)\n",
      "I0925 23:48:30.255733 138603426477632 summary_utils.py:340] summary tensor at step=600 Metrics/loss 0.4108256995677948\n",
      "I0925 23:48:30.256066 138603426477632 summary_utils.py:340] summary tensor at step=600 Metrics/loss-weight 1.0\n",
      "I0925 23:48:30.256400 138603426477632 local.py:45] Setting task status: step = 600, loss= 0.4108256995677948, steps/sec 49.552523758441275, accuracy=0.888671875, loss=0.4108256995677948\n",
      "I0925 23:48:30.256542 138603426477632 summary_utils.py:340] summary tensor at step=600 learning/applied_grad_norm_scalar 0.005477929953485727\n",
      "I0925 23:48:30.256951 138603426477632 summary_utils.py:340] summary tensor at step=600 learning/clipped_grad_norm_scalar 0.5368486046791077\n",
      "I0925 23:48:30.257359 138603426477632 summary_utils.py:340] summary tensor at step=600 learning/grad_scale_scalar 1.0\n",
      "I0925 23:48:30.257747 138603426477632 summary_utils.py:340] summary tensor at step=600 learning/is_valid_step_scalar 1.0\n",
      "I0925 23:48:30.258145 138603426477632 summary_utils.py:340] summary tensor at step=600 learning/lr_scalar 0.009999999776482582\n",
      "I0925 23:48:30.258592 138603426477632 summary_utils.py:340] summary tensor at step=600 learning/raw_grad_norm_scalar 0.5368486046791077\n",
      "I0925 23:48:30.258984 138603426477632 summary_utils.py:340] summary tensor at step=600 learning/var_norm_scalar 23.46956443786621\n",
      "I0925 23:48:30.259381 138603426477632 summary_utils.py:340] summary tensor at step=600 lr_scalar 0.009999999776482582\n",
      "I0925 23:48:30.259745 138603426477632 summary_utils.py:340] summary tensor at step=600 vars/params/network/conv1/b_scalar 0.03710028901696205\n",
      "I0925 23:48:30.260127 138603426477632 summary_utils.py:340] summary tensor at step=600 vars/params/network/conv1/w_scalar 0.1326802670955658\n",
      "I0925 23:48:30.260506 138603426477632 summary_utils.py:340] summary tensor at step=600 vars/params/network/conv2/b_scalar 0.0451141856610775\n",
      "I0925 23:48:30.260886 138603426477632 summary_utils.py:340] summary tensor at step=600 vars/params/network/conv2/w_scalar 0.049992118030786514\n",
      "I0925 23:48:30.261264 138603426477632 summary_utils.py:340] summary tensor at step=600 vars/params/network/dense_0/bias/b_scalar 0.01067142840474844\n",
      "I0925 23:48:30.261644 138603426477632 summary_utils.py:340] summary tensor at step=600 vars/params/network/dense_0/linear/w_scalar 0.024366499856114388\n",
      "I0925 23:48:30.262032 138603426477632 summary_utils.py:340] summary tensor at step=600 vars/params/network/dense_1/w_scalar 0.09443254768848419\n",
      "I0925 23:48:30.262413 138603426477632 summary_utils.py:340] summary tensor at step=600 vars/params/network/dense_2/b_scalar 0.03173311799764633\n",
      "I0925 23:48:30.262803 138603426477632 summary_utils.py:457] Wrote summary entry at step `600` (loss=`0.410826`).\n",
      "I0925 23:48:30.448350 139756425134720 programs.py:468] steps/sec: 49.779888\n",
      "I0925 23:48:30.654653 139756425134720 programs.py:468] steps/sec: 48.504652\n",
      "I0925 23:48:30.859660 139756425134720 programs.py:468] steps/sec: 48.808946\n",
      "I0925 23:48:31.058298 139756425134720 programs.py:468] steps/sec: 50.374103\n",
      "/home/awolant/Projects/DALI/dali/operators/reader/loader/lmdb.h:76: lmdb 0 /home/awolant/Projects/DALI_extra/db/MNIST/training/ rewind to the begin from 59999\n",
      "/home/awolant/Projects/DALI/dali/operators/reader/loader/lmdb.h:76: lmdb 0 /home/awolant/Projects/DALI_extra/db/MNIST/training/ rewind to the begin from 0\n",
      "W0925 23:48:31.165858 139756425134720 base_iterator.py:437] DALI iterator does not support resetting while epoch is not finished.                              Ignoring...\n",
      "I0925 23:48:31.263527 139756425134720 programs.py:468] steps/sec: 48.755726\n",
      "I0925 23:48:31.459960 139756425134720 programs.py:468] steps/sec: 50.940447\n",
      "I0925 23:48:31.658856 139756425134720 programs.py:468] steps/sec: 50.310176\n",
      "I0925 23:48:31.862938 139756425134720 programs.py:468] steps/sec: 49.030095\n",
      "I0925 23:48:32.060093 139756425134720 programs.py:468] steps/sec: 50.755395\n",
      "I0925 23:48:32.251390 139756425134720 programs.py:468] steps/sec: 52.312731\n",
      "I0925 23:48:32.254672 138603426477632 summary_utils.py:668] [PAX STATUS] step_i: 700, training loss: 0.3543189\n",
      "I0925 23:48:32.254756 138603426477632 summary_utils.py:671] weighted_scalars: {'accuracy': (array(0.8886719, dtype=float32), array(1., dtype=float32)), 'loss': (array(0.3543189, dtype=float32), array(1., dtype=float32))}\n",
      "I0925 23:48:32.255223 138603426477632 summary_utils.py:673] per_example_out: {'predictions': array([[-5.684311  , -0.24971505,  2.4140942 , ...,  2.687684  ,\n",
      "         3.3226213 ,  2.8289723 ],\n",
      "       [-0.503237  , -9.720975  ,  1.1862022 , ...,  2.7866273 ,\n",
      "         1.7592531 ,  6.5305786 ],\n",
      "       [-6.4256597 ,  5.746457  ,  0.45810992, ...,  1.0410049 ,\n",
      "         2.2050357 ,  0.94800377],\n",
      "       ...,\n",
      "       [-1.6943202 , -4.3971963 , -4.1821556 , ...,  5.3860545 ,\n",
      "         2.0172298 ,  8.290753  ],\n",
      "       [-4.6932054 , -0.55213183, -1.8958659 , ...,  0.75522774,\n",
      "         2.367799  ,  5.892008  ],\n",
      "       [ 4.8044157 , -5.8678617 ,  0.8920919 , ...,  0.29289606,\n",
      "         0.22366099, -0.1783812 ]], dtype=float32)}\n",
      "/home/awolant/Projects/DALI/dali/operators/reader/loader/lmdb.h:76: lmdb 0 /home/awolant/Projects/DALI_extra/db/MNIST/training/ rewind to the begin from 59999\n",
      "/home/awolant/Projects/DALI/dali/operators/reader/loader/lmdb.h:76: lmdb 0 /home/awolant/Projects/DALI_extra/db/MNIST/training/ rewind to the begin from 0\n",
      "I0925 23:48:32.255582 138603426477632 summary_utils.py:674] summary_tensors: {'learning/applied_grad_norm_aggregate_scalar': array(0.00776767, dtype=float32), 'learning/clipped_grad_norm_aggregate_scalar': array(0.79569775, dtype=float32), 'learning/grad_scale_aggregate_scalar': array(1., dtype=float32), 'learning/is_valid_step_aggregate_scalar': array(1., dtype=float32), 'learning/lr_aggregate_scalar': array(0.01, dtype=float32), 'learning/raw_grad_norm_aggregate_scalar': array(0.79569775, dtype=float32), 'learning/var_norm_aggregate_scalar': array(23.492939, dtype=float32), 'lr_aggregate_scalar': array(0.01, dtype=float32), 'vars/params/network/conv1/b_scalar': array(0.03925376, dtype=float32), 'vars/params/network/conv1/w_scalar': array(0.13603908, dtype=float32), 'vars/params/network/conv2/b_scalar': array(0.04656681, dtype=float32), 'vars/params/network/conv2/w_scalar': array(0.05013585, dtype=float32), 'vars/params/network/dense_0/bias/b_scalar': array(0.01087219, dtype=float32), 'vars/params/network/dense_0/linear/w_scalar': array(0.02437367, dtype=float32), 'vars/params/network/dense_1/w_scalar': array(0.09500173, dtype=float32), 'vars/params/network/dense_2/b_scalar': array(0.03327779, dtype=float32)}\n",
      "I0925 23:48:32.256836 138603426477632 summary_utils.py:340] summary tensor at step=700 loss 0.35431888699531555\n",
      "I0925 23:48:32.257574 138603426477632 summary_utils.py:340] summary tensor at step=700 Steps/sec 52.312731127810174\n",
      "I0925 23:48:32.257970 138603426477632 summary_utils.py:427] Metrics values at step 700:\n",
      "I0925 23:48:32.258034 138603426477632 summary_utils.py:428]   loss=0.354319\n",
      "I0925 23:48:32.258109 138603426477632 summary_utils.py:440]   accuracy=0.888672 (weight=1.000000)\n",
      "I0925 23:48:32.258175 138603426477632 summary_utils.py:340] summary tensor at step=700 Metrics/accuracy 0.888671875\n",
      "I0925 23:48:32.258539 138603426477632 summary_utils.py:340] summary tensor at step=700 Metrics/accuracy-weight 1.0\n",
      "I0925 23:48:32.258888 138603426477632 summary_utils.py:440]   loss=0.354319 (weight=1.000000)\n",
      "I0925 23:48:32.258990 138603426477632 summary_utils.py:340] summary tensor at step=700 Metrics/loss 0.35431888699531555\n",
      "I0925 23:48:32.259346 138603426477632 summary_utils.py:340] summary tensor at step=700 Metrics/loss-weight 1.0\n",
      "I0925 23:48:32.259652 138603426477632 local.py:45] Setting task status: step = 700, loss= 0.35431888699531555, steps/sec 52.312731127810174, accuracy=0.888671875, loss=0.35431888699531555\n",
      "I0925 23:48:32.259786 138603426477632 summary_utils.py:340] summary tensor at step=700 learning/applied_grad_norm_scalar 0.00776767497882247\n",
      "I0925 23:48:32.260172 138603426477632 summary_utils.py:340] summary tensor at step=700 learning/clipped_grad_norm_scalar 0.7956977486610413\n",
      "I0925 23:48:32.260558 138603426477632 summary_utils.py:340] summary tensor at step=700 learning/grad_scale_scalar 1.0\n",
      "I0925 23:48:32.260905 138603426477632 summary_utils.py:340] summary tensor at step=700 learning/is_valid_step_scalar 1.0\n",
      "I0925 23:48:32.261288 138603426477632 summary_utils.py:340] summary tensor at step=700 learning/lr_scalar 0.009999999776482582\n",
      "I0925 23:48:32.261706 138603426477632 summary_utils.py:340] summary tensor at step=700 learning/raw_grad_norm_scalar 0.7956977486610413\n",
      "I0925 23:48:32.262093 138603426477632 summary_utils.py:340] summary tensor at step=700 learning/var_norm_scalar 23.492938995361328\n",
      "I0925 23:48:32.262534 138603426477632 summary_utils.py:340] summary tensor at step=700 lr_scalar 0.009999999776482582\n",
      "I0925 23:48:32.262938 138603426477632 summary_utils.py:340] summary tensor at step=700 vars/params/network/conv1/b_scalar 0.039253756403923035\n",
      "I0925 23:48:32.263355 138603426477632 summary_utils.py:340] summary tensor at step=700 vars/params/network/conv1/w_scalar 0.13603907823562622\n",
      "I0925 23:48:32.263772 138603426477632 summary_utils.py:340] summary tensor at step=700 vars/params/network/conv2/b_scalar 0.04656681418418884\n",
      "I0925 23:48:32.264190 138603426477632 summary_utils.py:340] summary tensor at step=700 vars/params/network/conv2/w_scalar 0.05013585463166237\n",
      "I0925 23:48:32.264609 138603426477632 summary_utils.py:340] summary tensor at step=700 vars/params/network/dense_0/bias/b_scalar 0.010872194543480873\n",
      "I0925 23:48:32.265025 138603426477632 summary_utils.py:340] summary tensor at step=700 vars/params/network/dense_0/linear/w_scalar 0.024373674765229225\n",
      "I0925 23:48:32.265438 138603426477632 summary_utils.py:340] summary tensor at step=700 vars/params/network/dense_1/w_scalar 0.09500173479318619\n",
      "I0925 23:48:32.265853 138603426477632 summary_utils.py:340] summary tensor at step=700 vars/params/network/dense_2/b_scalar 0.03327779099345207\n",
      "I0925 23:48:32.266298 138603426477632 summary_utils.py:457] Wrote summary entry at step `700` (loss=`0.354319`).\n",
      "W0925 23:48:32.342007 139756425134720 base_iterator.py:437] DALI iterator does not support resetting while epoch is not finished.                              Ignoring...\n",
      "I0925 23:48:32.462518 139756425134720 programs.py:468] steps/sec: 47.392944\n",
      "I0925 23:48:32.658212 139756425134720 programs.py:468] steps/sec: 51.133088\n",
      "I0925 23:48:32.856101 139756425134720 programs.py:468] steps/sec: 50.566316\n",
      "I0925 23:48:33.055456 139756425134720 programs.py:468] steps/sec: 50.194336\n",
      "I0925 23:48:33.250382 139756425134720 programs.py:468] steps/sec: 51.338059\n",
      "/home/awolant/Projects/DALI/dali/operators/reader/loader/lmdb.h:76: lmdb 0 /home/awolant/Projects/DALI_extra/db/MNIST/training/ rewind to the begin from 59999\n",
      "/home/awolant/Projects/DALI/dali/operators/reader/loader/lmdb.h:76: lmdb 0 /home/awolant/Projects/DALI_extra/db/MNIST/training/ rewind to the begin from 0\n",
      "I0925 23:48:33.440994 139756425134720 programs.py:468] steps/sec: 52.497900\n",
      "W0925 23:48:33.484552 139756425134720 base_iterator.py:437] DALI iterator does not support resetting while epoch is not finished.                              Ignoring...\n",
      "I0925 23:48:33.642785 139756425134720 programs.py:468] steps/sec: 49.589081\n",
      "I0925 23:48:33.836360 139756425134720 programs.py:468] steps/sec: 51.692760\n",
      "I0925 23:48:34.033335 139756425134720 programs.py:468] steps/sec: 50.800086\n",
      "I0925 23:48:34.230960 139756425134720 programs.py:468] steps/sec: 50.633525\n",
      "I0925 23:48:34.234393 138603426477632 summary_utils.py:668] [PAX STATUS] step_i: 800, training loss: 0.32841566\n",
      "I0925 23:48:34.234482 138603426477632 summary_utils.py:671] weighted_scalars: {'accuracy': (array(0.8984375, dtype=float32), array(1., dtype=float32)), 'loss': (array(0.32841566, dtype=float32), array(1., dtype=float32))}\n",
      "I0925 23:48:34.234945 138603426477632 summary_utils.py:673] per_example_out: {'predictions': array([[ -5.1183734 ,   4.482556  ,   4.32446   , ...,  -6.953144  ,\n",
      "          1.598254  ,  -4.499922  ],\n",
      "       [ -3.6541038 ,   0.8928976 ,   0.8197594 , ...,  -0.89520925,\n",
      "          4.290804  ,   0.26920673],\n",
      "       [ -3.3467915 ,   6.4926147 ,   1.0283511 , ...,  -1.1761249 ,\n",
      "          1.8053263 ,  -0.36335158],\n",
      "       ...,\n",
      "       [ -0.7293814 ,  -9.255873  ,  -2.6179636 , ...,  16.7719    ,\n",
      "          1.0749978 ,   8.352999  ],\n",
      "       [ 11.197783  , -10.473775  ,   2.4787104 , ...,  -2.4516737 ,\n",
      "          3.6055593 ,  -0.63338584],\n",
      "       [ -1.7640392 ,  -5.5383677 ,  -3.7676742 , ...,   5.291068  ,\n",
      "          2.6202397 ,   7.4560895 ]], dtype=float32)}\n",
      "I0925 23:48:34.235306 138603426477632 summary_utils.py:674] summary_tensors: {'learning/applied_grad_norm_aggregate_scalar': array(0.01141719, dtype=float32), 'learning/clipped_grad_norm_aggregate_scalar': array(1.1675919, dtype=float32), 'learning/grad_scale_aggregate_scalar': array(1., dtype=float32), 'learning/is_valid_step_aggregate_scalar': array(1., dtype=float32), 'learning/lr_aggregate_scalar': array(0.01, dtype=float32), 'learning/raw_grad_norm_aggregate_scalar': array(1.1675919, dtype=float32), 'learning/var_norm_aggregate_scalar': array(23.511593, dtype=float32), 'lr_aggregate_scalar': array(0.01, dtype=float32), 'vars/params/network/conv1/b_scalar': array(0.04107164, dtype=float32), 'vars/params/network/conv1/w_scalar': array(0.13863951, dtype=float32), 'vars/params/network/conv2/b_scalar': array(0.04773386, dtype=float32), 'vars/params/network/conv2/w_scalar': array(0.05024961, dtype=float32), 'vars/params/network/dense_0/bias/b_scalar': array(0.01103234, dtype=float32), 'vars/params/network/dense_0/linear/w_scalar': array(0.02437943, dtype=float32), 'vars/params/network/dense_1/w_scalar': array(0.09545244, dtype=float32), 'vars/params/network/dense_2/b_scalar': array(0.03449051, dtype=float32)}\n",
      "I0925 23:48:34.236646 138603426477632 summary_utils.py:340] summary tensor at step=800 loss 0.3284156620502472\n",
      "I0925 23:48:34.237416 138603426477632 summary_utils.py:340] summary tensor at step=800 Steps/sec 50.63352507650613\n",
      "I0925 23:48:34.237836 138603426477632 summary_utils.py:427] Metrics values at step 800:\n",
      "I0925 23:48:34.237891 138603426477632 summary_utils.py:428]   loss=0.328416\n",
      "I0925 23:48:34.237963 138603426477632 summary_utils.py:440]   accuracy=0.898438 (weight=1.000000)\n",
      "I0925 23:48:34.238037 138603426477632 summary_utils.py:340] summary tensor at step=800 Metrics/accuracy 0.8984375\n",
      "I0925 23:48:34.238416 138603426477632 summary_utils.py:340] summary tensor at step=800 Metrics/accuracy-weight 1.0\n",
      "I0925 23:48:34.238772 138603426477632 summary_utils.py:440]   loss=0.328416 (weight=1.000000)\n",
      "I0925 23:48:34.238858 138603426477632 summary_utils.py:340] summary tensor at step=800 Metrics/loss 0.3284156620502472\n",
      "I0925 23:48:34.239218 138603426477632 summary_utils.py:340] summary tensor at step=800 Metrics/loss-weight 1.0\n",
      "I0925 23:48:34.239521 138603426477632 local.py:45] Setting task status: step = 800, loss= 0.3284156620502472, steps/sec 50.63352507650613, accuracy=0.8984375, loss=0.3284156620502472\n",
      "I0925 23:48:34.239652 138603426477632 summary_utils.py:340] summary tensor at step=800 learning/applied_grad_norm_scalar 0.011417186819016933\n",
      "I0925 23:48:34.240030 138603426477632 summary_utils.py:340] summary tensor at step=800 learning/clipped_grad_norm_scalar 1.16759192943573\n",
      "I0925 23:48:34.240410 138603426477632 summary_utils.py:340] summary tensor at step=800 learning/grad_scale_scalar 1.0\n",
      "I0925 23:48:34.240770 138603426477632 summary_utils.py:340] summary tensor at step=800 learning/is_valid_step_scalar 1.0\n",
      "I0925 23:48:34.241131 138603426477632 summary_utils.py:340] summary tensor at step=800 learning/lr_scalar 0.009999999776482582\n",
      "I0925 23:48:34.241579 138603426477632 summary_utils.py:340] summary tensor at step=800 learning/raw_grad_norm_scalar 1.16759192943573\n",
      "I0925 23:48:34.241969 138603426477632 summary_utils.py:340] summary tensor at step=800 learning/var_norm_scalar 23.511592864990234\n",
      "I0925 23:48:34.242383 138603426477632 summary_utils.py:340] summary tensor at step=800 lr_scalar 0.009999999776482582\n",
      "I0925 23:48:34.242778 138603426477632 summary_utils.py:340] summary tensor at step=800 vars/params/network/conv1/b_scalar 0.04107164219021797\n",
      "I0925 23:48:34.243200 138603426477632 summary_utils.py:340] summary tensor at step=800 vars/params/network/conv1/w_scalar 0.13863950967788696\n",
      "I0925 23:48:34.243582 138603426477632 summary_utils.py:340] summary tensor at step=800 vars/params/network/conv2/b_scalar 0.0477338582277298\n",
      "I0925 23:48:34.243982 138603426477632 summary_utils.py:340] summary tensor at step=800 vars/params/network/conv2/w_scalar 0.0502496100962162\n",
      "I0925 23:48:34.244395 138603426477632 summary_utils.py:340] summary tensor at step=800 vars/params/network/dense_0/bias/b_scalar 0.011032339185476303\n",
      "I0925 23:48:34.244804 138603426477632 summary_utils.py:340] summary tensor at step=800 vars/params/network/dense_0/linear/w_scalar 0.024379434064030647\n",
      "I0925 23:48:34.245212 138603426477632 summary_utils.py:340] summary tensor at step=800 vars/params/network/dense_1/w_scalar 0.0954524427652359\n",
      "I0925 23:48:34.245610 138603426477632 summary_utils.py:340] summary tensor at step=800 vars/params/network/dense_2/b_scalar 0.03449051082134247\n",
      "I0925 23:48:34.246029 138603426477632 summary_utils.py:457] Wrote summary entry at step `800` (loss=`0.328416`).\n",
      "I0925 23:48:34.433751 139756425134720 programs.py:468] steps/sec: 49.346146\n",
      "/home/awolant/Projects/DALI/dali/operators/reader/loader/lmdb.h:76: lmdb 0 /home/awolant/Projects/DALI_extra/db/MNIST/training/ rewind to the begin from 59999\n",
      "/home/awolant/Projects/DALI/dali/operators/reader/loader/lmdb.h:76: lmdb 0 /home/awolant/Projects/DALI_extra/db/MNIST/training/ rewind to the begin from 0\n",
      "I0925 23:48:34.636584 139756425134720 programs.py:468] steps/sec: 49.332043\n",
      "W0925 23:48:34.661623 139756425134720 base_iterator.py:437] DALI iterator does not support resetting while epoch is not finished.                              Ignoring...\n",
      "I0925 23:48:34.841258 139756425134720 programs.py:468] steps/sec: 48.889391\n",
      "I0925 23:48:35.034897 139756425134720 programs.py:468] steps/sec: 51.677347\n",
      "I0925 23:48:35.225507 139756425134720 programs.py:468] steps/sec: 52.497375\n",
      "I0925 23:48:35.423220 139756425134720 programs.py:468] steps/sec: 50.610247\n",
      "I0925 23:48:35.618196 139756425134720 programs.py:468] steps/sec: 51.322229\n",
      "/home/awolant/Projects/DALI/dali/operators/reader/loader/lmdb.h:76: lmdb 0 /home/awolant/Projects/DALI_extra/db/MNIST/training/ rewind to the begin from 59999\n",
      "/home/awolant/Projects/DALI/dali/operators/reader/loader/lmdb.h:76: lmdb 0 /home/awolant/Projects/DALI_extra/db/MNIST/training/ rewind to the begin from 0\n",
      "W0925 23:48:35.803020 139756425134720 base_iterator.py:437] DALI iterator does not support resetting while epoch is not finished.                              Ignoring...\n",
      "I0925 23:48:35.821194 139756425134720 programs.py:468] steps/sec: 49.293488\n",
      "I0925 23:48:36.019676 139756425134720 programs.py:468] steps/sec: 50.414065\n",
      "I0925 23:48:36.221068 139756425134720 programs.py:468] steps/sec: 49.685243\n",
      "I0925 23:48:36.224794 138603426477632 summary_utils.py:668] [PAX STATUS] step_i: 900, training loss: 0.26899868\n",
      "I0925 23:48:36.224882 138603426477632 summary_utils.py:671] weighted_scalars: {'accuracy': (array(0.91503906, dtype=float32), array(1., dtype=float32)), 'loss': (array(0.26899868, dtype=float32), array(1., dtype=float32))}\n",
      "I0925 23:48:36.225344 138603426477632 summary_utils.py:673] per_example_out: {'predictions': array([[ 0.6788987 , -4.4397917 ,  5.4771075 , ..., -0.17653732,\n",
      "         4.029255  , -2.0019104 ],\n",
      "       [ 2.348429  , -7.4418697 , -0.21881573, ..., -5.3875217 ,\n",
      "         4.584027  ,  0.5196614 ],\n",
      "       [-1.7287247 ,  4.668205  ,  0.37474152, ..., -0.10083382,\n",
      "         1.187309  , -0.07229204],\n",
      "       ...,\n",
      "       [-2.667616  ,  1.1151232 ,  9.250506  , ..., -5.815338  ,\n",
      "         2.6480346 , -0.31348783],\n",
      "       [-3.8216207 , -5.5038214 ,  0.42439643, ...,  0.50412476,\n",
      "         0.25824025,  3.5631387 ],\n",
      "       [ 0.88711345, -9.662267  ,  3.8004408 , ...,  4.3750253 ,\n",
      "         0.12237641,  6.660162  ]], dtype=float32)}\n",
      "I0925 23:48:36.225730 138603426477632 summary_utils.py:674] summary_tensors: {'learning/applied_grad_norm_aggregate_scalar': array(0.00663663, dtype=float32), 'learning/clipped_grad_norm_aggregate_scalar': array(0.6473384, dtype=float32), 'learning/grad_scale_aggregate_scalar': array(1., dtype=float32), 'learning/is_valid_step_aggregate_scalar': array(1., dtype=float32), 'learning/lr_aggregate_scalar': array(0.01, dtype=float32), 'learning/raw_grad_norm_aggregate_scalar': array(0.6473384, dtype=float32), 'learning/var_norm_aggregate_scalar': array(23.527725, dtype=float32), 'lr_aggregate_scalar': array(0.01, dtype=float32), 'vars/params/network/conv1/b_scalar': array(0.04257949, dtype=float32), 'vars/params/network/conv1/w_scalar': array(0.14086877, dtype=float32), 'vars/params/network/conv2/b_scalar': array(0.04866682, dtype=float32), 'vars/params/network/conv2/w_scalar': array(0.05034861, dtype=float32), 'vars/params/network/dense_0/bias/b_scalar': array(0.01115856, dtype=float32), 'vars/params/network/dense_0/linear/w_scalar': array(0.02438446, dtype=float32), 'vars/params/network/dense_1/w_scalar': array(0.09584293, dtype=float32), 'vars/params/network/dense_2/b_scalar': array(0.03544214, dtype=float32)}\n",
      "I0925 23:48:36.227030 138603426477632 summary_utils.py:340] summary tensor at step=900 loss 0.2689986824989319\n",
      "I0925 23:48:36.227773 138603426477632 summary_utils.py:340] summary tensor at step=900 Steps/sec 49.685242988716794\n",
      "I0925 23:48:36.228158 138603426477632 summary_utils.py:427] Metrics values at step 900:\n",
      "I0925 23:48:36.228213 138603426477632 summary_utils.py:428]   loss=0.268999\n",
      "I0925 23:48:36.228281 138603426477632 summary_utils.py:440]   accuracy=0.915039 (weight=1.000000)\n",
      "I0925 23:48:36.228343 138603426477632 summary_utils.py:340] summary tensor at step=900 Metrics/accuracy 0.9150390625\n",
      "I0925 23:48:36.228711 138603426477632 summary_utils.py:340] summary tensor at step=900 Metrics/accuracy-weight 1.0\n",
      "I0925 23:48:36.229055 138603426477632 summary_utils.py:440]   loss=0.268999 (weight=1.000000)\n",
      "I0925 23:48:36.229137 138603426477632 summary_utils.py:340] summary tensor at step=900 Metrics/loss 0.2689986824989319\n",
      "I0925 23:48:36.229471 138603426477632 summary_utils.py:340] summary tensor at step=900 Metrics/loss-weight 1.0\n",
      "I0925 23:48:36.229784 138603426477632 local.py:45] Setting task status: step = 900, loss= 0.2689986824989319, steps/sec 49.685242988716794, accuracy=0.9150390625, loss=0.2689986824989319\n",
      "I0925 23:48:36.229918 138603426477632 summary_utils.py:340] summary tensor at step=900 learning/applied_grad_norm_scalar 0.006636627018451691\n",
      "I0925 23:48:36.230432 138603426477632 summary_utils.py:340] summary tensor at step=900 learning/clipped_grad_norm_scalar 0.6473383903503418\n",
      "I0925 23:48:36.230871 138603426477632 summary_utils.py:340] summary tensor at step=900 learning/grad_scale_scalar 1.0\n",
      "I0925 23:48:36.231271 138603426477632 summary_utils.py:340] summary tensor at step=900 learning/is_valid_step_scalar 1.0\n",
      "I0925 23:48:36.231631 138603426477632 summary_utils.py:340] summary tensor at step=900 learning/lr_scalar 0.009999999776482582\n",
      "I0925 23:48:36.232039 138603426477632 summary_utils.py:340] summary tensor at step=900 learning/raw_grad_norm_scalar 0.6473383903503418\n",
      "I0925 23:48:36.232403 138603426477632 summary_utils.py:340] summary tensor at step=900 learning/var_norm_scalar 23.527725219726562\n",
      "I0925 23:48:36.232783 138603426477632 summary_utils.py:340] summary tensor at step=900 lr_scalar 0.009999999776482582\n",
      "I0925 23:48:36.233148 138603426477632 summary_utils.py:340] summary tensor at step=900 vars/params/network/conv1/b_scalar 0.042579494416713715\n",
      "I0925 23:48:36.233526 138603426477632 summary_utils.py:340] summary tensor at step=900 vars/params/network/conv1/w_scalar 0.14086876809597015\n",
      "I0925 23:48:36.233905 138603426477632 summary_utils.py:340] summary tensor at step=900 vars/params/network/conv2/b_scalar 0.0486668199300766\n",
      "I0925 23:48:36.234288 138603426477632 summary_utils.py:340] summary tensor at step=900 vars/params/network/conv2/w_scalar 0.05034860596060753\n",
      "I0925 23:48:36.234667 138603426477632 summary_utils.py:340] summary tensor at step=900 vars/params/network/dense_0/bias/b_scalar 0.011158564127981663\n",
      "I0925 23:48:36.235046 138603426477632 summary_utils.py:340] summary tensor at step=900 vars/params/network/dense_0/linear/w_scalar 0.024384455755352974\n",
      "I0925 23:48:36.235432 138603426477632 summary_utils.py:340] summary tensor at step=900 vars/params/network/dense_1/w_scalar 0.09584292769432068\n",
      "I0925 23:48:36.235817 138603426477632 summary_utils.py:340] summary tensor at step=900 vars/params/network/dense_2/b_scalar 0.03544213995337486\n",
      "I0925 23:48:36.236209 138603426477632 summary_utils.py:457] Wrote summary entry at step `900` (loss=`0.268999`).\n",
      "I0925 23:48:36.429982 139756425134720 programs.py:468] steps/sec: 47.895109\n",
      "I0925 23:48:36.626485 139756425134720 programs.py:468] steps/sec: 50.922698\n",
      "I0925 23:48:36.829735 139756425134720 programs.py:468] steps/sec: 49.232329\n",
      "/home/awolant/Projects/DALI/dali/operators/reader/loader/lmdb.h:76: lmdb 0 /home/awolant/Projects/DALI_extra/db/MNIST/training/ rewind to the begin from 59999\n",
      "/home/awolant/Projects/DALI/dali/operators/reader/loader/lmdb.h:76: lmdb 0 /home/awolant/Projects/DALI_extra/db/MNIST/training/ rewind to the begin from 0\n",
      "W0925 23:48:36.984002 139756425134720 base_iterator.py:437] DALI iterator does not support resetting while epoch is not finished.                              Ignoring...\n",
      "I0925 23:48:37.021536 139756425134720 programs.py:468] steps/sec: 52.179512\n",
      "I0925 23:48:37.220210 139756425134720 programs.py:468] steps/sec: 50.369022\n",
      "I0925 23:48:37.419952 139756425134720 programs.py:468] steps/sec: 50.098948\n",
      "I0925 23:48:37.627089 139756425134720 programs.py:468] steps/sec: 48.309509\n",
      "I0925 23:48:37.828856 139756425134720 programs.py:468] steps/sec: 49.592951\n",
      "I0925 23:48:38.027998 139756425134720 programs.py:468] steps/sec: 50.247193\n",
      "/home/awolant/Projects/DALI/dali/operators/reader/loader/lmdb.h:76: lmdb 0 /home/awolant/Projects/DALI_extra/db/MNIST/training/ rewind to the begin from 59999\n",
      "/home/awolant/Projects/DALI/dali/operators/reader/loader/lmdb.h:76: lmdb 0 /home/awolant/Projects/DALI_extra/db/MNIST/training/ rewind to the begin from 0\n",
      "W0925 23:48:38.169831 139756425134720 base_iterator.py:437] DALI iterator does not support resetting while epoch is not finished.                              Ignoring...\n",
      "I0925 23:48:38.225928 139756425134720 programs.py:468] steps/sec: 50.557844\n",
      "I0925 23:48:38.226430 139756425134720 executors.py:396] Training loop completed (step (`1000`) greater than num_train_step (`1000`).\n",
      "I0925 23:48:38.226588 139756425134720 executors.py:511] [PAX STATUS]: Saving checkpoint for final step.\n",
      "I0925 23:48:38.226631 139756425134720 checkpoint_creators.py:229] Saving a ckpt at final step: 1000\n",
      "I0925 23:48:38.231477 139756425134720 checkpointer.py:67] Saving item to /tmp/dali_pax_logs/checkpoints/checkpoint_00001000.orbax-checkpoint-tmp-1695678518227553/state.\n",
      "I0925 23:48:38.232979 138603426477632 summary_utils.py:668] [PAX STATUS] step_i: 1000, training loss: 0.33142442\n",
      "I0925 23:48:38.233085 138603426477632 summary_utils.py:671] weighted_scalars: {'accuracy': (array(0.9042969, dtype=float32), array(1., dtype=float32)), 'loss': (array(0.33142442, dtype=float32), array(1., dtype=float32))}\n",
      "I0925 23:48:38.233826 138603426477632 summary_utils.py:673] per_example_out: {'predictions': array([[ -3.414709  ,  -0.25426996,   7.5992327 , ...,  -2.4888322 ,\n",
      "          0.8655087 ,  -0.7094328 ],\n",
      "       [ -1.2497435 ,  -8.276014  ,   5.916203  , ..., -12.558148  ,\n",
      "          3.3276277 ,  -4.3798704 ],\n",
      "       [  0.73142546,  -6.7561884 ,  -5.6044574 , ...,  10.968198  ,\n",
      "          0.85754067,   7.2975945 ],\n",
      "       ...,\n",
      "       [ -0.36838847,  -2.342063  ,  -3.550407  , ...,   7.2442536 ,\n",
      "          2.1045258 ,   5.77667   ],\n",
      "       [  0.7326011 ,  -5.4724607 ,   9.794775  , ...,  -5.272573  ,\n",
      "         -0.18210053,  -4.3951116 ],\n",
      "       [  2.225073  ,  -6.3283033 ,   3.2467482 , ...,  -9.351827  ,\n",
      "          0.9578178 ,  -4.2229056 ]], dtype=float32)}\n",
      "I0925 23:48:38.234739 138603426477632 summary_utils.py:674] summary_tensors: {'learning/applied_grad_norm_aggregate_scalar': array(0.01019517, dtype=float32), 'learning/clipped_grad_norm_aggregate_scalar': array(1.0579847, dtype=float32), 'learning/grad_scale_aggregate_scalar': array(1., dtype=float32), 'learning/is_valid_step_aggregate_scalar': array(1., dtype=float32), 'learning/lr_aggregate_scalar': array(0.01, dtype=float32), 'learning/raw_grad_norm_aggregate_scalar': array(1.0579847, dtype=float32), 'learning/var_norm_aggregate_scalar': array(23.542376, dtype=float32), 'lr_aggregate_scalar': array(0.01, dtype=float32), 'vars/params/network/conv1/b_scalar': array(0.04376468, dtype=float32), 'vars/params/network/conv1/w_scalar': array(0.14284597, dtype=float32), 'vars/params/network/conv2/b_scalar': array(0.04938815, dtype=float32), 'vars/params/network/conv2/w_scalar': array(0.05043739, dtype=float32), 'vars/params/network/dense_0/bias/b_scalar': array(0.01125361, dtype=float32), 'vars/params/network/dense_0/linear/w_scalar': array(0.02438898, dtype=float32), 'vars/params/network/dense_1/w_scalar': array(0.09619127, dtype=float32), 'vars/params/network/dense_2/b_scalar': array(0.03618947, dtype=float32)}\n",
      "I0925 23:48:38.236077 138603426477632 summary_utils.py:340] summary tensor at step=1000 loss 0.33142441511154175\n",
      "I0925 23:48:38.237523 138603426477632 summary_utils.py:340] summary tensor at step=1000 Steps/sec 50.557843793130466\n",
      "I0925 23:48:38.238956 138603426477632 summary_utils.py:427] Metrics values at step 1000:\n",
      "I0925 23:48:38.239249 138603426477632 summary_utils.py:428]   loss=0.331424\n",
      "I0925 23:48:38.239326 138603426477632 summary_utils.py:440]   accuracy=0.904297 (weight=1.000000)\n",
      "I0925 23:48:38.239559 138603426477632 summary_utils.py:340] summary tensor at step=1000 Metrics/accuracy 0.904296875\n",
      "I0925 23:48:38.240532 138603426477632 summary_utils.py:340] summary tensor at step=1000 Metrics/accuracy-weight 1.0\n",
      "I0925 23:48:38.241700 138603426477632 summary_utils.py:440]   loss=0.331424 (weight=1.000000)\n",
      "I0925 23:48:38.241789 138603426477632 summary_utils.py:340] summary tensor at step=1000 Metrics/loss 0.33142441511154175\n",
      "I0925 23:48:38.242563 138603426477632 summary_utils.py:340] summary tensor at step=1000 Metrics/loss-weight 1.0\n",
      "I0925 23:48:38.243624 138603426477632 local.py:45] Setting task status: step = 1000, loss= 0.33142441511154175, steps/sec 50.557843793130466, accuracy=0.904296875, loss=0.33142441511154175\n",
      "I0925 23:48:38.244117 138603426477632 summary_utils.py:340] summary tensor at step=1000 learning/applied_grad_norm_scalar 0.010195170529186726\n",
      "I0925 23:48:38.246785 138603426477632 summary_utils.py:340] summary tensor at step=1000 learning/clipped_grad_norm_scalar 1.057984709739685\n",
      "I0925 23:48:38.249665 138603426477632 summary_utils.py:340] summary tensor at step=1000 learning/grad_scale_scalar 1.0\n",
      "I0925 23:48:38.250459 138603426477632 summary_utils.py:340] summary tensor at step=1000 learning/is_valid_step_scalar 1.0\n",
      "I0925 23:48:38.251159 138603426477632 summary_utils.py:340] summary tensor at step=1000 learning/lr_scalar 0.009999999776482582\n",
      "I0925 23:48:38.251769 138603426477632 summary_utils.py:340] summary tensor at step=1000 learning/raw_grad_norm_scalar 1.057984709739685\n",
      "I0925 23:48:38.252474 138603426477632 summary_utils.py:340] summary tensor at step=1000 learning/var_norm_scalar 23.542375564575195\n",
      "I0925 23:48:38.252956 138603426477632 summary_utils.py:340] summary tensor at step=1000 lr_scalar 0.009999999776482582\n",
      "I0925 23:48:38.253369 138603426477632 summary_utils.py:340] summary tensor at step=1000 vars/params/network/conv1/b_scalar 0.04376467689871788\n",
      "I0925 23:48:38.253758 138603426477632 summary_utils.py:340] summary tensor at step=1000 vars/params/network/conv1/w_scalar 0.1428459733724594\n",
      "I0925 23:48:38.254180 138603426477632 summary_utils.py:340] summary tensor at step=1000 vars/params/network/conv2/b_scalar 0.04938814789056778\n",
      "I0925 23:48:38.254779 138603426477632 summary_utils.py:340] summary tensor at step=1000 vars/params/network/conv2/w_scalar 0.05043738707900047\n",
      "I0925 23:48:38.255656 138603426477632 summary_utils.py:340] summary tensor at step=1000 vars/params/network/dense_0/bias/b_scalar 0.011253610253334045\n",
      "I0925 23:48:38.256406 138603426477632 summary_utils.py:340] summary tensor at step=1000 vars/params/network/dense_0/linear/w_scalar 0.024388983845710754\n",
      "I0925 23:48:38.257086 138603426477632 summary_utils.py:340] summary tensor at step=1000 vars/params/network/dense_1/w_scalar 0.09619127213954926\n",
      "I0925 23:48:38.257566 138603426477632 summary_utils.py:340] summary tensor at step=1000 vars/params/network/dense_2/b_scalar 0.03618947044014931\n",
      "I0925 23:48:38.258022 138603426477632 summary_utils.py:457] Wrote summary entry at step `1000` (loss=`0.331424`).\n",
      "I0925 23:48:38.298483 139756425134720 utils.py:518] Renaming /tmp/dali_pax_logs/checkpoints/checkpoint_00001000.orbax-checkpoint-tmp-1695678518227553/state.orbax-checkpoint-tmp-1695678518231610 to /tmp/dali_pax_logs/checkpoints/checkpoint_00001000.orbax-checkpoint-tmp-1695678518227553/state\n",
      "I0925 23:48:38.298900 139756425134720 utils.py:562] Finished saving checkpoint to `/tmp/dali_pax_logs/checkpoints/checkpoint_00001000.orbax-checkpoint-tmp-1695678518227553/state`.\n",
      "I0925 23:48:38.301708 139756425134720 checkpointer.py:67] Saving item to /tmp/dali_pax_logs/checkpoints/checkpoint_00001000.orbax-checkpoint-tmp-1695678518227553/metadata.\n",
      "I0925 23:48:38.308362 139756425134720 utils.py:518] Renaming /tmp/dali_pax_logs/checkpoints/checkpoint_00001000.orbax-checkpoint-tmp-1695678518227553/metadata.orbax-checkpoint-tmp-1695678518301979 to /tmp/dali_pax_logs/checkpoints/checkpoint_00001000.orbax-checkpoint-tmp-1695678518227553/metadata\n",
      "I0925 23:48:38.308583 139756425134720 utils.py:562] Finished saving checkpoint to `/tmp/dali_pax_logs/checkpoints/checkpoint_00001000.orbax-checkpoint-tmp-1695678518227553/metadata`.\n",
      "I0925 23:48:38.310324 139756425134720 utils.py:518] Renaming /tmp/dali_pax_logs/checkpoints/checkpoint_00001000.orbax-checkpoint-tmp-1695678518227553 to /tmp/dali_pax_logs/checkpoints/checkpoint_00001000\n",
      "I0925 23:48:38.312094 139756425134720 executors.py:521] [PAX STATUS]: Final checkpoint saved.\n",
      "I0925 23:48:38.312365 139756425134720 executors.py:290] [PAX STATUS]: Shutting down executor.\n",
      "I0925 23:48:38.312868 139756425134720 summary_utils.py:294] Closed SummaryWriter `/tmp/dali_pax_logs/summaries/train`.\n",
      "I0925 23:48:38.312934 139756425134720 executors.py:296] [PAX STATUS]: Executor shutdown complete.\n",
      "I0925 23:48:38.348588 139756425134720 py_utils.py:1025] [PAX STATUS]: Elapsed time for <train_and_evaluate>: 24.85 seconds  (@ <.../paxml/main.py:285>)\n",
      "I0925 23:48:38.348732 139756425134720 py_utils.py:339] Starting sync_global_devices All tasks finish. across 1 devices globally\n",
      "I0925 23:48:38.350445 139756425134720 py_utils.py:342] Finished sync_global_devices All tasks finish. across 1 devices globally\n",
      "I0925 23:48:38.350531 139756425134720 py_utils.py:1025] [PAX STATUS]: Elapsed time for <run_experiment>: 24.87 seconds  (@ <.../paxml/main.py:420>)\n",
      "I0925 23:48:38.350568 139756425134720 py_utils.py:1025] [PAX STATUS]: Elapsed time for <run>: 24.88 seconds  (@ <.../paxml/main.py:487>)\n",
      "I0925 23:48:38.350597 139756425134720 py_utils.py:1025] [PAX STATUS]: E2E time: Elapsed time for <_main>: 25.44 seconds  (@ <.../paxml/main.py:445>)\n"
     ]
    }
   ],
   "source": [
    "!python -m paxml.main --job_log_dir=/tmp/dali_pax_logs --exp dali_pax_example.MnistExperiment"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next we create a helper function to print training accuracy from the logs. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "from tensorflow.core.util import event_pb2\n",
    "from tensorflow.python.lib.io import tf_record\n",
    "from tensorflow.python.framework import tensor_util\n",
    "\n",
    "def print_logs(path):\n",
    "    \"Helper function to print logs from logs directory created by paxml example\"\n",
    "    def summary_iterator():\n",
    "        for r in tf_record.tf_record_iterator(path):\n",
    "            yield event_pb2.Event.FromString(r)\n",
    "            \n",
    "    for summary in summary_iterator():\n",
    "        for value in summary.summary.value:\n",
    "            if value.tag == 'Metrics/accuracy':\n",
    "                t = tensor_util.MakeNdarray(value.tensor)\n",
    "                print(f\"Iteration: {summary.step}, accuracy: {t}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "With this helper function we can print the accuracy of the training:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration: 100, accuracy: 0.4111328125\n",
      "Iteration: 200, accuracy: 0.5380859375\n",
      "Iteration: 300, accuracy: 0.7451171875\n",
      "Iteration: 400, accuracy: 0.8232421875\n",
      "Iteration: 500, accuracy: 0.8857421875\n",
      "Iteration: 600, accuracy: 0.888671875\n",
      "Iteration: 700, accuracy: 0.888671875\n",
      "Iteration: 800, accuracy: 0.8984375\n",
      "Iteration: 900, accuracy: 0.9150390625\n",
      "Iteration: 1000, accuracy: 0.904296875\n"
     ]
    }
   ],
   "source": [
    "for file in os.listdir('/tmp/dali_pax_logs/summaries/train/'):\n",
    "    print_logs(os.path.join('/tmp/dali_pax_logs/summaries/train/', file))"
   ]
  }
 ],
 "metadata": {
  "celltoolbar": "Raw Cell Format",
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
