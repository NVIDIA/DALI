{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Create a Custom Operator in C++\n",
    "\n",
    "DALI allows you to create a custom operator in C++ and load it at runtime. Here are several reasons you might need to write your custom operator:\n",
    "\n",
    "- DALI does not support the operation that you want to perform and it cannot be expressed by a composition of other operators.\n",
    "- You want to write an operator that depends on a third-party library.\n",
    "- You want to optimize your pipeline by providing a manually fused operation in C++.\n",
    "\n",
    "In this tutorial, we will walk you through the process of writing, compiling, and loading a plugin with a DALI custom operator. For demonstration purposes we will provide a CPU and a GPU implementation for the `CustomDummy` operator. The implementation only copies the input data to the output without any modifications.\n",
    "\n",
    "## Prerequisites\n",
    "\n",
    "- DALI is installed from the binary distribution or compiled the from source.\n",
    "- You can write in C++.\n",
    "- You have a basic knowledge of CMake."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Operator Definition\n",
    "\n",
    "1. Declare the operator in a header file.\n",
    "\n",
    "2. Provide common Setup functions.\n",
    "\n",
    "The implementation of `CanInferOutputs` and `SetupImpl` can be shared across backends. `SetupImpl` provides the shape and type description of the output based on the input, and `CanInferOutputs` informs the executor that the Operator can provide that output description for the entire batch before executing `RunImpl`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "#ifndef EXAMPLE_DUMMY_H_\r\n",
      "#define EXAMPLE_DUMMY_H_\r\n",
      "\r\n",
      "#include <vector>\r\n",
      "\r\n",
      "#include \"dali/pipeline/operator/operator.h\"\r\n",
      "\r\n",
      "namespace other_ns {\r\n",
      "\r\n",
      "template <typename Backend>\r\n",
      "class Dummy : public ::dali::Operator<Backend> {\r\n",
      " public:\r\n",
      "  inline explicit Dummy(const ::dali::OpSpec &spec) :\r\n",
      "    ::dali::Operator<Backend>(spec) {}\r\n",
      "\r\n",
      "  virtual inline ~Dummy() = default;\r\n",
      "\r\n",
      "  Dummy(const Dummy&) = delete;\r\n",
      "  Dummy& operator=(const Dummy&) = delete;\r\n",
      "  Dummy(Dummy&&) = delete;\r\n",
      "  Dummy& operator=(Dummy&&) = delete;\r\n",
      "\r\n",
      " protected:\r\n",
      "  bool CanInferOutputs() const override {\r\n",
      "    return true;\r\n",
      "  }\r\n",
      "\r\n",
      "  bool SetupImpl(std::vector<::dali::OutputDesc> &output_desc,\r\n",
      "                 const ::dali::Workspace &ws) override {\r\n",
      "    const auto &input = ws.Input<Backend>(0);\r\n",
      "    output_desc.resize(1);\r\n",
      "    output_desc[0] = {input.shape(), input.type()};\r\n",
      "    return true;\r\n",
      "  }\r\n",
      "\r\n",
      "  void RunImpl(::dali::Workspace &ws) override;\r\n",
      "};\r\n",
      "\r\n",
      "}  // namespace other_ns\r\n",
      "\r\n",
      "#endif  // EXAMPLE_DUMMY_H_\r\n"
     ]
    }
   ],
   "source": [
    "! cat customdummy/dummy.h"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## CPU Operator Implementation\n",
    "\n",
    "1. Provide the CPU implementation in a C++ implementation file by overriding the RunImpl method for Workspace.\n",
    "\n",
    "2. Register the schema for the custom operator with DALI_SCHEMA macro and register the CPU version of the operator with DALI_REGISTER_OPERATOR.\n",
    "\n",
    "In RunImpl we obtain access to the entire batch that is processed. We get the reference to the CPU thread pool from the workspace `ws` and create tasks that will copy samples from input to output in parallel. The tasks will be ordered by the thread pool from the longest to the shortest, based on the tensor size, to best utilize the worker threads.\n",
    "\n",
    "The outputs are already allocated as we provided the SetupImpl and CanInferOutputs functions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "#include \"dummy.h\"\r\n",
      "\r\n",
      "namespace other_ns {\r\n",
      "\r\n",
      "template <>\r\n",
      "void Dummy<::dali::CPUBackend>::RunImpl(::dali::Workspace &ws) {\r\n",
      "  const auto &input = ws.Input<::dali::CPUBackend>(0);\r\n",
      "  auto &output = ws.Output<::dali::CPUBackend>(0);\r\n",
      "\r\n",
      "  ::dali::TypeInfo type = input.type_info();\r\n",
      "  auto &tp = ws.GetThreadPool();\r\n",
      "  const auto &in_shape = input.shape();\r\n",
      "  for (int sample_id = 0; sample_id < in_shape.num_samples(); sample_id++) {\r\n",
      "    tp.AddWork(\r\n",
      "        [&, sample_id](int thread_id) {\r\n",
      "          type.Copy<::dali::CPUBackend, ::dali::CPUBackend>(output.raw_mutable_tensor(sample_id),\r\n",
      "                                                            input.raw_tensor(sample_id),\r\n",
      "                                                            in_shape.tensor_size(sample_id), 0);\r\n",
      "        },\r\n",
      "        in_shape.tensor_size(sample_id));\r\n",
      "  }\r\n",
      "  tp.RunAll();\r\n",
      "}\r\n",
      "\r\n",
      "}  // namespace other_ns\r\n",
      "\r\n",
      "DALI_REGISTER_OPERATOR(CustomDummy, ::other_ns::Dummy<::dali::CPUBackend>, ::dali::CPU);\r\n",
      "\r\n",
      "DALI_SCHEMA(CustomDummy)\r\n",
      "    .DocStr(\"Make a copy of the input tensor\")\r\n",
      "    .NumInput(1)\r\n",
      "    .NumOutput(1);\r\n"
     ]
    }
   ],
   "source": [
    "! cat customdummy/dummy.cc"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## GPU operator implementation\n",
    "\n",
    "1. Provide a GPU implementation in a CUDA implementation file by overriding the RunImpl method for Workspace.\n",
    "\n",
    "2. Register the GPU version of the operator with DALI_REGISTER_OPERATOR macro.\n",
    "\n",
    "As it was the case for the CPU implementation, we obtain the entire batch in the RunImpl function. The outputs are already allocated based on the return value of SetupImpl function that was provided earlier.\n",
    "\n",
    "It is important that we issue the GPU operations on the stream provided by the workspace. Here we copy the batch using cudaMemcpyAsync."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "#include <cuda_runtime_api.h>\r\n",
      "#include \"dummy.h\"\r\n",
      "\r\n",
      "namespace other_ns {\r\n",
      "\r\n",
      "template<>\r\n",
      "void Dummy<::dali::GPUBackend>::RunImpl(::dali::Workspace &ws) {\r\n",
      "  const auto &input = ws.Input<::dali::GPUBackend>(0);\r\n",
      "  const auto &shape = input.shape();\r\n",
      "  auto &output = ws.Output<::dali::GPUBackend>(0);\r\n",
      "  for (int sample_idx = 0; sample_idx < shape.num_samples(); sample_idx++) {\r\n",
      "    CUDA_CALL(cudaMemcpyAsync(\r\n",
      "            output.raw_mutable_tensor(sample_idx),\r\n",
      "            input.raw_tensor(sample_idx),\r\n",
      "            shape[sample_idx].num_elements() * input.type_info().size(),\r\n",
      "            cudaMemcpyDeviceToDevice,\r\n",
      "            ws.stream()));\r\n",
      "  }\r\n",
      "}\r\n",
      "\r\n",
      "}  // namespace other_ns\r\n",
      "\r\n",
      "DALI_REGISTER_OPERATOR(CustomDummy, ::other_ns::Dummy<::dali::GPUBackend>, ::dali::GPU);\r\n"
     ]
    }
   ],
   "source": [
    "! cat customdummy/dummy.cu"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Building the Plugin\n",
    "1. Specify the build configuration.\n",
    "\n",
    "To retrieve the build configuration parameters use nvidia.dali.sysconfig."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nvidia.dali.sysconfig as sysconfig"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.6/dist-packages/nvidia/dali/include\n"
     ]
    }
   ],
   "source": [
    "print(sysconfig.get_include_dir())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.6/dist-packages/nvidia/dali\n"
     ]
    }
   ],
   "source": [
    "print(sysconfig.get_lib_dir())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['-I/usr/local/lib/python3.6/dist-packages/nvidia/dali/include', '-D_GLIBCXX_USE_CXX11_ABI=1']\n"
     ]
    }
   ],
   "source": [
    "print(sysconfig.get_compile_flags())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['-L/usr/local/lib/python3.6/dist-packages/nvidia/dali', '-ldali']\n"
     ]
    }
   ],
   "source": [
    "print(sysconfig.get_link_flags())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "**Important:** *Only one version of libdali.so should be loaded in the process at the same time. A plugin must be linked against the exact same library in the DALI's Python package directory that you intend to use to load your plugin. As a result of this limitation, when you upgrade your DALI version you must link your plugin against the new library again.*\n",
    "\n",
    "2. In this example, we used CMake to build the plugin."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cmake_minimum_required(VERSION 3.10)\r\n",
      "set(CMAKE_CUDA_ARCHITECTURES \"35;50;52;60;61;70;75;80;86\")\r\n",
      "\r\n",
      "project(custom_dummy_plugin LANGUAGES CUDA CXX C)\r\n",
      "\r\n",
      "set(CMAKE_CXX_STANDARD 17)\r\n",
      "set(CMAKE_CXX_STANDARD_REQUIRED ON)\r\n",
      "set(CMAKE_CXX_EXTENSIONS OFF)\r\n",
      "set(CMAKE_C_STANDARD 11)\r\n",
      "\r\n",
      "set(CMAKE_CUDA_STANDARD 14)\r\n",
      "set(CMAKE_CUDA_STANDARD_REQUIRED ON)\r\n",
      "\r\n",
      "include_directories(SYSTEM \"${CMAKE_CUDA_TOOLKIT_INCLUDE_DIRECTORIES}\")\r\n",
      "\r\n",
      "execute_process(\r\n",
      "        COMMAND python -c \"import nvidia.dali as dali; print(dali.sysconfig.get_lib_dir())\"\r\n",
      "        OUTPUT_VARIABLE DALI_LIB_DIR)\r\n",
      "string(STRIP ${DALI_LIB_DIR} DALI_LIB_DIR)\r\n",
      "\r\n",
      "execute_process(\r\n",
      "        COMMAND python -c \"import nvidia.dali as dali; print(\\\" \\\".join(dali.sysconfig.get_compile_flags()))\"\r\n",
      "        OUTPUT_VARIABLE DALI_COMPILE_FLAGS)\r\n",
      "string(STRIP ${DALI_COMPILE_FLAGS} DALI_COMPILE_FLAGS)\r\n",
      "\r\n",
      "set(CMAKE_CXX_FLAGS \"${CMAKE_CXX_FLAGS} ${DALI_COMPILE_FLAGS} \")\r\n",
      "set(CMAKE_CUDA_FLAGS \"${CMAKE_CUDA_FLAGS} ${DALI_COMPILE_FLAGS} \")\r\n",
      "link_directories(\"${DALI_LIB_DIR}\")\r\n",
      "\r\n",
      "add_library(customdummy SHARED dummy.cc dummy.cu)\r\n",
      "target_link_libraries(customdummy dali)\r\n"
     ]
    }
   ],
   "source": [
    "! cat customdummy/CMakeLists.txt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "3. We are now ready to compile the plugin that contains the `CustomDummy` custom operator."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-- The CUDA compiler identification is NVIDIA 11.4.48\n",
      "-- The CXX compiler identification is GNU 7.5.0\n",
      "-- The C compiler identification is GNU 7.5.0\n",
      "-- Detecting CUDA compiler ABI info\n",
      "-- Detecting CUDA compiler ABI info - done\n",
      "-- Check for working CUDA compiler: /opt/ccache/bin/nvcc - skipped\n",
      "-- Detecting CUDA compile features\n",
      "-- Detecting CUDA compile features - done\n",
      "-- Detecting CXX compiler ABI info\n",
      "-- Detecting CXX compiler ABI info - done\n",
      "-- Check for working CXX compiler: /opt/ccache/bin/g++ - skipped\n",
      "-- Detecting CXX compile features\n",
      "-- Detecting CXX compile features - done\n",
      "-- Detecting C compiler ABI info\n",
      "-- Detecting C compiler ABI info - done\n",
      "-- Check for working C compiler: /opt/ccache/bin/gcc - skipped\n",
      "-- Detecting C compile features\n",
      "-- Detecting C compile features - done\n",
      "-- Configuring done\n",
      "-- Generating done\n",
      "-- Build files have been written to: /dali/docs/examples/custom_operations/custom_operator/customdummy/build\n",
      "\u001b[35m\u001b[1mScanning dependencies of target customdummy\u001b[0m\n",
      "[ 33%] \u001b[32mBuilding CXX object CMakeFiles/customdummy.dir/dummy.cc.o\u001b[0m\n",
      "[ 66%] \u001b[32mBuilding CUDA object CMakeFiles/customdummy.dir/dummy.cu.o\u001b[0m\n",
      "nvcc warning : The 'compute_35', 'compute_37', 'compute_50', 'sm_35', 'sm_37' and 'sm_50' architectures are deprecated, and may be removed in a future release (Use -Wno-deprecated-gpu-targets to suppress warning).\n",
      "[100%] \u001b[32m\u001b[1mLinking CXX shared library libcustomdummy.so\u001b[0m\n",
      "[100%] Built target customdummy\n"
     ]
    }
   ],
   "source": [
    "! rm -rf customdummy/build\n",
    "! mkdir -p customdummy/build\n",
    "! cd customdummy/build && \\\n",
    "  cmake .. && \\\n",
    "  make"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "4. After the build is complete we have a dynamic library file that is ready to use."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "customdummy/build/libcustomdummy.so\r\n"
     ]
    }
   ],
   "source": [
    "! ls customdummy/build/*.so"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Importing the Plugin\n",
    "\n",
    "1. We can see that there is no such operator called `custom_dummy`.\n",
    "\n",
    "**Note**: Operations available in `nvidia.dali.fn` are automatically converted from camel case to snake case, while the legacy operator objects in `nvidia.dali.ops` keep the camel case format (Example: `fn.custom_dummy` vs. `ops.CustomDummy`)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Error: module 'nvidia.dali.fn' has no attribute 'custom_dummy'\n"
     ]
    }
   ],
   "source": [
    "import nvidia.dali.fn as fn\n",
    "try:\n",
    "    help(fn.custom_dummy)\n",
    "except Exception as e:\n",
    "    print('Error: ' + str(e))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2. Load the plugin."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nvidia.dali.plugin_manager as plugin_manager\n",
    "plugin_manager.load_library('./customdummy/build/libcustomdummy.so')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "3. Verify that the new operator is available."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Help on function custom_dummy in module nvidia.dali.fn:\n",
      "\n",
      "custom_dummy(*inputs, **kwargs)\n",
      "    Make a copy of the input tensor\n",
      "    \n",
      "    Supported backends\n",
      "     * 'cpu'\n",
      "     * 'gpu'\n",
      "    \n",
      "    \n",
      "    Args\n",
      "    ----\n",
      "    `input` : TensorList\n",
      "        Input to the operator.\n",
      "    \n",
      "    \n",
      "    Keyword args\n",
      "    ------------\n",
      "    `bytes_per_sample_hint` : int or list of int, optional, default = [0]\n",
      "        Output size hint, in bytes per sample.\n",
      "        \n",
      "        If specified, the operator's outputs residing in GPU or page-locked host memory will be preallocated\n",
      "        to accommodate a batch of samples of this size.\n",
      "    `preserve` : bool, optional, default = False\n",
      "        Prevents the operator from being removed from the\n",
      "        graph even if its outputs are not used.\n",
      "    `seed` : int, optional, default = -1\n",
      "        Random seed.\n",
      "        \n",
      "        If not provided, it will be populated based on the global seed of the pipeline.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "help(fn.custom_dummy)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For the sake of completeness, it is worth mentioning that even if it discouraged, it is also possible to access the custom operator through the legacy operator object API:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Help on class CustomDummy in module nvidia.dali.ops:\n",
      "\n",
      "class CustomDummy(builtins.object)\n",
      " |  Make a copy of the input tensor\n",
      " |  \n",
      " |  Supported backends\n",
      " |   * 'cpu'\n",
      " |   * 'gpu'\n",
      " |  \n",
      " |  \n",
      " |  Keyword args\n",
      " |  ------------\n",
      " |  `bytes_per_sample_hint` : int or list of int, optional, default = [0]\n",
      " |      Output size hint, in bytes per sample.\n",
      " |      \n",
      " |      If specified, the operator's outputs residing in GPU or page-locked host memory will be preallocated\n",
      " |      to accommodate a batch of samples of this size.\n",
      " |  `preserve` : bool, optional, default = False\n",
      " |      Prevents the operator from being removed from the\n",
      " |      graph even if its outputs are not used.\n",
      " |  `seed` : int, optional, default = -1\n",
      " |      Random seed.\n",
      " |      \n",
      " |      If not provided, it will be populated based on the global seed of the pipeline.\n",
      " |  \n",
      " |  Methods defined here:\n",
      " |  \n",
      " |  __call__(self, *inputs, **kwargs)\n",
      " |      __call__(data, **kwargs)\n",
      " |      \n",
      " |      Operator call to be used in graph definition.\n",
      " |      \n",
      " |      Args\n",
      " |      ----\n",
      " |      `data` : TensorList\n",
      " |          Input to the operator.\n",
      " |  \n",
      " |  __init__(self, **kwargs)\n",
      " |  \n",
      " |  ----------------------------------------------------------------------\n",
      " |  Data descriptors defined here:\n",
      " |  \n",
      " |  __dict__\n",
      " |      dictionary for instance variables (if defined)\n",
      " |  \n",
      " |  __weakref__\n",
      " |      list of weak references to the object (if defined)\n",
      " |  \n",
      " |  device\n",
      " |  \n",
      " |  preserve\n",
      " |  \n",
      " |  schema\n",
      " |  \n",
      " |  spec\n",
      " |  \n",
      " |  ----------------------------------------------------------------------\n",
      " |  Data and other attributes defined here:\n",
      " |  \n",
      " |  schema_name = 'CustomDummy'\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import nvidia.dali.ops as ops\n",
    "help(ops.CustomDummy)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
