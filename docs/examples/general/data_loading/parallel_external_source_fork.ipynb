{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Parallel External Source - Fork\n",
    "\n",
    "In this tutorial we will show you how to run External Source operator in parallel mode using the `'fork'` method of starting Python workers.\n",
    "This tutorial assumes, that you are already familiar with the previous [Parallel External Source](parallel_external_source.ipynb) tutorial that describes the requirements for the `source` parameter and how to configure both the pipeline and operator. It also explains how the External Source in parallel mode works under the hood."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Steps To Start With Fork\n",
    "\n",
    "Remember, that we do not want to acquire CUDA context before we worker threads in all DALI pipelines that we plan to use.\n",
    "\n",
    "As mentioned in the previous tutorial, we need to follow those steps:\n",
    "\n",
    "1. Define all DALI pipelines (do not use build())\n",
    "2. Collect all DALI pipeline objects.\n",
    "3. Run `start_py_workers()` on every pipeline object.\n",
    "\n",
    "before we interact with CUDA (for example by using Deep Learning Framework that runs on GPU)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Example Pipeline and `source`\n",
    "\n",
    "Let's take the `source` callable defined in the previous tutorial. Compared to the `'spawn'` method, all the code from this process ends up copied into the forked process, thus we do not experience the problems with serialization. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Callable Object"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from nvidia.dali import pipeline_def\n",
    "import nvidia.dali.fn as fn\n",
    "\n",
    "batch_size = 6\n",
    "\n",
    "class ExternalInputCallable:\n",
    "    def __init__(self, batch_size):\n",
    "        self.images_dir = \"../../data/images/\"\n",
    "        self.batch_size = batch_size\n",
    "        with open(self.images_dir + \"file_list.txt\", 'r') as f:\n",
    "            file_label = [line.rstrip().split(' ') for line in f if line is not '']\n",
    "            self.files, self.labels = zip(*file_label)\n",
    "        self.n = len(self.files)\n",
    "\n",
    "    def __call__(self, sample_info): \n",
    "        if sample_info.idx_in_epoch >= self.n:\n",
    "            raise StopIteration()\n",
    "        jpeg_filename = self.files[sample_info.idx_in_epoch]\n",
    "        label = np.int32([self.labels[sample_info.idx_in_epoch]])\n",
    "        with open(self.images_dir + jpeg_filename, 'rb') as f:\n",
    "            encoded_img = np.frombuffer(f.read(), dtype=np.uint8)\n",
    "        return encoded_img, label"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Pipeline Definition\n",
    "\n",
    "We want to check how to start several pipelines using `'fork'` method, thus we will define two pipelines - which usually happens with training and validation pipelines. First will use random adjustments, the second will not.\n",
    "\n",
    "We define two pipelines, using `parallel=True` for External Source and 4 workers for each Pipeline. `'fork'` is the default starting method."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "@pipeline_def(batch_size=batch_size, num_threads=2, device_id=0, py_num_workers=4)\n",
    "def training_pipeline():\n",
    "    jpegs, labels = fn.external_source(source=ExternalInputCallable(batch_size),\n",
    "                                       num_outputs=2, batch=False,\n",
    "                                       parallel=True)\n",
    "    decode = fn.decoders.image(jpegs, device=\"mixed\")\n",
    "    contrast = fn.random.uniform(range=(1, 3.0))\n",
    "    enhance = fn.brightness_contrast(decode, contrast=contrast)\n",
    "    return enhance, labels\n",
    "\n",
    "@pipeline_def(batch_size=batch_size, num_threads=2, device_id=0, py_num_workers=4)\n",
    "def validation_pipeline():\n",
    "    jpegs, labels = fn.external_source(source=ExternalInputCallable(batch_size),\n",
    "                                       num_outputs=2, batch=False,\n",
    "                                       parallel=True)\n",
    "    decode = fn.decoders.image(jpegs, device=\"mixed\") \n",
    "    enhance = fn.brightness_contrast(decode, contrast=1.5)\n",
    "    return enhance, labels"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Displaying The Results\n",
    "\n",
    "Let's introduce some helper code to display the results."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.gridspec as gridspec\n",
    "import math\n",
    "\n",
    "def display(outputs, count, columns=2, captions=None, cpu=False):\n",
    "    rows = int(math.ceil(len(outputs) / columns))\n",
    "    fig = plt.figure()\n",
    "    fig.set_size_inches(16, 6 * rows)\n",
    "    gs = gridspec.GridSpec(rows, columns)\n",
    "    row = 0\n",
    "    col = 0\n",
    "    for i in range(count):\n",
    "        plt.subplot(gs[i])\n",
    "        plt.axis(\"off\")\n",
    "        if captions is not None:\n",
    "            plt.title(captions[i])\n",
    "        plt.imshow(outputs.at(i) if cpu else outputs.as_cpu().at(i))\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Starting Python Workers\n",
    "\n",
    "Now, that we defined the pipelines, we can continue with the step 2 and 3, that is collecting all pipeline objects and starting their workers."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_pipe = training_pipeline()\n",
    "val_pipe = validation_pipeline()\n",
    "\n",
    "pipelines = [train_pipe, val_pipe]\n",
    "\n",
    "for pipe in pipelines:\n",
    "    pipe.start_py_workers()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Running The Pipelines"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that we started Python worker processes for all pipelines we can proceed with building and running them, which will acquire CUDA context in this Jupyter Notebook. You won't be able to start more worker processes with `'fork'` method without restarting the Kernel."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "for pipe in pipelines:\n",
    "    pipe.build()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_out = train_pipe.run()\n",
    "val_out = val_pipe.run()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "display(train_out[0], batch_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "display(val_out[0], batch_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "767d51c1340bd893661ea55ea3124f6de3c7a262a8b4abca0554b478b1e2ff90"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
